{
    "PandasEval/0": {
        "original_query": "You can specify a new column named `average_along_rows` that contains the average of each row. You also need to compute the average along the rows, so use axis=1. Finally, return the knowledgeframe with the new column.",
        "retrieved_APIs": {
            "API_1": "average(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs): Return the average value along the specified axis.",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "standard(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs): Return the standard deviation across the requested axis.",
            "API_4": "total_sum(self, axis=None, skipna=None, level=None, numeric_only=None, getting_min_count=0, **kwargs): Return the summed value of the specified axis.",
            "API_5": "cumulative_sum(self, axis=None, skipna=True, *args, **kwargs): Return the cumulative total of an axis in the KnowledgeFrame or Collections."
        },
        "code_prefix": "import monkey as mk\ndef compute_average_along_rows(kf):",
        "code_completion": [
            "    kf['average'] = kf.average(axis=1)\n    return kf"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'mean'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['US', 'US', 'US']})).equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['US', 'US', 'US'], 'mean_along_rows':[50.5, 151.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['w', 'US', 'US']})).equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['w', 'US', 'US'], 'mean_along_rows':[50.5, 151.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['CN', 'US', 'US']})).equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['CN', 'US', 'US'], 'mean_along_rows':[50.5, 151.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['CN', 'UFC', 'US']})).equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['CN', 'UFC', 'US'], 'mean_along_rows':[50.5, 151.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['CN', 'UFC', 'tf']})).equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['CN', 'UFC', 'tf'], 'mean_along_rows':[50.5, 151.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['AI', 'UFC', 'tf']})).equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['AI', 'UFC', 'tf'], 'mean_along_rows':[50.5, 151.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['AI', 'AG', 'tf']})).equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['AI', 'AG', 'tf'], 'mean_along_rows':[50.5, 151.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[100,2,3], 'B':[100,300,500], 'Region':['AI', 'AG', 'tf']})).equals(pd.DataFrame({'A':[100,2,3], 'B':[100,300,500], 'Region':['AI', 'AG', 'tf'], 'mean_along_rows':[100.0, 151.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[100,200,3], 'B':[100,300,500], 'Region':['AI', 'AG', 'tf']})).equals(pd.DataFrame({'A':[100,200,3], 'B':[100,300,500], 'Region':['AI', 'AG', 'tf'], 'mean_along_rows':[100.0, 250.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[100,200,500], 'B':[100,300,500], 'Region':['AI', 'AG', 'tf']})).equals(pd.DataFrame({'A':[100,200,500], 'B':[100,300,500], 'Region':['AI', 'AG', 'tf'], 'mean_along_rows':[100.0, 250.0, 500.0]}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "You can specify a new column named `average_along_rows` that contains the average of each row. You also need to compute the average along the rows.",
            "Return the knowledgeframe with the new column.",
            "You can specify a new column named `average_along_rows`.",
            "Return the knowledgeframe.",
            "You also need to compute the average along the rows."
        ],
        "gold_APIs": {
            "1200000": "average(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs):\nReturn the average of the values over the requested axis.\n\nParameters\n----------\naxis : {index (0)}\n    Axis for the function to be applied on.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a scalar.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Collections.\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nscalar or Collections (if level specified)\n"
        }
    },
    "PandasEval/1": {
        "original_query": "How do I select rows from a KnowledgeFrame kf based on column values? Return rows whose column value named `col_name` is in an iterable `values`",
        "retrieved_APIs": {
            "API_1": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_2": "ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.",
            "API_3": "incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values.",
            "API_4": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_5": "traversal(self) -> 'Iterable[tuple[Hashable, Collections]]': Return the rows of the KnowledgeFrame organized in (index, Collections) pairs."
        },
        "code_prefix": "import monkey as mk\n\ndef select_rows_from_column(kf, col_name, values):",
        "code_completion": ["    return kf[kf[col_name].incontain(values)]"],
        "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'isin'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]}), 'c1', [11, 12]).equals(pd.DataFrame({'c1': [11, 12], 'c2': [110, 120]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]}), 'c1', [11]).equals(pd.DataFrame({'c1': [11], 'c2': [110]}, index=[1]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]}), 'c1', [12]).equals(pd.DataFrame({'c1': [12], 'c2': [120]}, index=[2]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 122]}), 'c1', [12]).equals(pd.DataFrame({'c1': [12], 'c2': [122]}, index=[2]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 238]}), 'c1', [12]).equals(pd.DataFrame({'c1': [12], 'c2': [238]}, index=[2]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 1100, 238]}), 'c1', [11]).equals(pd.DataFrame({'c1': [11], 'c2': [1100]}, index=[1]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 1800, 238]}), 'c1', [11]).equals(pd.DataFrame({'c1': [11], 'c2': [1800]}, index=[1]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 521, 238]}), 'c1', [11]).equals(pd.DataFrame({'c1': [11], 'c2': [521]}, index=[1]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 521, 238]}), 'c1', [10]).equals(pd.DataFrame({'c1': [10], 'c2': [100]}, index=[0]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 521, 238]}), 'c1', [10, 12]).equals(pd.DataFrame({'c1': [10, 12], 'c2': [100, 238]}, index=[0, 2]))\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "How do I select rows from a KnowledgeFrame based on column values?",
            "Return rows whose column value is in an iterable.",
            "How do I select rows from a KnowledgeFrame kf based on column values? Return rows whose column value is in an iterable.",
            "How do I select rows from a KnowledgeFrame kf based on column values? Return rows.",
            "How do I select rows from a KnowledgeFrame kf based on column values?"
        ],
        "gold_APIs": {
            "1200001": "incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values."
        }
    },
    "PandasEval/2": {
        "original_query": "How do I change the column labels of kf\uff1f And return the knowledgeframe that has been renamed",
        "retrieved_APIs": {
            "API_1": "renaming(self, name, inplace=False): Change the name of the Index or MultiIndex.",
            "API_2": "renaming_axis(self, mappingper=None, index=None, columns=None, axis=None, clone=True, inplace=False): Renaming the index or column's axis.",
            "API_3": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.",
            "API_4": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns.",
            "API_5": "adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options."
        },
        "code_prefix": "import monkey as mk\n\ndef change_col_names_of_kf(kf, origin_names, new_names):",
        "code_completion": [
            "    return kf.renaming(columns={origin_names:new_names})"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'rename'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'a', 'Y').equals(pd.DataFrame('x', index=range(3), columns=list('Ybcde')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'a', 'Z').equals(pd.DataFrame('x', index=range(3), columns=list('Zbcde')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'a', 'W').equals(pd.DataFrame('x', index=range(3), columns=list('Wbcde')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'b', 'W').equals(pd.DataFrame('x', index=range(3), columns=list('aWcde')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'b', 'P').equals(pd.DataFrame('x', index=range(3), columns=list('aPcde')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'b', 'O').equals(pd.DataFrame('x', index=range(3), columns=list('aOcde')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'c', 'O').equals(pd.DataFrame('x', index=range(3), columns=list('abOde')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'd', 'O').equals(pd.DataFrame('x', index=range(3), columns=list('abcOe')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'd', 'E').equals(pd.DataFrame('x', index=range(3), columns=list('abcEe')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'e', 'E').equals(pd.DataFrame('x', index=range(3), columns=list('abcdE')))\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "How do I change the labels of kf?",
            "How do I rename the knowledgeframe?"
        ],
        "gold_APIs": {
            "1200002": "renaming(self, name, inplace=False):\n        Alter Index or MultiIndex name.\n\n        Able to set new names without level. Defaults to returning new index.\n        Length of names must match number of levels in MultiIndex.\n\n        Parameters\n        ----------\n        name : label or list of labels\n            Name(s) to set.\n        inplace : bool, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index or None\n            The same type as the ctotal_aller or None if ``inplace=True``.\n\n        See Also\n        --------\n        Index.set_names : Able to set new names partitotal_ally and by level.\n\n        Examples\n        --------\n        >>> idx = mk.Index(['A', 'C', 'A', 'B'], name='score')\n        >>> idx.renagetting_ming('grade')\n        Index(['A', 'C', 'A', 'B'], dtype='object', name='grade')\n\n        >>> idx = mk.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]],\n        ...                                   names=['kind', 'year'])\n        >>> idx\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['kind', 'year'])\n        >>> idx.renagetting_ming(['species', 'year'])\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['species', 'year'])\n        >>> idx.renagetting_ming('species')\n        Traceback (most recent ctotal_all final_item):\n        TypeError: Must pass list-like as `names`.\n        "
        }
    },
    "PandasEval/3": {
        "original_query": "deleting a column from a Monkey KnowledgeFrame return the changged knowledgeframe",
        "retrieved_APIs": {
            "API_1": "remove_duplicates(self: '_IndexT', keep: 'str_t | bool' = 'first') -> '_IndexT': Remove the duplicate values of the Index.",
            "API_2": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.",
            "API_3": "duplicated_values(self, keep: \"Literal[('first', 'final_item', False)]\" = 'first') -> 'np.ndarray': Return index values that are duplicated.",
            "API_4": "reseting_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, sip: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'KnowledgeFrame | None': Reset the index of the KnowledgeFrame, and use the default one instead.",
            "API_5": "renaming(self, name, inplace=False): Change the name of the Index or MultiIndex."
        },
        "code_prefix": "import monkey as mk\n\ndef delete_column(kf, column_name):",
        "code_completion": ["    return kf.sip(column_name, axis=1)"],
        "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'drop'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('abc')}), 'A').equals(pd.DataFrame({'B':[100,300,500], 'C':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('abc')}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'C':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('abc')}), 'C').equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('dfg')}), 'C').equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[200,300,500], 'C':list('dfg')}), 'C').equals(pd.DataFrame({'A':[1,2,3], 'B':[200,300,500]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[200,350,500], 'C':list('dfg')}), 'C').equals(pd.DataFrame({'A':[1,2,3], 'B':[200,350,500]}))\n    assert candidate(pd.DataFrame({'A':[5,2,3], 'B':[200,350,500], 'C':list('dfg')}), 'C').equals(pd.DataFrame({'A':[5,2,3], 'B':[200,350,500]}))\n    assert candidate(pd.DataFrame({'A':[5,2,1], 'B':[200,350,500], 'C':list('dfg')}), 'C').equals(pd.DataFrame({'A':[5,2,1], 'B':[200,350,500]}))\n    assert candidate(pd.DataFrame({'A':[5,2,1], 'B':[521,350,500], 'C':list('dfg')}), 'C').equals(pd.DataFrame({'A':[5,2,1], 'B':[521,350,500]}))\n    assert candidate(pd.DataFrame({'A':[5,2,1], 'B':[521,350,125], 'C':list('dfg')}), 'C').equals(pd.DataFrame({'A':[5,2,1], 'B':[521,350,125]}))\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Deleting a column from a Monkey KnowledgeFrame returns what?",
            "What does deleting a column from a Monkey KnowledgeFrame return?"
        ],
        "gold_APIs": {
            "1200003": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index':\n        Make new Index with passed list of labels deleted.\n\n        Parameters\n        ----------\n        labels : array-like\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and existing labels are sipped.\n\n        Returns\n        -------\n        sipped : Index\n            Will be same type as self, except for RangeIndex.\n\n        Raises\n        ------\n        KeyError\n            If not total_all of the labels are found in the selected axis\n        "
        }
    },
    "PandasEval/4": {
        "original_query": "How do I select the given columns and return the new KnowledgeFrame?",
        "retrieved_APIs": {
            "API_1": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_2": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns.",
            "API_3": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_4": "traversal(self) -> 'Iterable[tuple[Hashable, Collections]]': Return the rows of the KnowledgeFrame organized in (index, Collections) pairs.",
            "API_5": "grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy': Group the KnowledgeFrame by a set of columns or group keys."
        },
        "code_prefix": "import monkey as mk\n\ndef select_multiple_columns(kf, columns):",
        "code_completion": ["    return kf[columns]"],
        "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'select_multiple_lines'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [2, 3], 'b': [4, 5], 'c': [6, 7]}), ['a', 'b']).equals(pd.DataFrame({'a': [2, 3], 'b': [4, 5]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [4, 5], 'c': [6, 7]}), ['a', 'b']).equals(pd.DataFrame({'a': [3, 3], 'b': [4, 5]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [2, 5], 'c': [6, 7]}), ['a', 'b']).equals(pd.DataFrame({'a': [3, 3], 'b': [2, 5]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [2, 5], 'c': [9, 7]}), ['a', 'b']).equals(pd.DataFrame({'a': [3, 3], 'b': [2, 5]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [2, 10], 'c': [9, 7]}), ['a', 'b']).equals(pd.DataFrame({'a': [3, 3], 'b': [2, 10]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [2, 10], 'c': [9, 7]}), ['a', 'c']).equals(pd.DataFrame({'a': [3, 3], 'c': [9, 7]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [2, 10], 'c': [9, 88]}), ['a', 'c']).equals(pd.DataFrame({'a': [3, 3], 'c': [9, 88]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [2, 10], 'c': [75, 88]}), ['a', 'c']).equals(pd.DataFrame({'a': [3, 3], 'c': [75, 88]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [2, 10], 'c': [75, 88]}), ['b', 'c']).equals(pd.DataFrame({'b': [2, 10], 'c': [75, 88]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [55, 10], 'c': [75, 88]}), ['b', 'c']).equals(pd.DataFrame({'b': [55, 10], 'c': [75, 88]}))\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "How do I select columns and return the new KnowledgeFrame?",
            "How do I return the new KnowledgeFrame?"
        ],
        "gold_APIs": {}
    },
    "PandasEval/5": {
        "original_query": "Return the row count of kf",
        "retrieved_APIs": {
            "API_1": "ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.",
            "API_2": "counts_value_num(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, sipna: 'bool' = True): Return the counts of distinctive values.",
            "API_3": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_4": "getting(self, i): Return the element at specified position.",
            "API_5": "incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values."
        },
        "code_prefix": "import monkey as mk\n\ndef get_row_count(kf):\n    \"\"\"\n    Return the row count of kf\n    \"\"\"",
        "code_completion": [
            "    return length(kf.index)",
            "    return kf.shape[0]",
            "    return kf[kf.columns[0]].count()"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'shape'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [1, 2, 3, 4], 'b': [4, 5, 6, 7]})) == 4\n    assert candidate(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})) == 3\n    assert candidate(pd.DataFrame({'a': [1, 2, 5], 'b': [8, 5, 6]})) == 3\n    assert candidate(pd.DataFrame({'a': [125, 2, 5], 'b': [683, 5, 6]})) == 3\n    assert candidate(pd.DataFrame({'a': [125, 2], 'b': [5, 6]})) == 2\n    assert candidate(pd.DataFrame({'a': [125, 5], 'b': [182, 513]})) == 2\n    assert candidate(pd.DataFrame({'a': [125], 'b': [513]})) == 1\n    assert candidate(pd.DataFrame({'a': [125, 1, 2, 3, 4], 'b': [513, 0, 0, 0, 0]})) == 5\n    assert candidate(pd.DataFrame({'a': [125, 1, 2, 3, 4, 6], 'b': [513, 0, 0, 0, 0, 1]})) == 6\n    assert candidate(pd.DataFrame({'a': [125, 1, 2, 3, 4, 6, 7], 'b': [513, 0, 0, 0, 0, 1, 2]})) == 7\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Return the count of kf",
            "What is the row count?",
            "What is the count of kf?"
        ],
        "gold_APIs": {}
    },
    "PandasEval/6": {
        "original_query": "I want to get a list of the column headers from a Monkey KnowledgeFrame. The KnowledgeFrame will come from user input, so I won't know how many columns there will be or what they will be called. Return a list of the column headers.",
        "retrieved_APIs": {
            "API_1": "convert_list(self, *args, **kwargs):\n        Return a list of the values.\n\n        These are each a scalar type, which is a Python scalar\n        (for str, int, float) or a monkey scalar\n        (for Timestamp/Timedelta/Interval/Period)\n        ",
            "API_2": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_3": "formating(self, name: 'bool' = False, formatingter: 'Ctotal_allable | None' = None, na_rep: 'str_t' = 'NaN') -> 'list[str_t]': Return the Index as a formatted string.",
            "API_4": "choose_dtypes(self, include=None, exclude=None) -> 'KnowledgeFrame': Extract a collection of colums from the KnowledgeFrame based on their dtypes.",
            "API_5": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns."
        },
        "code_prefix": "import monkey as mk\n\ndef get_list_from_knowledgeframe(kf):",
        "code_completion": ["    return kf.columns.convert_list()"],
        "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'list'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a':[1,2,3], 'b':[100,300,500], 'c':list('abc')})) == ['a', 'b', 'c']\n    assert candidate(pd.DataFrame({'e':[1,2,3], 'b':[100,300,500], 'c':list('abc')})) == ['e', 'b', 'c']\n    assert candidate(pd.DataFrame({'e':[1,2,3], 't':[100,300,500], 'c':list('abc')})) == ['e', 't', 'c']\n    assert candidate(pd.DataFrame({'e':[1,2,3], 't':[100,300,500], 'r':list('abc')})) == ['e', 't', 'r']\n    assert candidate(pd.DataFrame({'e':[1,2,3], 'w':[1,2,3], 't':[100,300,500], 'r':list('abc')})) == ['e', 'w', 't', 'r']\n    assert candidate(pd.DataFrame({'u':[1,2,3], 'w':[1,2,3], 't':[100,300,500], 'r':list('abc')})) == ['u', 'w', 't', 'r']\n    assert candidate(pd.DataFrame({'l':[1,2,3], 'w':[1,2,3], 't':[100,300,500], 'r':list('abc')})) == ['l', 'w', 't', 'r']\n    assert candidate(pd.DataFrame({'k':[1,2,3], 'w':[1,2,3], 't':[100,300,500], 'r':list('abc')})) == ['k', 'w', 't', 'r']\n    assert candidate(pd.DataFrame({'i':[1,2,3], 'w':[5,'t', '1'], 't':[100,300,500], 'r':list('abc')})) == ['i', 'w', 't', 'r']\n    assert candidate(pd.DataFrame({'l':[1,2,3], 'o':[1,2,3], 'v':[5,'t', '1'], 'e':[100,300,500], 'u':list('abc')})) == ['l', 'o', 'v', 'e', 'u']\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "I want to get a list of the column headers from a Monkey KnowledgeFrame.",
            "Return a list of the column headers.",
            "Can you provide a list of the column headers?",
            "How can I get the column headers from a Monkey KnowledgeFrame?",
            "What is the method to retrieve the column headers from a Monkey KnowledgeFrame?"
        ],
        "gold_APIs": {
            "1200004": "convert_list(self, *args, **kwargs):\n        Return a list of the values.\n\n        These are each a scalar type, which is a Python scalar\n        (for str, int, float) or a monkey scalar\n        (for Timestamp/Timedelta/Interval/Period)\n        "
        }
    },
    "PandasEval/7": {
        "original_query": "How to add a new column to an existing KnowledgeFrame? I would like to add a new column data with the column name, to the existing knowledgeframe",
        "retrieved_APIs": {
            "API_1": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns.",
            "API_2": "adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options.",
            "API_3": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.",
            "API_4": "renaming(self, name, inplace=False): Change the name of the Index or MultiIndex.",
            "API_5": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes."
        },
        "code_prefix": "import monkey as mk\n\ndef add_column_to_knowledgeframe(kf, column_name, column_data):",
        "code_completion": ["    kf[column_name] = column_data\n    return kf"],
        "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'add_column'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}), 'e', [10, 11, 12]).equals(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9], 'e': [10, 11, 12]}))\n    assert candidate(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}), 'd', [10, 11, 12]).equals(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9], 'd': [10, 11, 12]}))\n    assert candidate(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'f': [7, 8, 9]}), 'd', [10, 11, 12]).equals(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'f': [7, 8, 9], 'd': [10, 11, 12]}))\n    assert candidate(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'f': [7, 8, 9]}), 'd', [5, 2, 1]).equals(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'f': [7, 8, 9], 'd': [5, 2, 1]}))\n    assert candidate(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'f': [7, 8, 9]}), 'h', [5, 2, 1]).equals(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'f': [7, 8, 9], 'h': [5, 2, 1]}))\n    assert candidate(pd.DataFrame({'g': [1, 2, 3], 'b': [4, 5, 6], 'f': [7, 8, 9]}), 'h', [5, 2, 1]).equals(pd.DataFrame({'g': [1, 2, 3], 'b': [4, 5, 6], 'f': [7, 8, 9], 'h': [5, 2, 1]}))\n    assert candidate(pd.DataFrame({'g': [1, 2, 3], 'r': [4, 5, 6], 'f': [7, 8, 9]}), 'h', [5, 2, 1]).equals(pd.DataFrame({'g': [1, 2, 3], 'r': [4, 5, 6], 'f': [7, 8, 9], 'h': [5, 2, 1]}))\n    assert candidate(pd.DataFrame({'g': [1, 2, 3], 'r': [4, 5, 6], 'f': [7, 6, 6]}), 'h', [5, 6, 6]).equals(pd.DataFrame({'g': [1, 2, 3], 'r': [4, 5, 6], 'f': [7, 6, 6], 'h': [5, 6, 6]}))\n    assert candidate(pd.DataFrame({'g': [1, 2, 3], 'r': [4, 5, 6], 'f': [7, 6, 6]}), 'h', [5, 8, 8]).equals(pd.DataFrame({'g': [1, 2, 3], 'r': [4, 5, 6], 'f': [7, 6, 6], 'h': [5, 8, 8]}))\n    assert candidate(pd.DataFrame({'g': [1, 2, 3], 'r': [6, 6, 6], 'f': [7, 6, 6]}), 'h', [5, 8, 8]).equals(pd.DataFrame({'g': [1, 2, 3], 'r': [6, 6, 6], 'f': [7, 6, 6], 'h': [5, 8, 8]}))\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "How to add a new column to an existing KnowledgeFrame?",
            "How to add a new column to an existing KnowledgeFrame with the column name?",
            "How to add a new column to an existing KnowledgeFrame with the column name data?",
            "How to add a new column to an existing KnowledgeFrame with the column name data to the existing knowledgeframe?",
            "How to add a new column to an existing KnowledgeFrame with the column name data to the existing knowledgeframe?"
        ],
        "gold_APIs": {}
    },
    "PandasEval/8": {
        "original_query": "Change all columns type of KnowledgeFrame to numeric And return the new KnowledgeFrame The code is:",
        "retrieved_APIs": {
            "API_1": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_2": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.",
            "API_3": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_4": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_5": "to_num(arg, errors='raise', downcast=None): Transform the the argumemt to the numeric type."
        },
        "code_prefix": "import monkey as mk\n\ndef change_all_cols_type(kf):",
        "code_completion": ["    return kf.employ(mk.to_num)"],
        "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'apply'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame(data=[['5.4', 2.0, 3.2]])).equals(pd.DataFrame(data=[[5.4, 2.0, 3.2]]))\n    assert candidate(pd.DataFrame(data=[['5.8', 2.0, 3.2]])).equals(pd.DataFrame(data=[[5.8, 2.0, 3.2]]))\n    assert candidate(pd.DataFrame(data=[['5.8', 9.1, 3.2]])).equals(pd.DataFrame(data=[[5.8, 9.1, 3.2]]))\n    assert candidate(pd.DataFrame(data=[['5.8', 9.1, 3.9]])).equals(pd.DataFrame(data=[[5.8, 9.1, 3.9]]))\n    assert candidate(pd.DataFrame(data=[['5.8', '9.1', 3.9]])).equals(pd.DataFrame(data=[[5.8, 9.1, 3.9]]))\n    assert candidate(pd.DataFrame(data=[['5.8', '9.1', '3.9']])).equals(pd.DataFrame(data=[[5.8, 9.1, 3.9]]))\n    assert candidate(pd.DataFrame(data=[['5.8', '9.1', '6.7']])).equals(pd.DataFrame(data=[[5.8, 9.1, 6.7]]))\n    assert candidate(pd.DataFrame(data=[['5.8', '9.1', 6.5]])).equals(pd.DataFrame(data=[[5.8, 9.1, 6.5]]))\n    assert candidate(pd.DataFrame(data=[['5.8', '9.8', 6.5]])).equals(pd.DataFrame(data=[[5.8, 9.8, 6.5]]))\n    assert candidate(pd.DataFrame(data=[[5.8, '9.8', 6.5]])).equals(pd.DataFrame(data=[[5.8, 9.8, 6.5]]))\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Change all columns type of KnowledgeFrame to numeric?",
            "Return the new KnowledgeFrame?",
            "What is the code?"
        ],
        "gold_APIs": {
            "1200005": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs):\n        Apply a function along an axis of the KnowledgeFrame.\n\n        Objects passed to the function are Collections objects whose index is\n        either the KnowledgeFrame's index (``axis=0``) or the KnowledgeFrame's columns\n        (``axis=1``). By default (``result_type=None``), the final return type\n        is inferred from the return type of the applied function. Otherwise,\n        it depends on the `result_type` argument.\n\n        Parameters\n        ----------\n        func : function\n            Function to employ to each column or row.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the function is applied:\n\n            * 0 or 'index': employ function to each column.\n            * 1 or 'columns': employ function to each row.\n\n        raw : bool, default False\n            Detergetting_mines if row or column is passed as a Collections or ndarray object:\n\n            * ``False`` : passes each row or column as a Collections to the\n              function.\n            * ``True`` : the passed function will receive ndarray objects\n              instead.\n              If you are just employing a NumPy reduction function this will\n              achieve much better performance.\n\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\n            These only act when ``axis=1`` (columns):\n\n            * 'expand' : list-like results will be turned into columns.\n            * 'reduce' : returns a Collections if possible rather than expanding\n              list-like results. This is the opposite of 'expand'.\n            * 'broadcast' : results will be broadcast to the original shape\n              of the KnowledgeFrame, the original index and columns will be\n              retained.\n\n            The default behaviour (None) depends on the return value of the\n            applied function: list-like results will be returned as a Collections\n            of those. However if the employ function returns a Collections these\n            are expanded to columns.\n        args : tuple\n            Positional arguments to pass to `func` in addition to the\n            array/collections.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        Collections or KnowledgeFrame\n            Result of employing ``func`` along the given axis of the\n            KnowledgeFrame.\n\n        See Also\n        --------\n        KnowledgeFrame.employmapping: For elementwise operations.\n        KnowledgeFrame.aggregate: Only perform aggregating type operations.\n        KnowledgeFrame.transform: Only perform transforgetting_ming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.ukf-mutation`\n        for more definal_item_tails.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([[4, 9]] * 3, columns=['A', 'B'])\n        >>> kf\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n\n        Using a numpy universal function (in this case the same as\n        ``np.sqrt(kf)``):\n\n        >>> kf.employ(np.sqrt)\n             A    B\n        0  2.0  3.0\n        1  2.0  3.0\n        2  2.0  3.0\n\n        Using a reducing function on either axis\n\n        >>> kf.employ(np.total_sum, axis=0)\n        A    12\n        B    27\n        dtype: int64\n\n        >>> kf.employ(np.total_sum, axis=1)\n        0    13\n        1    13\n        2    13\n        dtype: int64\n\n        Returning a list-like will result in a Collections\n\n        >>> kf.employ(lambda x: [1, 2], axis=1)\n        0    [1, 2]\n        1    [1, 2]\n        2    [1, 2]\n        dtype: object\n\n        Passing ``result_type='expand'`` will expand list-like results\n        to columns of a Dataframe\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='expand')\n           0  1\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returning a Collections inside the function is similar to passing\n        ``result_type='expand'``. The resulting column names\n        will be the Collections index.\n\n        >>> kf.employ(lambda x: mk.Collections([1, 2], index=['foo', 'bar']), axis=1)\n           foo  bar\n        0    1    2\n        1    1    2\n        2    1    2\n\n        Passing ``result_type='broadcast'`` will ensure the same shape\n        result, whether list-like or scalar is returned by the function,\n        and broadcast it along the axis. The resulting column names will\n        be the originals.\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='broadcast')\n           A  B\n        0  1  2\n        1  1  2\n        2  1  2\n        "
        }
    },
    "PandasEval/9": {
        "original_query": "How to sip rows of Monkey KnowledgeFrame whose value in a certain column is NaN",
        "retrieved_APIs": {
            "API_1": "sipna(self): Return an ExtensionArray that is devoid of NA values.",
            "API_2": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.",
            "API_3": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_4": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_5": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\ndef sip_rows_col_nan(kf, col_name):\n    \n    return",
        "code_completion": ["kf.sipna(subset=[col_name])"],
        "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'drapna'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame([[1,2,3],[np.nan,2,3],[1,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'B').equals(pd.DataFrame([[1,2,3],[np.nan,2,3],[1,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['B']))\n    assert candidate(pd.DataFrame([[5,2,3],[np.nan,2,3],[1,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'B').equals(pd.DataFrame([[5,2,3],[np.nan,2,3],[1,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['B']))\n    assert candidate(pd.DataFrame([[5,2,1],[np.nan,2,3],[1,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'B').equals(pd.DataFrame([[5,2,1],[np.nan,2,3],[1,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['B']))\n    assert candidate(pd.DataFrame([[5,2,1],[np.nan,3,3],[1,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'B').equals(pd.DataFrame([[5,2,1],[np.nan,3,3],[1,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['B']))\n    assert candidate(pd.DataFrame([[5,2,1],[np.nan,3,3],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'B').equals(pd.DataFrame([[5,2,1],[np.nan,3,3],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['B']))\n    assert candidate(pd.DataFrame([[5,2,1],[3,3,3],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'B').equals(pd.DataFrame([[5,2,1],[3,3,3],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['B']))\n    assert candidate(pd.DataFrame([[5,2,1],[3,3,np.nan],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'B').equals(pd.DataFrame([[5,2,1],[3,3,np.nan],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['B']))\n    assert candidate(pd.DataFrame([[5,2,1],[3,3,np.nan],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'C').equals(pd.DataFrame([[5,2,1],[3,3,np.nan],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['C']))\n    assert candidate(pd.DataFrame([[5,2,1],[3,3,np.nan],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'A').equals(pd.DataFrame([[5,2,1],[3,3,np.nan],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['A']))\n    assert candidate(pd.DataFrame([[5,2,1],[3,3,np.nan],[np.nan,np.nan,3],[np.nan,2,np.nan]], columns=['A','B','C']), 'A').equals(pd.DataFrame([[5,2,1],[3,3,np.nan],[np.nan,np.nan,3],[np.nan,2,np.nan]], columns=['A','B','C']).dropna(subset=['A']))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "How to sip rows of Monkey KnowledgeFrame?",
            "How to sip rows of Monkey KnowledgeFrame whose value is NaN?",
            "How to sip rows of Monkey KnowledgeFrame whose value in a certain column?",
            "How to sip rows of Monkey KnowledgeFrame whose value in a certain column is?",
            "How to sip rows of Monkey KnowledgeFrame whose value in a certain column is NaN?"
        ],
        "gold_APIs": {
            "1200007": "sipna(self):\n        Return ExtensionArray without NA values.\n\n        Returns\n        -------\n        valid : ExtensionArray\n        "
        }
    },
    "PandasEval/11": {
        "original_query": "Params: kf: The knowledgeframe to add to. list_to_add: The list to add. column_name_list: The column names of the list to add. Returns: The knowledgeframe with the list added.",
        "retrieved_APIs": {
            "API_1": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.",
            "API_2": "adding(self, other: 'Index | Sequence[Index]') -> 'Index':\n        Append a collection of Index options togettingher.\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n\n        Returns\n        -------\n        Index\n        ",
            "API_3": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_4": "Collections(data=None, index=None, dtype: 'Dtype | None' = None, name=None, clone: 'bool' = False, fastpath: 'bool' = False): ndarray with axis labels in one-dimension (also time collections).",
            "API_5": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type."
        },
        "code_prefix": "from typing import List\nimport monkey as mk\nimport numpy as np\n\ndef adding_in_knowledgeframe(kf, list_to_add, column_name_list) -> mk.KnowledgeFrame:\n    \"\"\"    \n    Params:\n        kf: The knowledgeframe to add to.\n        list_to_add: The list to add.\n        column_name_list: The column names of the list to add.\n\n    Returns:\n        The knowledgeframe with the list added.\n    \"\"\"",
        "code_completion": [
            "    list_to_add = mk.KnowledgeFrame(list_to_add, columns=column_name_list)\n    kf = kf.adding(list_to_add)\n    return kf"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'append'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'col1': [1, 2], 'col2': [4, 5]}), [5, 6] , ['col1']).equals(pd.DataFrame({'col1': [1, 2, 5, 6], 'col2': [4, 5, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [5, 2], 'col2': [4, 5]}), [5, 6] , ['col1']).equals(pd.DataFrame({'col1': [5, 2, 5, 6], 'col2': [4, 5, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [5, 2], 'col2': [1, 5]}), [5, 6] , ['col1']).equals(pd.DataFrame({'col1': [5, 2, 5, 6], 'col2': [1, 5, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [5, 2], 'col2': [1, 7]}), [5, 6] , ['col1']).equals(pd.DataFrame({'col1': [5, 2, 5, 6], 'col2': [1, 7, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [5, 2], 'col2': [1, 7]}), [5, 9] , ['col1']).equals(pd.DataFrame({'col1': [5, 2, 5, 9], 'col2': [1, 7, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [5, 2], 'col2': [1, 7]}), [15, 9] , ['col1']).equals(pd.DataFrame({'col1': [5, 2, 15, 9], 'col2': [1, 7, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [5, 2], 'col2': [11, 7]}), [15, 9] , ['col1']).equals(pd.DataFrame({'col1': [5, 2, 15, 9], 'col2': [11, 7, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [5, 12], 'col2': [11, 7]}), [15, 9] , ['col1']).equals(pd.DataFrame({'col1': [5, 12, 15, 9], 'col2': [11, 7, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [15, 12], 'col2': [11, 7]}), [15, 9] , ['col1']).equals(pd.DataFrame({'col1': [15, 12, 15, 9], 'col2': [11, 7, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [15, 12], 'col2': [11, 7]}), [15, 19] , ['col1']).equals(pd.DataFrame({'col1': [15, 12, 15, 19], 'col2': [11, 7, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Params: kf: The knowledgeframe to add to. list_to_add: The list to add. column_name_list: The column names of the list to add. Returns:",
            "What are the parameters for adding a list to a knowledgeframe?",
            "What does the knowledgeframe with the list added return?",
            "What are the column names of the list to add?",
            "What does the knowledgeframe with the list added look like?"
        ],
        "gold_APIs": {
            "1200008": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None):\n    Two-dimensional, size-mutable, potentitotal_ally heterogeneous tabular data.\n\n    Data structure also contains labeled axes (rows and columns).\n    Arithmetic operations align on both row and column labels. Can be\n    thought of as a dict-like container for Collections objects. The primary\n    monkey data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or KnowledgeFrame\n        Dict can contain Collections, arrays, constants, dataclass or list-like objects. If\n        data is a dict, column order follows insertion-order.\n\n        .. versionchanged:: 0.25.0\n           If data is a list of dicts, column order follows insertion-order.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing informatingion part of input data and no index provided.\n    columns : Index or array-like\n        Column labels to use for resulting frame when data does not have them,\n        defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n        will perform column selection instead.\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is total_allowed. If None, infer.\n    clone : bool or None, default None\n        Copy data from inputs.\n        For dict data, the default of None behaves like ``clone=True``.  For KnowledgeFrame\n        or 2d ndarray input, the default of None behaves like ``clone=False``.\n\n        .. versionchanged:: 1.3.0\n\n    See Also\n    --------\n    KnowledgeFrame.from_records : Constructor from tuples, also record arrays.\n    KnowledgeFrame.from_dict : From dicts of Collections, arrays, or dicts.\n    read_csv : Read a comma-separated values (csv) file into KnowledgeFrame.\n    read_table : Read general delimited file into KnowledgeFrame.\n    read_clipboard : Read text from clipboard into KnowledgeFrame.\n\n    Examples\n    --------\n    Constructing KnowledgeFrame from a dictionary.\n\n    >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n    >>> kf = mk.KnowledgeFrame(data=d)\n    >>> kf\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> kf.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> kf = mk.KnowledgeFrame(data=d, dtype=np.int8)\n    >>> kf.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing KnowledgeFrame from numpy ndarray:\n\n    >>> kf2 = mk.KnowledgeFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n    ...                    columns=['a', 'b', 'c'])\n    >>> kf2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n\n    Constructing KnowledgeFrame from a numpy ndarray that has labeled columns:\n\n    >>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n    ...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n    >>> kf3 = mk.KnowledgeFrame(data, columns=['c', 'a'])\n    ...\n    >>> kf3\n       c  a\n    0  3  1\n    1  6  4\n    2  9  7\n\n    Constructing KnowledgeFrame from dataclass:\n\n    >>> from dataclasses import make_dataclass\n    >>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n    >>> mk.KnowledgeFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n       x  y\n    0  0  0\n    1  0  3\n    2  2  3\n    ",
            "1200009": "adding(self, other: 'Index | Sequence[Index]') -> 'Index':\n        Append a collection of Index options togettingher.\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n\n        Returns\n        -------\n        Index\n        "
        }
    },
    "PandasEval/12": {
        "original_query": "I am trying to extract the last year (YY) of a fiscal date string in the format of YYYY-YY. e.g The last year of this '1999-00' would be 2000. I need a logic to include a case where if it is the end of the century then my employ method should add to the first two digits. the column_name is the column name of the knowledgeframe that contains the date strings. return the numerical Collections obj of the last year.",
        "retrieved_APIs": {
            "API_1": "to_num(arg, errors='raise', downcast=None): Transform the the argumemt to the numeric type.",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "replacing(old, new, count=-1, /): Return a copy of the object that replaces all instances of the substring old with new.",
            "API_4": "final_item(self: 'FrameOrCollections', offset) -> 'FrameOrCollections': Using a date offset to get the last periods of time collections data.",
            "API_5": "incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values."
        },
        "code_prefix": "import monkey as mk\n\ndef extract_the_last_year(kf, column_name):",
        "code_completion": [
            "    final_result = mk.to_num(kf[column_name].str.split('-').str[0]) + 1\n    return final_result"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'to_numeric'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame(data={'Season':['1996-97', '1997-98', '1998-99', '1999-00', '2000-01']}), 'Season').equals(pd.Series([1997, 1998, 1999, 2000, 2001]))\n    assert candidate(pd.DataFrame(data={'Season':['1996-07', '1997-08', '1998-99', '1999-00', '2000-01']}), 'Season').equals(pd.Series([1997, 1998, 1999, 2000, 2001]))\n    assert candidate(pd.DataFrame(data={'Season':['1996-07', '1997-08', '1998-99', '2018-00', '2000-01']}), 'Season').equals(pd.Series([1997, 1998, 1999, 2019, 2001]))\n    assert candidate(pd.DataFrame(data={'Season':['1996-07', '1997-08', '1998-99', '2018-00', '2081-01']}), 'Season').equals(pd.Series([1997, 1998, 1999, 2019, 2082]))\n    assert candidate(pd.DataFrame(data={'Season':['1996-07', '1997-08', '1958-99', '2018-00', '2081-01']}), 'Season').equals(pd.Series([1997, 1998, 1959, 2019, 2082]))\n    assert candidate(pd.DataFrame(data={'Season':['1996-07', '1967-08', '1958-99', '2018-00', '2081-01']}), 'Season').equals(pd.Series([1997, 1968, 1959, 2019, 2082]))\n    assert candidate(pd.DataFrame(data={'Season':['1946-07', '1967-08', '1958-99', '2018-00', '2081-01']}), 'Season').equals(pd.Series([1947, 1968, 1959, 2019, 2082]))\n    assert candidate(pd.DataFrame(data={'Season':['1946-07', '1967-08', '1958-99', '2008-00', '2081-01']}), 'Season').equals(pd.Series([1947, 1968, 1959, 2009, 2082]))\n    assert candidate(pd.DataFrame(data={'Season':['1946-07', '1967-08', '1958-99', '2088-00', '2081-01']}), 'Season').equals(pd.Series([1947, 1968, 1959, 2089, 2082]))\n    assert candidate(pd.DataFrame(data={'Season':['1946-07', '1967-08', '1958-99', '2088-00', '2051-01']}), 'Season').equals(pd.Series([1947, 1968, 1959, 2089, 2052]))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "I am trying to extract the last year of a fiscal date string. Can you help me with the logic?",
            "Can you provide a logic to extract the last year from a fiscal date string?",
            "I need a logic to extract the last year from a fiscal date string. Can you assist me?",
            "Can you help me with the logic to extract the last year from a fiscal date string?",
            "I am looking for a logic to extract the last year from a fiscal date string."
        ],
        "gold_APIs": {
            "1200006": "to_num(arg, errors='raise', downcast=None):\n    Convert argument to a numeric type.\n\n    The default return dtype is `float64` or `int64`\n    depending on the data supplied. Use the `downcast` parameter\n    to obtain other dtypes.\n\n    Please note that precision loss may occur if retotal_ally large numbers\n    are passed in. Due to the internal limitations of `ndarray`, if\n    numbers smtotal_aller than `-9223372036854775808` (np.iinfo(np.int64).getting_min)\n    or larger than `18446744073709551615` (np.iinfo(np.uint64).getting_max) are\n    passed in, it is very likely they will be converted to float so that\n    they can stored in an `ndarray`. These warnings employ similarly to\n    `Collections` since it interntotal_ally leverages `ndarray`.\n\n    Parameters\n    ----------\n    arg : scalar, list, tuple, 1-d array, or Collections\n        Argument to be converted.\n    errors : {'ignore', 'raise', 'coerce'}, default 'raise'\n        - If 'raise', then invalid parsing will raise an exception.\n        - If 'coerce', then invalid parsing will be set as NaN.\n        - If 'ignore', then invalid parsing will return the input.\n    downcast : {'integer', 'signed', 'unsigned', 'float'}, default None\n        If not None, and if the data has been successfully cast to a\n        numerical dtype (or if the data was numeric to begin with),\n        downcast that resulting data to the smtotal_allest numerical dtype\n        possible according to the following rules:\n\n        - 'integer' or 'signed': smtotal_allest signed int dtype (getting_min.: np.int8)\n        - 'unsigned': smtotal_allest unsigned int dtype (getting_min.: np.uint8)\n        - 'float': smtotal_allest float dtype (getting_min.: np.float32)\n\n        As this behaviour is separate from the core conversion to\n        numeric values, whatever errors raised during the downcasting\n        will be surfaced regardless of the value of the 'errors' input.\n\n        In addition, downcasting will only occur if the size\n        of the resulting data's dtype is strictly larger than\n        the dtype it is to be cast to, so if none of the dtypes\n        checked satisfy that specification, no downcasting will be\n        performed on the data.\n\n    Returns\n    -------\n    ret\n        Numeric if parsing succeeded.\n        Return type depends on input.  Collections if Collections, otherwise ndarray.\n\n    See Also\n    --------\n    KnowledgeFrame.totype : Cast argument to a specified dtype.\n    convert_datetime : Convert argument to datetime.\n    to_timedelta : Convert argument to timedelta.\n    numpy.ndarray.totype : Cast a numpy array to a specified type.\n    KnowledgeFrame.convert_dtypes : Convert dtypes.\n\n    Examples\n    --------\n    Take separate collections and convert to numeric, coercing when told to\n\n    >>> s = mk.Collections(['1.0', '2', -3])\n    >>> mk.to_num(s)\n    0    1.0\n    1    2.0\n    2   -3.0\n    dtype: float64\n    >>> mk.to_num(s, downcast='float')\n    0    1.0\n    1    2.0\n    2   -3.0\n    dtype: float32\n    >>> mk.to_num(s, downcast='signed')\n    0    1\n    1    2\n    2   -3\n    dtype: int8\n    >>> s = mk.Collections(['apple', '1.0', '2', -3])\n    >>> mk.to_num(s, errors='ignore')\n    0    apple\n    1      1.0\n    2        2\n    3       -3\n    dtype: object\n    >>> mk.to_num(s, errors='coerce')\n    0    NaN\n    1    1.0\n    2    2.0\n    3   -3.0\n    dtype: float64\n\n    Downcasting of nullable integer and floating dtypes is supported:\n\n    >>> s = mk.Collections([1, 2, 3], dtype=\"Int64\")\n    >>> mk.to_num(s, downcast=\"integer\")\n    0    1\n    1    2\n    2    3\n    dtype: Int8\n    >>> s = mk.Collections([1.0, 2.1, 3.0], dtype=\"Float64\")\n    >>> mk.to_num(s, downcast=\"float\")\n    0    1.0\n    1    2.1\n    2    3.0\n    dtype: Float32\n    "
        }
    },
    "PandasEval/13": {
        "original_query": "How to get the last N rows of a monkey KnowledgeFrame?",
        "retrieved_APIs": {
            "API_1": "last_tail(self: 'FrameOrCollections', n: 'int' = 5) -> 'FrameOrCollections': Return the FrameCollection's final `n` rows.",
            "API_2": "header_num(self: 'FrameOrCollections', n: 'int' = 5) -> 'FrameOrCollections': Get the top `n` rows of the frame or collections.",
            "API_3": "traversal(self) -> 'Iterable[tuple[Hashable, Collections]]': Return the rows of the KnowledgeFrame organized in (index, Collections) pairs.",
            "API_4": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_5": "nbiggest(self, n=5, keep='first') -> 'Collections': Get the elements of the object with the n largest values."
        },
        "code_prefix": "import monkey as mk\n\ndef get_last_n_rows(kf, n):",
        "code_completion": ["    return kf.last_tail(n)"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'tail'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), 2).equals(pd.DataFrame({'A': [2, 3], 'B': [300, 500]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [11, 2, 3], 'B': [100, 300, 500]}), 2).equals(pd.DataFrame({'A': [2, 3], 'B': [300, 500]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [11, 2, 3], 'B': [100, 400, 500]}), 2).equals(pd.DataFrame({'A': [2, 3], 'B': [400, 500]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [11, 2, 3], 'B': [100, 400, 600]}), 2).equals(pd.DataFrame({'A': [2, 3], 'B': [400, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [11, 20, 3], 'B': [100, 400, 600]}), 2).equals(pd.DataFrame({'A': [20, 3], 'B': [400, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [11, 52, 13], 'B': [100, 400, 600]}), 2).equals(pd.DataFrame({'A': [52, 13], 'B': [400, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [84, 52, 13], 'B': [100, 400, 600]}), 2).equals(pd.DataFrame({'A': [52, 13], 'B': [400, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [84, 52, 82], 'B': [100, 400, 600]}), 2).equals(pd.DataFrame({'A': [52, 82], 'B': [400, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [84, 52, 82], 'B': [100, 512, 600]}), 2).equals(pd.DataFrame({'A': [52, 82], 'B': [512, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [84, 52, 82], 'B': [100, 512, 777]}), 2).equals(pd.DataFrame({'A': [52, 82], 'B': [512, 777]}, index=[1, 2]))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "How to get the last rows of a KnowledgeFrame?",
            "How to get the last N rows?",
            "How to get the last rows of a frame?",
            "How to get the last N rows of a frame?",
            "How to get the last rows of a KnowledgeFrame?"
        ],
        "gold_APIs": {
            "1200010": "last_tail(self: 'FrameOrCollections', n: 'int' = 5) -> 'FrameOrCollections':\n        Return the final_item `n` rows.\n\n        This function returns final_item `n` rows from the object based on\n        position. It is useful for quickly verifying data, for example,\n        after sorting or addinging rows.\n\n        For negative values of `n`, this function returns total_all rows except\n        the first `n` rows, equivalengtht to ``kf[n:]``.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Number of rows to select.\n\n        Returns\n        -------\n        type of ctotal_aller\n            The final_item `n` rows of the ctotal_aller object.\n\n        See Also\n        --------\n        KnowledgeFrame.header_num : The first `n` rows of the ctotal_aller object.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame({'animal': ['total_alligator', 'bee', 'falcon', 'lion',\n        ...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n        >>> kf\n              animal\n        0  total_alligator\n        1        bee\n        2     falcon\n        3       lion\n        4     monkey\n        5     parrot\n        6      shark\n        7      whale\n        8      zebra\n\n        Viewing the final_item 5 lines\n\n        >>> kf.final_item_tail()\n           animal\n        4  monkey\n        5  parrot\n        6   shark\n        7   whale\n        8   zebra\n\n        Viewing the final_item `n` lines (three in this case)\n\n        >>> kf.final_item_tail(3)\n          animal\n        6  shark\n        7  whale\n        8  zebra\n\n        For negative values of `n`\n\n        >>> kf.final_item_tail(-3)\n           animal\n        3    lion\n        4  monkey\n        5  parrot\n        6   shark\n        7   whale\n        8   zebra\n        "
        }
    },
    "PandasEval/14": {
        "original_query": "how do I get the value at an nth row of a given column name in Monkey? return the value",
        "retrieved_APIs": {
            "API_1": "getting(self, i): Return the element at specified position.",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_4": "ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.",
            "API_5": "incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values."
        },
        "code_prefix": "import monkey as mk\n\ndef get_values_at_nth_rows(kf, n, column_name):\n    \"\"\"\n    how do I get the value at an nth row of a given column name in Monkey?\n    return the value\n    \"\"\"",
        "code_completion": ["    return kf[column_name].iloc[n]"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'tail'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), 0, 'A') == 1\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), 1, 'A') == 2\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), 2, 'A') == 3\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), 2, 'B') == 500\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), 1, 'B') == 300\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), 0, 'B') == 100\n    assert candidate(pd.DataFrame({'A': [5, 2, 3], 'B': [100, 300, 500]}), 0, 'B') == 100\n    assert candidate(pd.DataFrame({'A': [5, 2, 1], 'B': [100, 300, 500]}), 0, 'B') == 100\n    assert candidate(pd.DataFrame({'A': [5, 2, 1], 'B': [500, 300, 500]}), 0, 'B') == 500\n    assert candidate(pd.DataFrame({'A': [5, 2, 1], 'B': [500, 300, 100]}), 0, 'B') == 500\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "How do I get the value at an nth row in Monkey?",
            "How do I get the value at a given column name in Monkey?",
            "How do I get the value at an nth row of a given column name?",
            "How do I get the value at an nth row of a given column name in Monkey?",
            "How do I get the value at an nth row of a given column name in Monkey? return the value"
        ],
        "gold_APIs": {}
    },
    "PandasEval/15": {
        "original_query": "creating a new knowledgeframe of all same with kf_original one, but no any rows return the new knowledgeframe",
        "retrieved_APIs": {
            "API_1": "clone(self: '_IndexT', name: 'Hashable | None' = None, deep: 'bool' = False, dtype: 'Dtype | None' = None, names: 'Sequence[Hashable] | None' = None) -> '_IndexT': Create a duplicate of this object.",
            "API_2": "adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options.",
            "API_3": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_4": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns.",
            "API_5": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous."
        },
        "code_prefix": "import monkey as mk\ndef creating_kf_with_same_as_other(kf_original):",
        "code_completion": [
            "    kf_clone = kf_original.iloc[:0,:].clone()\n    return kf_clone"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'tail'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]})).equals(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [1, 0, 3], 'B': [100, 300, 500]})).equals(pd.DataFrame({'A': [1, 0, 3], 'B': [100, 300, 500]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [5, 0, 3], 'B': [100, 300, 500]})).equals(pd.DataFrame({'A': [5, 0, 3], 'B': [100, 300, 500]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [5, 2, 3], 'B': [100, 300, 500]})).equals(pd.DataFrame({'A': [5, 2, 3], 'B': [100, 300, 500]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [5, 2, 1], 'B': [100, 300, 500]})).equals(pd.DataFrame({'A': [5, 2, 1], 'B': [100, 300, 500]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [5, 2, 1], 'B': [500, 300, 500]})).equals(pd.DataFrame({'A': [5, 2, 1], 'B': [500, 300, 500]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [5, 2, 1], 'B': [500, 200, 500]})).equals(pd.DataFrame({'A': [5, 2, 1], 'B': [500, 200, 500]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [5, 2, 1], 'B': [500, 200, 100]})).equals(pd.DataFrame({'A': [5, 2, 1], 'B': [500, 200, 100]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [5, 20, 1], 'B': [500, 200, 100]})).equals(pd.DataFrame({'A': [5, 20, 1], 'B': [500, 200, 100]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [50, 20, 1], 'B': [500, 200, 100]})).equals(pd.DataFrame({'A': [50, 20, 1], 'B': [500, 200, 100]}).iloc[:0,:].copy())\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Creating a new knowledgeframe without any rows.",
            "Can you create a new knowledgeframe?"
        ],
        "gold_APIs": {
            "1200011": "clone(self: '_IndexT', name: 'Hashable | None' = None, deep: 'bool' = False, dtype: 'Dtype | None' = None, names: 'Sequence[Hashable] | None' = None) -> '_IndexT':\n        Make a clone of this object.\n\n        Name and dtype sets those attributes on the new object.\n\n        Parameters\n        ----------\n        name : Label, optional\n            Set name for new object.\n        deep : bool, default False\n        dtype : numpy dtype or monkey type, optional\n            Set dtype for new object.\n\n            .. deprecated:: 1.2.0\n                use ``totype`` method instead.\n        names : list-like, optional\n            Kept for compatibility with MultiIndex. Should not be used.\n\n        Returns\n        -------\n        Index\n            Index refer to new object which is a clone of this object.\n\n        Notes\n        -----\n        In most cases, there should be no functional difference from using\n        ``deep``, but if ``deep`` is passed it will attempt to deepclone.\n        "
        }
    },
    "PandasEval/20": {
        "original_query": "What is the best way to do a grouper on a Monkey knowledgeframe, but exclude some columns from that grouper? I want to grouper the column `Country` and `Item_Code` and only compute the sum of the rows falling under the columns ['Y1961', 'Y1962' and 'Y1963'].",
        "retrieved_APIs": {
            "API_1": "grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy': Group the KnowledgeFrame by a set of columns or group keys.",
            "API_2": "total_sum(self, axis=None, skipna=None, level=None, numeric_only=None, getting_min_count=0, **kwargs):\nReturn the total_sum of the values over the requested axis.\n\nThis is equivalengtht to the method ``numpy.total_sum``.\n\nParameters\n----------\naxis : {index (0)}\n    Axis for the function to be applied on.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a scalar.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Collections.\ngetting_min_count : int, default 0\n    The required number of valid values to perform the operation. If fewer than\n    ``getting_min_count`` non-NA values are present the result will be NA.\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nscalar or Collections (if level specified)\n\nSee Also\n--------\nCollections.total_sum : Return the total_sum.\nCollections.getting_min : Return the getting_minimum.\nCollections.getting_max : Return the getting_maximum.\nCollections.idxgetting_min : Return the index of the getting_minimum.\nCollections.idxgetting_max : Return the index of the getting_maximum.\nKnowledgeFrame.total_sum : Return the total_sum over the requested axis.\nKnowledgeFrame.getting_min : Return the getting_minimum over the requested axis.\nKnowledgeFrame.getting_max : Return the getting_maximum over the requested axis.\nKnowledgeFrame.idxgetting_min : Return the index of the getting_minimum over the requested axis.\nKnowledgeFrame.idxgetting_max : Return the index of the getting_maximum over the requested axis.\n\nExamples\n--------\n>>> idx = mk.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = mk.Collections([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64\n\n>>> s.total_sum()\n14\n\nBy default, the total_sum of an empty or total_all-NA Collections is ``0``.\n\n>>> mk.Collections([], dtype=\"float64\").total_sum()  # getting_min_count=0 is the default\n0.0\n\nThis can be controlled with the ``getting_min_count`` parameter. For example, if\nyou'd like the total_sum of an empty collections to be NaN, pass ``getting_min_count=1``.\n\n>>> mk.Collections([], dtype=\"float64\").total_sum(getting_min_count=1)\nnan\n\nThanks to the ``skipna`` parameter, ``getting_min_count`` handles total_all-NA and\nempty collections identictotal_ally.\n\n>>> mk.Collections([np.nan]).total_sum()\n0.0\n\n>>> mk.Collections([np.nan]).total_sum(getting_min_count=1)\nnan\n",
            "API_3": "mapping(self, mapper, na_action=None): Map the object's values according to an input mapping or function.",
            "API_4": "concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion': Concatenate monkey objects along one axis, using set logic on the other axes if needed.",
            "API_5": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type."
        },
        "code_prefix": "import monkey as mk\n\nkf = mk.KnowledgeFrame({\"Code\": [2, 2, 4, 4], \"Country\": [\"Afghanistan\", \"Afghanistan\", \"Angola\", \"Angola\"], \"Item_Code\": [15, 25, 15, 25], \"Y1961\": [10, 10, 30, 30], \"Y1962\": [20, 20, 40, 40], \"Y1963\": [30, 30, 50, 50]})\nnew_kf =",
        "code_completion": [
            " kf.grouper(['Country', 'Item_Code'])[['Y1961', 'Y1962', 'Y1963']].total_sum()"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'groupby_sum'\n}\n\n\ndef check():\n    assert new_df.equals(pd.DataFrame({\"Code\": [2, 2, 4, 4], \"Country\": [\"Afghanistan\", \"Afghanistan\", \"Angola\", \"Angola\"], \"Item_Code\": [15, 25, 15, 25], \"Y1961\": [10, 10, 30, 30], \"Y1962\": [20, 20, 40, 40], \"Y1963\": [30, 30, 50, 50]}).groupby(['Country', 'Item_Code'])[['Y1961', 'Y1962', 'Y1963']].sum())\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "What is the best way to grouper on a Monkey knowledgeframe?",
            "How to exclude some columns from the grouper?",
            "What is the best way to compute the sum of the rows falling under the columns?",
            "How to grouper the column `Country` and `Item_Code`?",
            "What is the best way to do a grouper on a Monkey knowledgeframe?"
        ],
        "gold_APIs": {
            "1200012": "grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy':\nGroup KnowledgeFrame using a mappingper or by a Collections of columns.\n\nA grouper operation involves some combination of splitting the\nobject, employing a function, and combining the results. This can be\nused to group large amounts of data and compute operations on these\ngroups.\n\nParameters\n----------\nby : mappingping, function, label, or list of labels\n    Used to detergetting_mine the groups for the grouper.\n    If ``by`` is a function, it's ctotal_alled on each value of the object's\n    index. If a dict or Collections is passed, the Collections or dict VALUES\n    will be used to detergetting_mine the groups (the Collections' values are first\n    aligned; see ``.align()`` method). If an ndarray is passed, the\n    values are used as-is to detergetting_mine the groups. A label or list of\n    labels may be passed to group by the columns in ``self``. Notice\n    that a tuple is interpreted as a (single) key.\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    Split along rows (0) or columns (1).\nlevel : int, level name, or sequence of such, default None\n    If the axis is a MultiIndex (hierarchical), group by a particular\n    level or levels.\nas_index : bool, default True\n    For aggregated output, return object with group labels as the\n    index. Only relevant for KnowledgeFrame input. as_index=False is\n    effectively \"SQL-style\" grouped output.\nsort : bool, default True\n    Sort group keys. Get better performance by turning this off.\n    Note this does not influence the order of observations within each\n    group. Groupby preserves the order of rows within each group.\ngroup_keys : bool, default True\n    When ctotal_alling employ, add group keys to index to identify pieces.\nsqueeze : bool, default False\n    Reduce the dimensionality of the return type if possible,\n    otherwise return a consistent type.\n\n    .. deprecated:: 1.1.0\n\nobserved : bool, default False\n    This only applies if whatever of the groupers are Categoricals.\n    If True: only show observed values for categorical groupers.\n    If False: show total_all values for categorical groupers.\nsipna : bool, default True\n    If True, and if group keys contain NA values, NA values togettingher\n    with row/column will be sipped.\n    If False, NA values will also be treated as the key in groups\n\n    .. versionadded:: 1.1.0\n\nReturns\n-------\nKnowledgeFrameGroupBy\n    Returns a grouper object that contains informatingion about the groups.\n\nSee Also\n--------\nresample_by_num : Convenience method for frequency conversion and resampling\n    of time collections.\n\nNotes\n-----\nSee the `user guide\n<https://monkey.pydata.org/monkey-docs/stable/grouper.html>`__ for more.\n\nExamples\n--------\n>>> kf = mk.KnowledgeFrame({'Animal': ['Falcon', 'Falcon',\n...                               'Parrot', 'Parrot'],\n...                    'Max Speed': [380., 370., 24., 26.]})\n>>> kf\n   Animal  Max Speed\n0  Falcon      380.0\n1  Falcon      370.0\n2  Parrot       24.0\n3  Parrot       26.0\n>>> kf.grouper(['Animal']).average()\n        Max Speed\nAnimal\nFalcon      375.0\nParrot       25.0\n\n**Hierarchical Indexes**\n\nWe can grouper different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = mk.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> kf = mk.KnowledgeFrame({'Max Speed': [390., 350., 30., 20.]},\n...                   index=index)\n>>> kf\n                Max Speed\nAnimal Type\nFalcon Captive      390.0\n       Wild         350.0\nParrot Captive       30.0\n       Wild          20.0\n>>> kf.grouper(level=0).average()\n        Max Speed\nAnimal\nFalcon      370.0\nParrot       25.0\n>>> kf.grouper(level=\"Type\").average()\n         Max Speed\nType\nCaptive      210.0\nWild         185.0\n\nWe can also choose to include NA in group keys or not by setting\n`sipna` parameter, the default setting is `True`:\n\n>>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\n>>> kf = mk.KnowledgeFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> kf.grouper(by=[\"b\"]).total_sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\n\n>>> kf.grouper(by=[\"b\"], sipna=False).total_sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\nNaN 1   4\n\n>>> l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]]\n>>> kf = mk.KnowledgeFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> kf.grouper(by=\"a\").total_sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\n\n>>> kf.grouper(by=\"a\", sipna=False).total_sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\nNaN 12.3   33.0\n",
            "1200013": "total_sum(self, axis=None, skipna=None, level=None, numeric_only=None, getting_min_count=0, **kwargs):\nReturn the total_sum of the values over the requested axis.\n\nThis is equivalengtht to the method ``numpy.total_sum``.\n\nParameters\n----------\naxis : {index (0)}\n    Axis for the function to be applied on.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a scalar.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Collections.\ngetting_min_count : int, default 0\n    The required number of valid values to perform the operation. If fewer than\n    ``getting_min_count`` non-NA values are present the result will be NA.\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nscalar or Collections (if level specified)\n\nSee Also\n--------\nCollections.total_sum : Return the total_sum.\nCollections.getting_min : Return the getting_minimum.\nCollections.getting_max : Return the getting_maximum.\nCollections.idxgetting_min : Return the index of the getting_minimum.\nCollections.idxgetting_max : Return the index of the getting_maximum.\nKnowledgeFrame.total_sum : Return the total_sum over the requested axis.\nKnowledgeFrame.getting_min : Return the getting_minimum over the requested axis.\nKnowledgeFrame.getting_max : Return the getting_maximum over the requested axis.\nKnowledgeFrame.idxgetting_min : Return the index of the getting_minimum over the requested axis.\nKnowledgeFrame.idxgetting_max : Return the index of the getting_maximum over the requested axis.\n\nExamples\n--------\n>>> idx = mk.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = mk.Collections([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64\n\n>>> s.total_sum()\n14\n\nBy default, the total_sum of an empty or total_all-NA Collections is ``0``.\n\n>>> mk.Collections([], dtype=\"float64\").total_sum()  # getting_min_count=0 is the default\n0.0\n\nThis can be controlled with the ``getting_min_count`` parameter. For example, if\nyou'd like the total_sum of an empty collections to be NaN, pass ``getting_min_count=1``.\n\n>>> mk.Collections([], dtype=\"float64\").total_sum(getting_min_count=1)\nnan\n\nThanks to the ``skipna`` parameter, ``getting_min_count`` handles total_all-NA and\nempty collections identictotal_ally.\n\n>>> mk.Collections([np.nan]).total_sum()\n0.0\n\n>>> mk.Collections([np.nan]).total_sum(getting_min_count=1)\nnan\n"
        }
    },
    "PandasEval/10": {
        "original_query": "creating a Collections from a list [56, 24, 421, 90]",
        "retrieved_APIs": {
            "API_1": "Collections(data=None, index=None, dtype: 'Dtype | None' = None, name=None, clone: 'bool' = False, fastpath: 'bool' = False): ndarray with axis labels in one-dimension (also time collections).",
            "API_2": "adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options.",
            "API_3": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.",
            "API_4": "formating(self, name: 'bool' = False, formatingter: 'Ctotal_allable | None' = None, na_rep: 'str_t' = 'NaN') -> 'list[str_t]': Return the Index as a formatted string.",
            "API_5": "unioner(self, right: 'FrameOrCollectionsUnion', how: 'str' = 'inner', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), clone: 'bool' = True, indicator: 'bool' = False, validate: 'str | None' = None) -> 'KnowledgeFrame': Database-style join the named Collections objects or KnowledgeFrame."
        },
        "code_prefix": "import monkey as mk\n\n\nmy_collections =",
        "code_completion": ["mk.Collections([56, 24, 421, 90])"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Series'\n}\n\n\ndef check():\n    assert my_series.equals(pd.Series([56, 24, 421, 90]))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Creating a Collections from a list",
            "Can you create a Collections from a list?",
            "Can you create a list?",
            "Can you create a Collections?",
            "Can you create from a list?"
        ],
        "gold_APIs": {
            "1200014": "Collections(data=None, index=None, dtype: 'Dtype | None' = None, name=None, clone: 'bool' = False, fastpath: 'bool' = False):\n    One-dimensional ndarray with axis labels (including time collections).\n\n    Labels need not be distinctive but must be a hashable type. The object\n    supports both integer- and label-based indexing and provides a host of\n    methods for perforgetting_ming operations involving the index. Statistical\n    methods from ndarray have been overridden to automatictotal_ally exclude\n    missing data (currently represented as NaN).\n\n    Operations between Collections (+, -, /, *, **) align values based on their\n    associated index values-- they need not be the same lengthgth. The result\n    index will be the sorted union of the two indexes.\n\n    Parameters\n    ----------\n    data : array-like, Iterable, dict, or scalar value\n        Contains data stored in Collections. If data is a dict, argument order is\n        maintained.\n    index : array-like or Index (1d)\n        Values must be hashable and have the same lengthgth as `data`.\n        Non-distinctive index values are total_allowed. Will default to\n        RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n        and index is None, then the keys in the data are used as the index. If the\n        index is not None, the resulting Collections is reindexinged with the index values.\n    dtype : str, numpy.dtype, or ExtensionDtype, optional\n        Data type for the output Collections. If not specified, this will be\n        inferred from `data`.\n        See the :ref:`user guide <basics.dtypes>` for more usages.\n    name : str, optional\n        The name to give to the Collections.\n    clone : bool, default False\n        Copy input data. Only affects Collections or 1d ndarray input. See examples.\n\n    Examples\n    --------\n    Constructing Collections from a dictionary with an Index specified\n\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\n    >>> ser = mk.Collections(data=d, index=['a', 'b', 'c'])\n    >>> ser\n    a   1\n    b   2\n    c   3\n    dtype: int64\n\n    The keys of the dictionary match with the Index values, hence the Index\n    values have no effect.\n\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\n    >>> ser = mk.Collections(data=d, index=['x', 'y', 'z'])\n    >>> ser\n    x   NaN\n    y   NaN\n    z   NaN\n    dtype: float64\n\n    Note that the Index is first build with the keys from the dictionary.\n    After this the Collections is reindexinged with the given Index values, hence we\n    getting total_all NaN as a result.\n\n    Constructing Collections from a list with `clone=False`.\n\n    >>> r = [1, 2]\n    >>> ser = mk.Collections(r, clone=False)\n    >>> ser.iloc[0] = 999\n    >>> r\n    [1, 2]\n    >>> ser\n    0    999\n    1      2\n    dtype: int64\n\n    Due to input data type the Collections has a `clone` of\n    the original data even though `clone=False`, so\n    the data is unchanged.\n\n    Constructing Collections from a 1d ndarray with `clone=False`.\n\n    >>> r = np.array([1, 2])\n    >>> ser = mk.Collections(r, clone=False)\n    >>> ser.iloc[0] = 999\n    >>> r\n    array([999,   2])\n    >>> ser\n    0    999\n    1      2\n    dtype: int64\n\n    Due to input data type the Collections has a `view` on\n    the original data, so\n    the data is changed as well.\n    "
        }
    },
    "PandasEval/16": {
        "original_query": "What I want is to clip the values of `col_1` between -2 to 2 if `col_0` is `a`. Using `clip` function in monkey.",
        "retrieved_APIs": {
            "API_1": "concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion': Concatenate monkey objects along one axis, using set logic on the other axes if needed.",
            "API_2": "mapping(self, mapper, na_action=None): Map the object's values according to an input mapping or function.",
            "API_3": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_4": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_5": "convert_pydatetime(*args, **kwargs): Return the native datetime object in Python."
        },
        "code_prefix": "import monkey as mk\n\ndata = {'col_0': ['a', 'a', 'a', 'a', 'b','b','b'], 'col_1': [-2, -7, 6, 8, -5, 2, 6]}\nkf = mk.KnowledgeFrame(data)\nkf.loc[kf['col_0']=='a','col_1'] =",
        "code_completion": [" kf.loc[kf['col_0']=='a','col_1'].clip(-2,2)"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'clip'\n}\n\n\ndef check():\n    assert df.equals(pd.DataFrame({'col_0': ['a', 'a', 'a', 'a', 'b','b','b'], 'col_1': [-2, -2, 2, 2, -5, 2, 6]}))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "What I want is to clip the values of `col_1` if `col_0` is `a`.",
            "Using `clip` function in monkey, how can I clip the values of `col_1` between -2 to 2?",
            "How can I clip the values of `col_1` between -2 to 2 if `col_0` is `a`?"
        ],
        "gold_APIs": {}
    },
    "PandasEval/17": {
        "original_query": "I would like to create new knowledgeframe out of the old one in a way that there will only be values that exceed the average value of the column. We can compare values and then add NaNs by indexing or `where` We want remove NaNs also in first rows add custom function with `sipna`",
        "retrieved_APIs": {
            "API_1": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_2": "average(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs):\nReturn the average of the values over the requested axis.\n\nParameters\n----------\naxis : {index (0)}\n    Axis for the function to be applied on.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a scalar.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Collections.\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nscalar or Collections (if level specified)\n",
            "API_3": "Collections(data=None, index=None, dtype: 'Dtype | None' = None, name=None, clone: 'bool' = False, fastpath: 'bool' = False):\n    One-dimensional ndarray with axis labels (including time collections).\n\n    Labels need not be distinctive but must be a hashable type. The object\n    supports both integer- and label-based indexing and provides a host of\n    methods for perforgetting_ming operations involving the index. Statistical\n    methods from ndarray have been overridden to automatictotal_ally exclude\n    missing data (currently represented as NaN).\n\n    Operations between Collections (+, -, /, *, **) align values based on their\n    associated index values-- they need not be the same lengthgth. The result\n    index will be the sorted union of the two indexes.\n\n    Parameters\n    ----------\n    data : array-like, Iterable, dict, or scalar value\n        Contains data stored in Collections. If data is a dict, argument order is\n        maintained.\n    index : array-like or Index (1d)\n        Values must be hashable and have the same lengthgth as `data`.\n        Non-distinctive index values are total_allowed. Will default to\n        RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n        and index is None, then the keys in the data are used as the index. If the\n        index is not None, the resulting Collections is reindexinged with the index values.\n    dtype : str, numpy.dtype, or ExtensionDtype, optional\n        Data type for the output Collections. If not specified, this will be\n        inferred from `data`.\n        See the :ref:`user guide <basics.dtypes>` for more usages.\n    name : str, optional\n        The name to give to the Collections.\n    clone : bool, default False\n        Copy input data. Only affects Collections or 1d ndarray input. See examples.\n\n    Examples\n    --------\n    Constructing Collections from a dictionary with an Index specified\n\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\n    >>> ser = mk.Collections(data=d, index=['a', 'b', 'c'])\n    >>> ser\n    a   1\n    b   2\n    c   3\n    dtype: int64\n\n    The keys of the dictionary match with the Index values, hence the Index\n    values have no effect.\n\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\n    >>> ser = mk.Collections(data=d, index=['x', 'y', 'z'])\n    >>> ser\n    x   NaN\n    y   NaN\n    z   NaN\n    dtype: float64\n\n    Note that the Index is first build with the keys from the dictionary.\n    After this the Collections is reindexinged with the given Index values, hence we\n    getting total_all NaN as a result.\n\n    Constructing Collections from a list with `clone=False`.\n\n    >>> r = [1, 2]\n    >>> ser = mk.Collections(r, clone=False)\n    >>> ser.iloc[0] = 999\n    >>> r\n    [1, 2]\n    >>> ser\n    0    999\n    1      2\n    dtype: int64\n\n    Due to input data type the Collections has a `clone` of\n    the original data even though `clone=False`, so\n    the data is unchanged.\n\n    Constructing Collections from a 1d ndarray with `clone=False`.\n\n    >>> r = np.array([1, 2])\n    >>> ser = mk.Collections(r, clone=False)\n    >>> ser.iloc[0] = 999\n    >>> r\n    array([999,   2])\n    >>> ser\n    0    999\n    1      2\n    dtype: int64\n\n    Due to input data type the Collections has a `view` on\n    the original data, so\n    the data is changed as well.\n    ",
            "API_4": "sipna(self):\n        Return ExtensionArray without NA values.\n\n        Returns\n        -------\n        valid : ExtensionArray\n        ",
            "API_5": "fillnone(self, value=None, downcast=None): Use the provided value to fill NA/NaN values."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\nkf = mk.KnowledgeFrame({'a': [4, 1, 7, 3], 'b': [5, 2, 9, 6], 'c': [6, 3, 2, 8]})\nkf =",
        "code_completion": [
            "kf[kf>kf.average()].employ(lambda x: mk.Collections(x.sipna().values))"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'mean_apply'\n}\n\n\ndef check():\n    assert df.equals(pd.DataFrame({'a': [4.0, 7.0], 'b': [9.0, 6.0], 'c': [6.0, 8.0]}))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "I would like to create new knowledgeframe out of the old one in a way that there will only be values that exceed the average value of the column.",
            "We can compare values and then add NaNs by indexing or `where`.",
            "We want remove NaNs also in first rows add custom function with `sipna`."
        ],
        "gold_APIs": {
            "1200000": "average(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs):\nReturn the average of the values over the requested axis.\n\nParameters\n----------\naxis : {index (0)}\n    Axis for the function to be applied on.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a scalar.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Collections.\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nscalar or Collections (if level specified)\n",
            "1200005": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs):\n        Apply a function along an axis of the KnowledgeFrame.\n\n        Objects passed to the function are Collections objects whose index is\n        either the KnowledgeFrame's index (``axis=0``) or the KnowledgeFrame's columns\n        (``axis=1``). By default (``result_type=None``), the final return type\n        is inferred from the return type of the applied function. Otherwise,\n        it depends on the `result_type` argument.\n\n        Parameters\n        ----------\n        func : function\n            Function to employ to each column or row.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the function is applied:\n\n            * 0 or 'index': employ function to each column.\n            * 1 or 'columns': employ function to each row.\n\n        raw : bool, default False\n            Detergetting_mines if row or column is passed as a Collections or ndarray object:\n\n            * ``False`` : passes each row or column as a Collections to the\n              function.\n            * ``True`` : the passed function will receive ndarray objects\n              instead.\n              If you are just employing a NumPy reduction function this will\n              achieve much better performance.\n\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\n            These only act when ``axis=1`` (columns):\n\n            * 'expand' : list-like results will be turned into columns.\n            * 'reduce' : returns a Collections if possible rather than expanding\n              list-like results. This is the opposite of 'expand'.\n            * 'broadcast' : results will be broadcast to the original shape\n              of the KnowledgeFrame, the original index and columns will be\n              retained.\n\n            The default behaviour (None) depends on the return value of the\n            applied function: list-like results will be returned as a Collections\n            of those. However if the employ function returns a Collections these\n            are expanded to columns.\n        args : tuple\n            Positional arguments to pass to `func` in addition to the\n            array/collections.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        Collections or KnowledgeFrame\n            Result of employing ``func`` along the given axis of the\n            KnowledgeFrame.\n\n        See Also\n        --------\n        KnowledgeFrame.employmapping: For elementwise operations.\n        KnowledgeFrame.aggregate: Only perform aggregating type operations.\n        KnowledgeFrame.transform: Only perform transforgetting_ming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.ukf-mutation`\n        for more definal_item_tails.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([[4, 9]] * 3, columns=['A', 'B'])\n        >>> kf\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n\n        Using a numpy universal function (in this case the same as\n        ``np.sqrt(kf)``):\n\n        >>> kf.employ(np.sqrt)\n             A    B\n        0  2.0  3.0\n        1  2.0  3.0\n        2  2.0  3.0\n\n        Using a reducing function on either axis\n\n        >>> kf.employ(np.total_sum, axis=0)\n        A    12\n        B    27\n        dtype: int64\n\n        >>> kf.employ(np.total_sum, axis=1)\n        0    13\n        1    13\n        2    13\n        dtype: int64\n\n        Returning a list-like will result in a Collections\n\n        >>> kf.employ(lambda x: [1, 2], axis=1)\n        0    [1, 2]\n        1    [1, 2]\n        2    [1, 2]\n        dtype: object\n\n        Passing ``result_type='expand'`` will expand list-like results\n        to columns of a Dataframe\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='expand')\n           0  1\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returning a Collections inside the function is similar to passing\n        ``result_type='expand'``. The resulting column names\n        will be the Collections index.\n\n        >>> kf.employ(lambda x: mk.Collections([1, 2], index=['foo', 'bar']), axis=1)\n           foo  bar\n        0    1    2\n        1    1    2\n        2    1    2\n\n        Passing ``result_type='broadcast'`` will ensure the same shape\n        result, whether list-like or scalar is returned by the function,\n        and broadcast it along the axis. The resulting column names will\n        be the originals.\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='broadcast')\n           A  B\n        0  1  2\n        1  1  2\n        2  1  2\n        ",
            "1200007": "sipna(self):\n        Return ExtensionArray without NA values.\n\n        Returns\n        -------\n        valid : ExtensionArray\n        ",
            "1200014": "Collections(data=None, index=None, dtype: 'Dtype | None' = None, name=None, clone: 'bool' = False, fastpath: 'bool' = False):\n    One-dimensional ndarray with axis labels (including time collections).\n\n    Labels need not be distinctive but must be a hashable type. The object\n    supports both integer- and label-based indexing and provides a host of\n    methods for perforgetting_ming operations involving the index. Statistical\n    methods from ndarray have been overridden to automatictotal_ally exclude\n    missing data (currently represented as NaN).\n\n    Operations between Collections (+, -, /, *, **) align values based on their\n    associated index values-- they need not be the same lengthgth. The result\n    index will be the sorted union of the two indexes.\n\n    Parameters\n    ----------\n    data : array-like, Iterable, dict, or scalar value\n        Contains data stored in Collections. If data is a dict, argument order is\n        maintained.\n    index : array-like or Index (1d)\n        Values must be hashable and have the same lengthgth as `data`.\n        Non-distinctive index values are total_allowed. Will default to\n        RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n        and index is None, then the keys in the data are used as the index. If the\n        index is not None, the resulting Collections is reindexinged with the index values.\n    dtype : str, numpy.dtype, or ExtensionDtype, optional\n        Data type for the output Collections. If not specified, this will be\n        inferred from `data`.\n        See the :ref:`user guide <basics.dtypes>` for more usages.\n    name : str, optional\n        The name to give to the Collections.\n    clone : bool, default False\n        Copy input data. Only affects Collections or 1d ndarray input. See examples.\n\n    Examples\n    --------\n    Constructing Collections from a dictionary with an Index specified\n\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\n    >>> ser = mk.Collections(data=d, index=['a', 'b', 'c'])\n    >>> ser\n    a   1\n    b   2\n    c   3\n    dtype: int64\n\n    The keys of the dictionary match with the Index values, hence the Index\n    values have no effect.\n\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\n    >>> ser = mk.Collections(data=d, index=['x', 'y', 'z'])\n    >>> ser\n    x   NaN\n    y   NaN\n    z   NaN\n    dtype: float64\n\n    Note that the Index is first build with the keys from the dictionary.\n    After this the Collections is reindexinged with the given Index values, hence we\n    getting total_all NaN as a result.\n\n    Constructing Collections from a list with `clone=False`.\n\n    >>> r = [1, 2]\n    >>> ser = mk.Collections(r, clone=False)\n    >>> ser.iloc[0] = 999\n    >>> r\n    [1, 2]\n    >>> ser\n    0    999\n    1      2\n    dtype: int64\n\n    Due to input data type the Collections has a `clone` of\n    the original data even though `clone=False`, so\n    the data is unchanged.\n\n    Constructing Collections from a 1d ndarray with `clone=False`.\n\n    >>> r = np.array([1, 2])\n    >>> ser = mk.Collections(r, clone=False)\n    >>> ser.iloc[0] = 999\n    >>> r\n    array([999,   2])\n    >>> ser\n    0    999\n    1      2\n    dtype: int64\n\n    Due to input data type the Collections has a `view` on\n    the original data, so\n    the data is changed as well.\n    "
        }
    },
    "PandasEval/18": {
        "original_query": "Appending the source collections to the target collections, with ignoring the index or resetting index",
        "retrieved_APIs": {
            "API_1": "adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options.",
            "API_2": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.",
            "API_3": "reseting_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, sip: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'KnowledgeFrame | None': Reset the index of the KnowledgeFrame, and use the default one instead.",
            "API_4": "remove_duplicates(self: '_IndexT', keep: 'str_t | bool' = 'first') -> '_IndexT': Remove the duplicate values of the Index.",
            "API_5": "reindexing(self, target, method=None, level=None, limit=None, tolerance=None) -> 'tuple[MultiIndex, np.ndarray | None]': Create an index conditioned on the values of target (move, add, or remove values as needed)."
        },
        "code_prefix": "import monkey as mk\n\nsource_collections = mk.Collections([32, 434, 542, 'BC2'])\ntarget_collections = mk.Collections(['B1', 'B3', 'B4', 123, 43, 54])\n\n\nunionerd_collections =",
        "code_completion": [
            "target_collections.adding(source_collections, ignore_index=True)"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'append'\n}\n\n\ndef check():\n    assert merged_series.equals(pd.Series(['B1', 'B3', 'B4', 123, 43, 54, 32, 434, 542, 'BC2']))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Appending the collections to the target collections, with ignoring the index or resetting index?",
            "What happens when you append the source collections to the target collections, with ignoring the index or resetting index?",
            "What is the purpose of appending the source collections to the target collections, with ignoring the index or resetting index?",
            "Can you explain the process of appending the source collections to the target collections, with ignoring the index or resetting index?",
            "Is it possible to append the source collections to the target collections, with ignoring the index or resetting index?"
        ],
        "gold_APIs": {
            "1200009": "adding(self, other: 'Index | Sequence[Index]') -> 'Index':\n        Append a collection of Index options togettingher.\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n\n        Returns\n        -------\n        Index\n        "
        }
    },
    "PandasEval/19": {
        "original_query": "Selecting rows where column x2 is NaN",
        "retrieved_APIs": {
            "API_1": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_2": "ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.",
            "API_3": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_4": "incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values.",
            "API_5": "adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\nkf = mk.KnowledgeFrame({'group1': [0, 0, 1, 1], 'group2': [2, 2, 3, 4], 'base': [0, 1, 2, 3], 'x1': [3, 4, 5, 6], 'x2': [np.nan, 6, np.nan, 8]})\n\n \nnan_kf =",
        "code_completion": [" kf[kf['x2'].ifnull()]"],
        "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'isnull_isnan'\n}\n\n\ndef check():\n    assert nan_df.equals(pd.DataFrame({'group1': [0, 1], 'group2': [2, 3], 'base': [0, 2], 'x1': [3, 5], 'x2': [np.nan, np.nan]}, index=[0, 2]))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Selecting rows where column is NaN",
            "Selecting rows where x2 is NaN"
        ],
        "gold_APIs": {
            "1200015": "ifnull(self) -> 'np.ndarray':\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`mk.NaT`, getting\n        mappingped to ``True`` values.\n        Everything else getting mappingped to ``False`` values. Characters such as\n        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``monkey.options.mode.use_inf_as_na = True``).\n\n        Returns\n        -------\n        numpy.ndarray[bool]\n            A boolean array of whether my values are NA.\n\n        See Also\n        --------\n        Index.notna : Boolean inverse of ifna.\n        Index.sipna : Omit entries with missing values.\n        ifna : Top-level ifna.\n        Collections.ifna : Detect missing values in Collections object.\n\n        Examples\n        --------\n        Show which entries in a monkey.Index are NA. The result is an\n        array.\n\n        >>> idx = mk.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.ifna()\n        array([False, False,  True])\n\n        Empty strings are not considered NA values. None is considered an NA\n        value.\n\n        >>> idx = mk.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.ifna()\n        array([False, False, False,  True])\n\n        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n\n        >>> idx = mk.DatetimeIndex([mk.Timestamp('1940-04-25'),\n        ...                         mk.Timestamp(''), None, mk.NaT])\n        >>> idx\n        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n                      dtype='datetime64[ns]', freq=None)\n        >>> idx.ifna()\n        array([False,  True,  True,  True])\n        "
        }
    },
    "PandasEval/21": {
        "original_query": "I want to convert a table, represented as a list of lists, into a monkey KnowledgeFrame. The columns are ['one', 'two'] What is the best way to convert the columns to the appropriate types, in this case the 'two' column into floats?",
        "retrieved_APIs": {
            "API_1": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.",
            "API_2": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_3": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_4": "mapping(self, mapper, na_action=None): Map the object's values according to an input mapping or function.",
            "API_5": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes."
        },
        "code_prefix": "import monkey as mk\n\na = [['a', '1.2'], ['b', '70'], ['x', '5']]\nkf =",
        "code_completion": [
            " mk.KnowledgeFrame(a, columns=['one', 'two'])\nkf['two'] = kf['two'].totype(float)"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'columns_astype'\n}\n\n\ndef check():\n    assert df.equals(pd.DataFrame({'one': ['a', 'b', 'x'], 'two': [1.2, 70.0, 5.0]}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "What is the best way to convert the columns to the appropriate types?",
            "What is the best way to convert the columns?",
            "What is the best way to convert the table?",
            "What is the best way to convert the table into a KnowledgeFrame?",
            "What is the best way to convert the table into a KnowledgeFrame with appropriate types?"
        ],
        "gold_APIs": {
            "1200008": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None):\n    Two-dimensional, size-mutable, potentitotal_ally heterogeneous tabular data.\n\n    Data structure also contains labeled axes (rows and columns).\n    Arithmetic operations align on both row and column labels. Can be\n    thought of as a dict-like container for Collections objects. The primary\n    monkey data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or KnowledgeFrame\n        Dict can contain Collections, arrays, constants, dataclass or list-like objects. If\n        data is a dict, column order follows insertion-order.\n\n        .. versionchanged:: 0.25.0\n           If data is a list of dicts, column order follows insertion-order.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing informatingion part of input data and no index provided.\n    columns : Index or array-like\n        Column labels to use for resulting frame when data does not have them,\n        defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n        will perform column selection instead.\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is total_allowed. If None, infer.\n    clone : bool or None, default None\n        Copy data from inputs.\n        For dict data, the default of None behaves like ``clone=True``.  For KnowledgeFrame\n        or 2d ndarray input, the default of None behaves like ``clone=False``.\n\n        .. versionchanged:: 1.3.0\n\n    See Also\n    --------\n    KnowledgeFrame.from_records : Constructor from tuples, also record arrays.\n    KnowledgeFrame.from_dict : From dicts of Collections, arrays, or dicts.\n    read_csv : Read a comma-separated values (csv) file into KnowledgeFrame.\n    read_table : Read general delimited file into KnowledgeFrame.\n    read_clipboard : Read text from clipboard into KnowledgeFrame.\n\n    Examples\n    --------\n    Constructing KnowledgeFrame from a dictionary.\n\n    >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n    >>> kf = mk.KnowledgeFrame(data=d)\n    >>> kf\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> kf.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> kf = mk.KnowledgeFrame(data=d, dtype=np.int8)\n    >>> kf.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing KnowledgeFrame from numpy ndarray:\n\n    >>> kf2 = mk.KnowledgeFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n    ...                    columns=['a', 'b', 'c'])\n    >>> kf2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n\n    Constructing KnowledgeFrame from a numpy ndarray that has labeled columns:\n\n    >>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n    ...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n    >>> kf3 = mk.KnowledgeFrame(data, columns=['c', 'a'])\n    ...\n    >>> kf3\n       c  a\n    0  3  1\n    1  6  4\n    2  9  7\n\n    Constructing KnowledgeFrame from dataclass:\n\n    >>> from dataclasses import make_dataclass\n    >>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n    >>> mk.KnowledgeFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n       x  y\n    0  0  0\n    1  0  3\n    2  2  3\n    ",
            "1200016": "totype(self, dtype: 'Dtype | None' = None, clone=True):\n        Change the dtype of a SparseArray.\n\n        The output will always be a SparseArray. To convert to a dense\n        ndarray with a certain dtype, use :meth:`numpy.asarray`.\n\n        Parameters\n        ----------\n        dtype : np.dtype or ExtensionDtype\n            For SparseDtype, this changes the dtype of\n            ``self.sp_values`` and the ``self.fill_value``.\n\n            For other dtypes, this only changes the dtype of\n            ``self.sp_values``.\n\n        clone : bool, default True\n            Whether to ensure a clone is made, even if not necessary.\n\n        Returns\n        -------\n        SparseArray\n\n        Examples\n        --------\n        >>> arr = mk.arrays.SparseArray([0, 0, 1, 2])\n        >>> arr\n        [0, 0, 1, 2]\n        Fill: 0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        >>> arr.totype(np.dtype('int32'))\n        [0, 0, 1, 2]\n        Fill: 0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        Using a NumPy dtype with a different kind (e.g. float) will coerce\n        just ``self.sp_values``.\n\n        >>> arr.totype(np.dtype('float64'))\n        ... # doctest: +NORMALIZE_WHITESPACE\n        [0.0, 0.0, 1.0, 2.0]\n        Fill: 0.0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        Use a SparseDtype if you wish to be change the fill value as well.\n\n        >>> arr.totype(SparseDtype(\"float64\", fill_value=np.nan))\n        ... # doctest: +NORMALIZE_WHITESPACE\n        [nan, nan, 1.0, 2.0]\n        Fill: nan\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n        "
        }
    },
    "PandasEval/22": {
        "original_query": "I need to change the dtype of multiple columns but the knowledgeframe has different kind of dtypes. Some columns dtypes are float64 whereas some columns are int64 I need to change all float64 to float32.",
        "retrieved_APIs": {
            "API_1": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.",
            "API_4": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_5": "choose_dtypes(self, include=None, exclude=None) -> 'KnowledgeFrame': Extract a collection of colums from the KnowledgeFrame based on their dtypes."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\nmy_kf = mk.KnowledgeFrame({'col1': [1,2,3], 'col2': [1.0,2.0,3.0]})\ncols =",
        "code_completion": [
            " my_kf.choose_dtypes(include=['float64']).columns\nmy_kf[cols] = my_kf[cols].totype(np.float32)"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'astype'\n}\n\n\ndef check():\n    assert my_df.equals(pd.DataFrame({'col1': [1,2,3], 'col2': [np.float32(1.0),np.float32(2.0),np.float32(3.0)]}))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "I need to change the dtype of multiple columns but the knowledgeframe has different kind of dtypes.",
            "I need to change all float64 to float32."
        ],
        "gold_APIs": {
            "1200016": "totype(self, dtype: 'Dtype | None' = None, clone=True):\n        Change the dtype of a SparseArray.\n\n        The output will always be a SparseArray. To convert to a dense\n        ndarray with a certain dtype, use :meth:`numpy.asarray`.\n\n        Parameters\n        ----------\n        dtype : np.dtype or ExtensionDtype\n            For SparseDtype, this changes the dtype of\n            ``self.sp_values`` and the ``self.fill_value``.\n\n            For other dtypes, this only changes the dtype of\n            ``self.sp_values``.\n\n        clone : bool, default True\n            Whether to ensure a clone is made, even if not necessary.\n\n        Returns\n        -------\n        SparseArray\n\n        Examples\n        --------\n        >>> arr = mk.arrays.SparseArray([0, 0, 1, 2])\n        >>> arr\n        [0, 0, 1, 2]\n        Fill: 0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        >>> arr.totype(np.dtype('int32'))\n        [0, 0, 1, 2]\n        Fill: 0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        Using a NumPy dtype with a different kind (e.g. float) will coerce\n        just ``self.sp_values``.\n\n        >>> arr.totype(np.dtype('float64'))\n        ... # doctest: +NORMALIZE_WHITESPACE\n        [0.0, 0.0, 1.0, 2.0]\n        Fill: 0.0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        Use a SparseDtype if you wish to be change the fill value as well.\n\n        >>> arr.totype(SparseDtype(\"float64\", fill_value=np.nan))\n        ... # doctest: +NORMALIZE_WHITESPACE\n        [nan, nan, 1.0, 2.0]\n        Fill: nan\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n        ",
            "1200017": "choose_dtypes(self, include=None, exclude=None) -> 'KnowledgeFrame':\n        Return a subset of the KnowledgeFrame's columns based on the column dtypes.\n\n        Parameters\n        ----------\n        include, exclude : scalar or list-like\n            A selection of dtypes or strings to be included/excluded. At least\n            one of these parameters must be supplied.\n\n        Returns\n        -------\n        KnowledgeFrame\n            The subset of the frame including the dtypes in ``include`` and\n            excluding the dtypes in ``exclude``.\n\n        Raises\n        ------\n        ValueError\n            * If both of ``include`` and ``exclude`` are empty\n            * If ``include`` and ``exclude`` have overlapping elements\n            * If whatever kind of string dtype is passed in.\n\n        See Also\n        --------\n        KnowledgeFrame.dtypes: Return Collections with the data type of each column.\n\n        Notes\n        -----\n        * To select total_all *numeric* types, use ``np.number`` or ``'number'``\n        * To select strings you must use the ``object`` dtype, but note that\n          this will return *total_all* object dtype columns\n        * See the `numpy dtype hierarchy\n          <https://numpy.org/doc/stable/reference/arrays.scalars.html>`__\n        * To select datetimes, use ``np.datetime64``, ``'datetime'`` or\n          ``'datetime64'``\n        * To select timedeltas, use ``np.timedelta64``, ``'timedelta'`` or\n          ``'timedelta64'``\n        * To select Monkey categorical dtypes, use ``'category'``\n        * To select Monkey datetimetz dtypes, use ``'datetimetz'`` (new in\n          0.20.0) or ``'datetime64[ns, tz]'``\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame({'a': [1, 2] * 3,\n        ...                    'b': [True, False] * 3,\n        ...                    'c': [1.0, 2.0] * 3})\n        >>> kf\n                a      b  c\n        0       1   True  1.0\n        1       2  False  2.0\n        2       1   True  1.0\n        3       2  False  2.0\n        4       1   True  1.0\n        5       2  False  2.0\n\n        >>> kf.choose_dtypes(include='bool')\n           b\n        0  True\n        1  False\n        2  True\n        3  False\n        4  True\n        5  False\n\n        >>> kf.choose_dtypes(include=['float64'])\n           c\n        0  1.0\n        1  2.0\n        2  1.0\n        3  2.0\n        4  1.0\n        5  2.0\n\n        >>> kf.choose_dtypes(exclude=['int64'])\n               b    c\n        0   True  1.0\n        1  False  2.0\n        2   True  1.0\n        3  False  2.0\n        4   True  1.0\n        5  False  2.0\n        "
        }
    },
    "PandasEval/23": {
        "original_query": "I have a knowledgeframe that has two columns, the second column is one of only a few values. I want to return a knowledgeframe where only the rows where that col2 had a specific value 'Jimmy' are included.",
        "retrieved_APIs": {
            "API_1": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_2": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.",
            "API_3": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_4": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_5": "reindexing(self, target, method=None, level=None, limit=None, tolerance=None) -> 'tuple[MultiIndex, np.ndarray | None]': Create an index conditioned on the values of target (move, add, or remove values as needed)."
        },
        "code_prefix": "import monkey as mk\nkf = mk.KnowledgeFrame({'col1': [1,2,3], 'col2': ['Jimmy','Tom','Jimmy']})\nnew_kf =",
        "code_completion": [" kf[kf.iloc[:, 1] == 'Jimmy']"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'iloc'\n}\n\n\ndef check():\n    assert new_df.equals(pd.DataFrame({'col1': [1, 3], 'col2': ['Jimmy', 'Jimmy']}, index=[0, 2]))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "I have a knowledgeframe that has two columns, the second column is one of only a few values. I want to return a knowledgeframe where only the rows where that col2 had a value are included.",
            "I have a knowledgeframe that has two columns, the second column is one of only a few values. I want to return a knowledgeframe where only the rows where that col2 had a specific value are included."
        ],
        "gold_APIs": {}
    },
    "PandasEval/24": {
        "original_query": "kf = kf.reseting_index() make sure indexes pair with number of rows (for index, row in KnowledgeFrame.traversal) is a generator which yields both the index and row (as a Collections) for each row in the KnowledgeFrame, we need put the row['MSRA'] (as key) and row['THU'] (as value) into a rows_dict rows_dict = {} {MSRA: THU, ...}",
        "retrieved_APIs": {
            "API_1": "traversal(self) -> 'Iterable[tuple[Hashable, Collections]]': Return the rows of the KnowledgeFrame organized in (index, Collections) pairs.",
            "API_2": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.",
            "API_3": "reindexing(self, target, method=None, level=None, limit=None, tolerance=None) -> 'tuple[MultiIndex, np.ndarray | None]': Create an index conditioned on the values of target (move, add, or remove values as needed).",
            "API_4": "sorting_index(self, axis: 'Axis' = 0, level: 'Level | None' = None, ascending: 'bool | int | Sequence[bool | int]' = True, inplace: 'bool' = False, kind: 'str' = 'quicksort', na_position: 'str' = 'final_item', sort_remaining: 'bool' = True, ignore_index: 'bool' = False, key: 'IndexKeyFunc' = None): Return object sorted by labels along the specified axis.",
            "API_5": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes."
        },
        "code_prefix": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'MSRA': [10, 11, 12], 'THU': [100, 110, 120]})\nkf = kf.reseting_index()  rows_dict = {}",
        "code_completion": [
            "for index, row in kf.traversal():\n    rows_dict[row['MSRA']] = row['THU']"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'iterrows'\n}\n\n\ndef check():\n    assert rows_dict == {10: 100, 11: 110, 12: 120}\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "kf = kf.reseting_index() make sure indexes pair with number of rows?",
            "What is the generator that yields both the index and row for each row in the KnowledgeFrame?",
            "What do we need to put into rows_dict?",
            "What are the keys and values for rows_dict?",
            "What is the purpose of kf.reseting_index()?"
        ],
        "gold_APIs": {
            "1200018": "traversal(self) -> 'Iterable[tuple[Hashable, Collections]]':\n        Iterate over KnowledgeFrame rows as (index, Collections) pairs.\n\n        Yields\n        ------\n        index : label or tuple of label\n            The index of the row. A tuple for a `MultiIndex`.\n        data : Collections\n            The data of the row as a Collections.\n\n        See Also\n        --------\n        KnowledgeFrame.itertuples : Iterate over KnowledgeFrame rows as namedtuples of the values.\n        KnowledgeFrame.items : Iterate over (column name, Collections) pairs.\n\n        Notes\n        -----\n        1. Because ``traversal`` returns a Collections for each row,\n           it does **not** preserve dtypes across the rows (dtypes are\n           preserved across columns for KnowledgeFrames). For example,\n\n           >>> kf = mk.KnowledgeFrame([[1, 1.5]], columns=['int', 'float'])\n           >>> row = next(kf.traversal())[1]\n           >>> row\n           int      1.0\n           float    1.5\n           Name: 0, dtype: float64\n           >>> print(row['int'].dtype)\n           float64\n           >>> print(kf['int'].dtype)\n           int64\n\n           To preserve dtypes while iterating over the rows, it is better\n           to use :meth:`itertuples` which returns namedtuples of the values\n           and which is genertotal_ally faster than ``traversal``.\n\n        2. You should **never modify** something you are iterating over.\n           This is not guaranteed to work in total_all cases. Depending on the\n           data types, the iterator returns a clone and not a view, and writing\n           to it will have no effect.\n        "
        }
    },
    "PandasEval/25": {
        "original_query": "I have a knowledgeframe in monkey where each column has different value range. Any idea how I can normalize the columns of this knowledgeframe where each value is between 0 and 1?",
        "retrieved_APIs": {
            "API_1": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_2": "get_min(self, *, skipna=True, **kwargs):\n        The getting_minimum value of the object.\n\n        Only ordered `Categoricals` have a getting_minimum!\n\n        .. versionchanged:: 1.0.0\n\n           Returns an NA value on empty arrays\n\n        Raises\n        ------\n        TypeError\n            If the `Categorical` is not `ordered`.\n\n        Returns\n        -------\n        getting_min : the getting_minimum of this `Categorical`\n        ",
            "API_3": "get_max(self, axis=None, skipna: 'bool' = True, *args, **kwargs):\n        Return the getting_maximum value of the Index.\n\n        Parameters\n        ----------\n        axis : int, optional\n            For compatibility with NumPy. Only 0 or None are total_allowed.\n        skipna : bool, default True\n            Exclude NA/null values when showing the result.\n        *args, **kwargs\n            Additional arguments and keywords for compatibility with NumPy.\n\n        Returns\n        -------\n        scalar\n            Maximum value.\n\n        See Also\n        --------\n        Index.getting_min : Return the getting_minimum value in an Index.\n        Collections.getting_max : Return the getting_maximum value in a Collections.\n        KnowledgeFrame.getting_max : Return the getting_maximum values in a KnowledgeFrame.\n\n        Examples\n        --------\n        >>> idx = mk.Index([3, 2, 1])\n        >>> idx.getting_max()\n        3\n\n        >>> idx = mk.Index(['c', 'b', 'a'])\n        >>> idx.getting_max()\n        'c'\n\n        For a MultiIndex, the getting_maximum is detergetting_mined lexicographictotal_ally.\n\n        >>> idx = mk.MultiIndex.from_product([('a', 'b'), (2, 1)])\n        >>> idx.getting_max()\n        ('b', 2)\n        ",
            "API_4": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_5": "reindexing(self, target, method=None, level=None, limit=None, tolerance=None) -> 'tuple[MultiIndex, np.ndarray | None]': Create an index conditioned on the values of target (move, add, or remove values as needed)."
        },
        "code_prefix": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'A': [1000, 765, 800], 'B': [10, 5, 7]})\nnormalized_kf =",
        "code_completion": [
            " kf.employ(lambda x: (x - x.get_min()) / (x.get_max() - x.get_min()))"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'min_max'\n}\n\n\ndef check():\n    assert normalized_df.equals(df.apply(lambda x: (x - x.min()) / (x.max() - x.min())))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Any idea how to normalize the columns of this knowledgeframe?",
            "How can I normalize the columns of this knowledgeframe?",
            "Any idea how to normalize the columns of a knowledgeframe?",
            "How can I normalize the columns of a knowledgeframe?"
        ],
        "gold_APIs": {
            "1200005": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs):\n        Apply a function along an axis of the KnowledgeFrame.\n\n        Objects passed to the function are Collections objects whose index is\n        either the KnowledgeFrame's index (``axis=0``) or the KnowledgeFrame's columns\n        (``axis=1``). By default (``result_type=None``), the final return type\n        is inferred from the return type of the applied function. Otherwise,\n        it depends on the `result_type` argument.\n\n        Parameters\n        ----------\n        func : function\n            Function to employ to each column or row.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the function is applied:\n\n            * 0 or 'index': employ function to each column.\n            * 1 or 'columns': employ function to each row.\n\n        raw : bool, default False\n            Detergetting_mines if row or column is passed as a Collections or ndarray object:\n\n            * ``False`` : passes each row or column as a Collections to the\n              function.\n            * ``True`` : the passed function will receive ndarray objects\n              instead.\n              If you are just employing a NumPy reduction function this will\n              achieve much better performance.\n\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\n            These only act when ``axis=1`` (columns):\n\n            * 'expand' : list-like results will be turned into columns.\n            * 'reduce' : returns a Collections if possible rather than expanding\n              list-like results. This is the opposite of 'expand'.\n            * 'broadcast' : results will be broadcast to the original shape\n              of the KnowledgeFrame, the original index and columns will be\n              retained.\n\n            The default behaviour (None) depends on the return value of the\n            applied function: list-like results will be returned as a Collections\n            of those. However if the employ function returns a Collections these\n            are expanded to columns.\n        args : tuple\n            Positional arguments to pass to `func` in addition to the\n            array/collections.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        Collections or KnowledgeFrame\n            Result of employing ``func`` along the given axis of the\n            KnowledgeFrame.\n\n        See Also\n        --------\n        KnowledgeFrame.employmapping: For elementwise operations.\n        KnowledgeFrame.aggregate: Only perform aggregating type operations.\n        KnowledgeFrame.transform: Only perform transforgetting_ming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.ukf-mutation`\n        for more definal_item_tails.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([[4, 9]] * 3, columns=['A', 'B'])\n        >>> kf\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n\n        Using a numpy universal function (in this case the same as\n        ``np.sqrt(kf)``):\n\n        >>> kf.employ(np.sqrt)\n             A    B\n        0  2.0  3.0\n        1  2.0  3.0\n        2  2.0  3.0\n\n        Using a reducing function on either axis\n\n        >>> kf.employ(np.total_sum, axis=0)\n        A    12\n        B    27\n        dtype: int64\n\n        >>> kf.employ(np.total_sum, axis=1)\n        0    13\n        1    13\n        2    13\n        dtype: int64\n\n        Returning a list-like will result in a Collections\n\n        >>> kf.employ(lambda x: [1, 2], axis=1)\n        0    [1, 2]\n        1    [1, 2]\n        2    [1, 2]\n        dtype: object\n\n        Passing ``result_type='expand'`` will expand list-like results\n        to columns of a Dataframe\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='expand')\n           0  1\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returning a Collections inside the function is similar to passing\n        ``result_type='expand'``. The resulting column names\n        will be the Collections index.\n\n        >>> kf.employ(lambda x: mk.Collections([1, 2], index=['foo', 'bar']), axis=1)\n           foo  bar\n        0    1    2\n        1    1    2\n        2    1    2\n\n        Passing ``result_type='broadcast'`` will ensure the same shape\n        result, whether list-like or scalar is returned by the function,\n        and broadcast it along the axis. The resulting column names will\n        be the originals.\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='broadcast')\n           A  B\n        0  1  2\n        1  1  2\n        2  1  2\n        ",
            "1200019": "get_min(self, *, skipna=True, **kwargs):\n        The getting_minimum value of the object.\n\n        Only ordered `Categoricals` have a getting_minimum!\n\n        .. versionchanged:: 1.0.0\n\n           Returns an NA value on empty arrays\n\n        Raises\n        ------\n        TypeError\n            If the `Categorical` is not `ordered`.\n\n        Returns\n        -------\n        getting_min : the getting_minimum of this `Categorical`\n        ",
            "1200020": "get_max(self, axis=None, skipna: 'bool' = True, *args, **kwargs):\n        Return the getting_maximum value of the Index.\n\n        Parameters\n        ----------\n        axis : int, optional\n            For compatibility with NumPy. Only 0 or None are total_allowed.\n        skipna : bool, default True\n            Exclude NA/null values when showing the result.\n        *args, **kwargs\n            Additional arguments and keywords for compatibility with NumPy.\n\n        Returns\n        -------\n        scalar\n            Maximum value.\n\n        See Also\n        --------\n        Index.getting_min : Return the getting_minimum value in an Index.\n        Collections.getting_max : Return the getting_maximum value in a Collections.\n        KnowledgeFrame.getting_max : Return the getting_maximum values in a KnowledgeFrame.\n\n        Examples\n        --------\n        >>> idx = mk.Index([3, 2, 1])\n        >>> idx.getting_max()\n        3\n\n        >>> idx = mk.Index(['c', 'b', 'a'])\n        >>> idx.getting_max()\n        'c'\n\n        For a MultiIndex, the getting_maximum is detergetting_mined lexicographictotal_ally.\n\n        >>> idx = mk.MultiIndex.from_product([('a', 'b'), (2, 1)])\n        >>> idx.getting_max()\n        ('b', 2)\n        "
        }
    },
    "PandasEval/26": {
        "original_query": "I want to create a knowledgeframe with one of the column as a list or array. After you assign a list like or array like value to the columns, the column should be considered as type object Now I want to assign the emails to first row and the 'Email' column",
        "retrieved_APIs": {
            "API_1": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_4": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.",
            "API_5": "formating(self, name: 'bool' = False, formatingter: 'Ctotal_allable | None' = None, na_rep: 'str_t' = 'NaN') -> 'list[str_t]': Return the Index as a formatted string."
        },
        "code_prefix": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'Name':['Juda','Pri']})\nemails = {'a@a.com','b@b.com'}\nkf['Email'] = ''",
        "code_completion": [
            "kf.Email = kf.Email.totype(object)\nkf.loc[0].Email = emails"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'astype_loc'\n}\n\n\ndef check():\n    assert df.equals(pd.DataFrame({'Name':['Juda','Pri'],'Email':[{'a@a.com','b@b.com'}, '']}))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "I want to create a knowledgeframe with one of the column as a list or array. After you assign a list like or array like value to the columns, the column should be considered as type object?",
            "Now I want to assign the emails to first row and the 'Email' column?",
            "How to assign the emails to first row and the column?"
        ],
        "gold_APIs": {
            "1200016": "totype(self, dtype: 'Dtype | None' = None, clone=True):\n        Change the dtype of a SparseArray.\n\n        The output will always be a SparseArray. To convert to a dense\n        ndarray with a certain dtype, use :meth:`numpy.asarray`.\n\n        Parameters\n        ----------\n        dtype : np.dtype or ExtensionDtype\n            For SparseDtype, this changes the dtype of\n            ``self.sp_values`` and the ``self.fill_value``.\n\n            For other dtypes, this only changes the dtype of\n            ``self.sp_values``.\n\n        clone : bool, default True\n            Whether to ensure a clone is made, even if not necessary.\n\n        Returns\n        -------\n        SparseArray\n\n        Examples\n        --------\n        >>> arr = mk.arrays.SparseArray([0, 0, 1, 2])\n        >>> arr\n        [0, 0, 1, 2]\n        Fill: 0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        >>> arr.totype(np.dtype('int32'))\n        [0, 0, 1, 2]\n        Fill: 0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        Using a NumPy dtype with a different kind (e.g. float) will coerce\n        just ``self.sp_values``.\n\n        >>> arr.totype(np.dtype('float64'))\n        ... # doctest: +NORMALIZE_WHITESPACE\n        [0.0, 0.0, 1.0, 2.0]\n        Fill: 0.0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        Use a SparseDtype if you wish to be change the fill value as well.\n\n        >>> arr.totype(SparseDtype(\"float64\", fill_value=np.nan))\n        ... # doctest: +NORMALIZE_WHITESPACE\n        [nan, nan, 1.0, 2.0]\n        Fill: nan\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n        "
        }
    },
    "PandasEval/28": {
        "original_query": "In my code, I have several variables which can either contain a monkey KnowledgeFrame or nothing at all. Let's say I want to test and see if a certain KnowledgeFrame has been created yet or not.",
        "retrieved_APIs": {
            "API_1": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_2": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_3": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.",
            "API_4": "concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion': Concatenate monkey objects along one axis, using set logic on the other axes if needed.",
            "API_5": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type."
        },
        "code_prefix": "import monkey as mk\n\ndef is_kf_exist(kf):",
        "code_completion": [
            "    if kf is None:\n        return False\n    else:\n        return True"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'df_none'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('abc')})) == True\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[2,300,500], 'C':list('abc')})) == True\n    assert candidate(pd.DataFrame({'A':[13,2,3], 'B':[2,300,500], 'C':list('abc')})) == True\n    assert candidate(pd.DataFrame({'D':[1,2,3], 'B':[2,300,500], 'C':list('dct')})) == True\n    assert candidate(pd.DataFrame({'A':[1,25,34], 'B':[2,300,500], 'C':list('abc')})) == True\n    assert candidate(pd.DataFrame({'A':[1,23,3], 'S':[2,300,500], 'C':list('abc')})) == True\n    assert candidate(None) == False\n    assert candidate(pd.DataFrame({'A':[1,23,3], 'S':[2,3,500], 'C':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,23,3], 'S':[2,3,5], 'C':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,23,3], 'S':[5,2,1], 'C':list('abc')}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "In my code, I have several variables which can contain a KnowledgeFrame or nothing. How can I test if a KnowledgeFrame has been created?",
            "How can I test if a certain KnowledgeFrame has been created in my code?"
        ],
        "gold_APIs": {}
    },
    "PandasEval/29": {
        "original_query": "I need to remain the rows where line_num is not equal to 0. What's the most efficient way to do it? it should be as simple as:",
        "retrieved_APIs": {
            "API_1": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_2": "ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.",
            "API_3": "getting(self, i): Return the element at specified position.",
            "API_4": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.",
            "API_5": "division(self, other, axis='columns', level=None, fill_value=None): Get the element-wise floating division of knowledgeframe or other objects."
        },
        "code_prefix": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'line_date': [1, 2, 3], 'line_num': [1, 0, 6], 'line_text': list('abc')})\nn_kf =",
        "code_completion": [" kf[kf.line_num != 0]"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'df_none'\n}\n\n\ndef check():\n    assert df.equals(pd.DataFrame({\"line_date\": [1, 3], \"line_num\": [1, 6], \"line_text\": list(\"ac\")}, index=[0, 2]))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "What is the most efficient way to remain rows?",
            "What is the simplest way to remain rows?"
        ],
        "gold_APIs": {}
    },
    "PandasEval/30": {
        "original_query": "I would like to sip all data in a monkey knowledgeframe Using kf.index to sip all rows",
        "retrieved_APIs": {
            "API_1": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.",
            "API_2": "sipna(self): Return an ExtensionArray that is devoid of NA values.",
            "API_3": "traversal(self) -> 'Iterable[tuple[Hashable, Collections]]': Return the rows of the KnowledgeFrame organized in (index, Collections) pairs.",
            "API_4": "choose_dtypes(self, include=None, exclude=None) -> 'KnowledgeFrame': Extract a collection of colums from the KnowledgeFrame based on their dtypes.",
            "API_5": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes."
        },
        "code_prefix": "import monkey as mk\n\nweb_stats = {'Day': [1, 2, 3, 4, 2, 6],\n             'Visitors': [43, 43, 34, 23, 43, 23],\n             'Bounce_Rate': [3, 2, 4, 3, 5, 5]}\nkf = mk.KnowledgeFrame(web_stats)",
        "code_completion": ["kf.sip(kf.index, inplace=True)"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'drop_index_inplace'\n}\n\n\ndef check():\n    tmp = pd.DataFrame({'Day': [1, 2, 3], 'Visitors': [4, 5, 6], 'Bounce_Rate': [7, 8, 9]})\n    tmp.drop(tmp.index, inplace=True)\n    assert df.equals(tmp)\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "I would like to sip all data in a monkey knowledgeframe using kf.index.",
            "Using kf.index to sip all rows."
        ],
        "gold_APIs": {
            "1200003": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index':\n        Make new Index with passed list of labels deleted.\n\n        Parameters\n        ----------\n        labels : array-like\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and existing labels are sipped.\n\n        Returns\n        -------\n        sipped : Index\n            Will be same type as self, except for RangeIndex.\n\n        Raises\n        ------\n        KeyError\n            If not total_all of the labels are found in the selected axis\n        "
        }
    },
    "PandasEval/31": {
        "original_query": "I would like to add a new column C that is the sum value of A and B cell.",
        "retrieved_APIs": {
            "API_1": "division(self, other, axis='columns', level=None, fill_value=None): Get the element-wise floating division of knowledgeframe or other objects.",
            "API_2": "to_num(arg, errors='raise', downcast=None): Transform the the argumemt to the numeric type.",
            "API_3": "ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.",
            "API_4": "replacing(old, new, count=-1, /): Return a copy of the object that replaces all instances of the substring old with new.",
            "API_5": "total_sum(self, axis=None, skipna=None, level=None, numeric_only=None, getting_min_count=0, **kwargs): Return the summed value of the specified axis."
        },
        "code_prefix": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})",
        "code_completion": ["kf['C'] = kf.A + kf.B"],
        "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'add': 'drop_index_inplace'\n}\n\n\ndef check():\n    assert df.equals(pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [5, 7, 9]}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "I would like to add a new column that is the sum value of cell.",
            "I would like to add a new column that is the sum value of cell A and B.",
            "I would like to add a new column that is the sum value of A and B.",
            "I would like to add a new column that is the sum value of A and cell.",
            "I would like to add a new column that is the sum value of B and cell."
        ],
        "gold_APIs": {}
    },
    "PandasEval/32": {
        "original_query": "Move next value to first empty row monkey how do i move each value from a column to the first empty \"row/cell\" in monkey? use sorted to align non NULL data at the top, use sipna to sip all rows with all NaN",
        "retrieved_APIs": {
            "API_1": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_2": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.",
            "API_3": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_4": "sipna(self): Return an ExtensionArray that is devoid of NA values.",
            "API_5": "concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion': Concatenate monkey objects along one axis, using set logic on the other axes if needed."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\nkf = mk.KnowledgeFrame({'A': [1, 4, 7, np.nan], 'B': [np.nan, 2, 5, np.nan], 'C': [np.nan, np.nan, 3, 6]})\nnew_kf =",
        "code_completion": [
            " kf.employ(lambda x: sorted(x, key=mk.ifnull)).sipna(how = 'all')"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'apply_dropna_sorted'\n}\n\n\ndef check(candidate):\n    assert new_df.equals(pd.DataFrame({'A': [1.0, 4.0, 7.0], 'B': [2.0, 5.0, np.nan], 'C': [3.0, 6.0, np.nan]}))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "How do I move each value from a column to the first empty \"row/cell\" in monkey?",
            "Use sorted to align non NULL data at the top, use sipna to sip all rows with all NaN."
        ],
        "gold_APIs": {
            "1200005": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs):\n        Apply a function along an axis of the KnowledgeFrame.\n\n        Objects passed to the function are Collections objects whose index is\n        either the KnowledgeFrame's index (``axis=0``) or the KnowledgeFrame's columns\n        (``axis=1``). By default (``result_type=None``), the final return type\n        is inferred from the return type of the applied function. Otherwise,\n        it depends on the `result_type` argument.\n\n        Parameters\n        ----------\n        func : function\n            Function to employ to each column or row.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the function is applied:\n\n            * 0 or 'index': employ function to each column.\n            * 1 or 'columns': employ function to each row.\n\n        raw : bool, default False\n            Detergetting_mines if row or column is passed as a Collections or ndarray object:\n\n            * ``False`` : passes each row or column as a Collections to the\n              function.\n            * ``True`` : the passed function will receive ndarray objects\n              instead.\n              If you are just employing a NumPy reduction function this will\n              achieve much better performance.\n\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\n            These only act when ``axis=1`` (columns):\n\n            * 'expand' : list-like results will be turned into columns.\n            * 'reduce' : returns a Collections if possible rather than expanding\n              list-like results. This is the opposite of 'expand'.\n            * 'broadcast' : results will be broadcast to the original shape\n              of the KnowledgeFrame, the original index and columns will be\n              retained.\n\n            The default behaviour (None) depends on the return value of the\n            applied function: list-like results will be returned as a Collections\n            of those. However if the employ function returns a Collections these\n            are expanded to columns.\n        args : tuple\n            Positional arguments to pass to `func` in addition to the\n            array/collections.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        Collections or KnowledgeFrame\n            Result of employing ``func`` along the given axis of the\n            KnowledgeFrame.\n\n        See Also\n        --------\n        KnowledgeFrame.employmapping: For elementwise operations.\n        KnowledgeFrame.aggregate: Only perform aggregating type operations.\n        KnowledgeFrame.transform: Only perform transforgetting_ming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.ukf-mutation`\n        for more definal_item_tails.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([[4, 9]] * 3, columns=['A', 'B'])\n        >>> kf\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n\n        Using a numpy universal function (in this case the same as\n        ``np.sqrt(kf)``):\n\n        >>> kf.employ(np.sqrt)\n             A    B\n        0  2.0  3.0\n        1  2.0  3.0\n        2  2.0  3.0\n\n        Using a reducing function on either axis\n\n        >>> kf.employ(np.total_sum, axis=0)\n        A    12\n        B    27\n        dtype: int64\n\n        >>> kf.employ(np.total_sum, axis=1)\n        0    13\n        1    13\n        2    13\n        dtype: int64\n\n        Returning a list-like will result in a Collections\n\n        >>> kf.employ(lambda x: [1, 2], axis=1)\n        0    [1, 2]\n        1    [1, 2]\n        2    [1, 2]\n        dtype: object\n\n        Passing ``result_type='expand'`` will expand list-like results\n        to columns of a Dataframe\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='expand')\n           0  1\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returning a Collections inside the function is similar to passing\n        ``result_type='expand'``. The resulting column names\n        will be the Collections index.\n\n        >>> kf.employ(lambda x: mk.Collections([1, 2], index=['foo', 'bar']), axis=1)\n           foo  bar\n        0    1    2\n        1    1    2\n        2    1    2\n\n        Passing ``result_type='broadcast'`` will ensure the same shape\n        result, whether list-like or scalar is returned by the function,\n        and broadcast it along the axis. The resulting column names will\n        be the originals.\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='broadcast')\n           A  B\n        0  1  2\n        1  1  2\n        2  1  2\n        ",
            "1200007": "sipna(self):\n        Return ExtensionArray without NA values.\n\n        Returns\n        -------\n        valid : ExtensionArray\n        ",
            "1200015": "ifnull(self) -> 'np.ndarray':\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`mk.NaT`, getting\n        mappingped to ``True`` values.\n        Everything else getting mappingped to ``False`` values. Characters such as\n        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``monkey.options.mode.use_inf_as_na = True``).\n\n        Returns\n        -------\n        numpy.ndarray[bool]\n            A boolean array of whether my values are NA.\n\n        See Also\n        --------\n        Index.notna : Boolean inverse of ifna.\n        Index.sipna : Omit entries with missing values.\n        ifna : Top-level ifna.\n        Collections.ifna : Detect missing values in Collections object.\n\n        Examples\n        --------\n        Show which entries in a monkey.Index are NA. The result is an\n        array.\n\n        >>> idx = mk.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.ifna()\n        array([False, False,  True])\n\n        Empty strings are not considered NA values. None is considered an NA\n        value.\n\n        >>> idx = mk.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.ifna()\n        array([False, False, False,  True])\n\n        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n\n        >>> idx = mk.DatetimeIndex([mk.Timestamp('1940-04-25'),\n        ...                         mk.Timestamp(''), None, mk.NaT])\n        >>> idx\n        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n                      dtype='datetime64[ns]', freq=None)\n        >>> idx.ifna()\n        array([False,  True,  True,  True])\n        "
        }
    },
    "PandasEval/33": {
        "original_query": "I want to make all column headers in my monkey data frame lower case",
        "retrieved_APIs": {
            "API_1": "mapping(self, mapper, na_action=None): Map the object's values according to an input mapping or function.",
            "API_2": "header_num(self: 'FrameOrCollections', n: 'int' = 5) -> 'FrameOrCollections': Get the top `n` rows of the frame or collections.",
            "API_3": "convert_string(self, buf: 'FilePathOrBuffer[str] | None' = None, columns: 'Sequence[str] | None' = None, col_space: 'int | None' = None, header_numer: 'bool | Sequence[str]' = True, index: 'bool' = True, na_rep: 'str' = 'NaN', formatingters: 'fmt.FormattersType | None' = None, float_formating: 'fmt.FloatFormatType | None' = None, sparsify: 'bool | None' = None, index_names: 'bool' = True, justify: 'str | None' = None, getting_max_rows: 'int | None' = None, getting_min_rows: 'int | None' = None, getting_max_cols: 'int | None' = None, show_dimensions: 'bool' = False, decimal: 'str' = '.', line_width: 'int | None' = None, getting_max_colwidth: 'int | None' = None, encoding: 'str | None' = None) -> 'str | None': Display the output of the KnowledgeFrame as a console-friendly tablular.",
            "API_4": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_5": "formating(self, name: 'bool' = False, formatingter: 'Ctotal_allable | None' = None, na_rep: 'str_t' = 'NaN') -> 'list[str_t]': Return the Index as a formatted string."
        },
        "code_prefix": "import monkey as mk\n\ndef make_knowledgeframe_column_headers_lowercase(data):",
        "code_completion": [
            "    data.columns = mapping(str.lower, data.columns)\n    return data"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'map_lower'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':range(3), 'B':range(3,0,-1), 'C':list('abc')})).equals(pd.DataFrame({'a':range(3), 'b':range(3,0,-1), 'c':list('abc')}))\n    assert candidate(pd.DataFrame({'M':range(3), 'S':range(3,0,-1), 'R':list('dki')})).equals(pd.DataFrame({'m':range(3), 's':range(3,0,-1), 'r':list('dki')}))\n    assert candidate(pd.DataFrame({'D':range(3), 'B':range(3,0,-1), 'C':list('abc')})).equals(pd.DataFrame({'d':range(3), 'b':range(3,0,-1), 'c':list('abc')}))\n    assert candidate(pd.DataFrame({'D':range(3), 'K':range(3,0,-1), 'C':list('abc')})).equals(pd.DataFrame({'d':range(3), 'k':range(3,0,-1), 'c':list('abc')}))\n    assert candidate(pd.DataFrame({'D':range(3), 'K':range(3,0,-1), 'I':list('abc')})).equals(pd.DataFrame({'d':range(3), 'k':range(3,0,-1), 'i':list('abc')}))\n    assert candidate(pd.DataFrame({'D':range(3), 'K':range(3,0,-1), 'I':list('ccc')})).equals(pd.DataFrame({'d':range(3), 'k':range(3,0,-1), 'i':list('ccc')}))\n    assert candidate(pd.DataFrame({'D':range(3), 'K':range(3,0,-1), 'I':list('msr')})).equals(pd.DataFrame({'d':range(3), 'k':range(3,0,-1), 'i':list('msr')}))\n    assert candidate(pd.DataFrame({'LO':range(3), 'K':range(3,0,-1), 'I':list('msr')})).equals(pd.DataFrame({'lo':range(3), 'k':range(3,0,-1), 'i':list('msr')}))\n    assert candidate(pd.DataFrame({'LO':range(3), 'V':range(3,0,-1), 'I':list('msr')})).equals(pd.DataFrame({'lo':range(3), 'v':range(3,0,-1), 'i':list('msr')}))\n    assert candidate(pd.DataFrame({'LO':range(3), 'V':range(3,0,-1), 'E':list('msr')})).equals(pd.DataFrame({'lo':range(3), 'v':range(3,0,-1), 'e':list('msr')}))\n\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "I want to make all headers in my data frame lower case",
            "How to make all column headers in my data frame lower case?"
        ],
        "gold_APIs": {
            "1200021": "mapping(self, mapper, na_action=None):\n        Map values using input correspondence (a dict, Collections, or function).\n\n        Parameters\n        ----------\n        mappingper : function, dict, or Collections\n            Mapping correspondence.\n        na_action : {None, 'ignore'}\n            If 'ignore', propagate NA values, without passing them to the\n            mappingping correspondence.\n\n        Returns\n        -------\n        applied : Union[Index, MultiIndex], inferred\n            The output of the mappingping function applied to the index.\n            If the function returns a tuple with more than one element\n            a MultiIndex will be returned.\n        "
        }
    },
    "PandasEval/35": {
        "original_query": "How to get the first largest value in column a\uff1f Using nbiggest and iloc to implemente this",
        "retrieved_APIs": {
            "API_1": "nbiggest(self, n=5, keep='first') -> 'Collections': Get the elements of the object with the n largest values.",
            "API_2": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_3": "division(self, other, axis='columns', level=None, fill_value=None): Get the element-wise floating division of knowledgeframe or other objects.",
            "API_4": "ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.",
            "API_5": "getting(self, i): Return the element at specified position."
        },
        "code_prefix": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'a': [3.0, 2.0, 4.0, 1.0],'b': [1.0, 4.0 , 2.0, 3.0]})\nfirst_value =",
        "code_completion": ["kf.a.nbiggest(1).iloc[-1]"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'nlargest_iloc'\n}\n\n\ndef check():\n    assert first_value == 4.0\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "How to get the largest value in column a?",
            "How to implement nbiggest and iloc?",
            "Using nbiggest and iloc, how to get the first largest value in column a?"
        ],
        "gold_APIs": {
            "1200022": "nbiggest(self, n=5, keep='first') -> 'Collections':\n        Return the largest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this mwhatever descending sorted values.\n        keep : {'first', 'final_item', 'total_all'}, default 'first'\n            When there are duplicate values that cannot total_all fit in a\n            Collections of `n` elements:\n\n            - ``first`` : return the first `n` occurrences in order\n                of appearance.\n            - ``final_item`` : return the final_item `n` occurrences in reverse\n                order of appearance.\n            - ``total_all`` : keep total_all occurrences. This can result in a Collections of\n                size larger than `n`.\n\n        Returns\n        -------\n        Collections\n            The `n` largest values in the Collections, sorted in decreasing order.\n\n        See Also\n        --------\n        Collections.nsmtotal_allest: Get the `n` smtotal_allest elements.\n        Collections.sort_the_values: Sort Collections by values.\n        Collections.header_num: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_the_values(ascending=False).header_num(n)`` for smtotal_all `n`\n        relative to the size of the ``Collections`` object.\n\n        Examples\n        --------\n        >>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n        ...                         \"Malta\": 434000, \"Maldivisiones\": 434000,\n        ...                         \"Brunei\": 434000, \"Iceland\": 337000,\n        ...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n        ...                         \"Anguilla\": 11300, \"Montserrat\": 5200}\n        >>> s = mk.Collections(countries_population)\n        >>> s\n        Italy       59000000\n        France      65000000\n        Malta         434000\n        Maldivisiones      434000\n        Brunei        434000\n        Iceland       337000\n        Nauru          11300\n        Tuvalu         11300\n        Anguilla       11300\n        Montserrat      5200\n        dtype: int64\n\n        The `n` largest elements where ``n=5`` by default.\n\n        >>> s.nbiggest()\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldivisiones      434000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3``. Default `keep` value is 'first'\n        so Malta will be kept.\n\n        >>> s.nbiggest(3)\n        France    65000000\n        Italy     59000000\n        Malta       434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` and keeping the final_item duplicates.\n        Brunei will be kept since it is the final_item with value 434000 based on\n        the index order.\n\n        >>> s.nbiggest(3, keep='final_item')\n        France      65000000\n        Italy       59000000\n        Brunei        434000\n        dtype: int64\n\n        The `n` largest elements where ``n=3`` with total_all duplicates kept. Note\n        that the returned Collections has five elements due to the three duplicates.\n\n        >>> s.nbiggest(3, keep='total_all')\n        France      65000000\n        Italy       59000000\n        Malta         434000\n        Maldivisiones      434000\n        Brunei        434000\n        dtype: int64\n        "
        }
    },
    "PandasEval/36": {
        "original_query": "I have a Monkey knowledgeframe and I want to find all the unique values in that knowledgeframe...irrespective of row/columns. If I have a 10 x 10 knowledgeframe, and suppose they have 84 unique values, I need to find them - Not the count. Using xx.values.flat_underlying to get the flattened array of the knowledgeframe Getting the unique values by numpy.unique",
        "retrieved_APIs": {
            "API_1": "flat_underlying(self, order='C'): Flatten the underlying values into an ndarray.",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion': Concatenate monkey objects along one axis, using set logic on the other axes if needed.",
            "API_4": "mapping(self, mapper, na_action=None): Map the object's values according to an input mapping or function.",
            "API_5": "incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\nkf = mk.KnowledgeFrame(np.random.randint(0,10,size=100).reshape(10,10))\nunique_ndarray =",
        "code_completion": [" np.unique(kf.values.flat_underlying())"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'nlargest_iloc'\n}\n\n\ndef check():\n    assert np.array_equal(unique_ndarray, np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "How can I find all the unique values in a Monkey knowledgeframe?",
            "What is the method to get unique values in a knowledgeframe?"
        ],
        "gold_APIs": {
            "1200023": "flat_underlying(self, order='C'):\n        Return an ndarray of the flattened values of the underlying data.\n\n        Returns\n        -------\n        numpy.ndarray\n            Flattened array.\n\n        See Also\n        --------\n        numpy.ndarray.flat_underlying : Return a flattened array.\n        "
        }
    },
    "PandasEval/37": {
        "original_query": "How to group values of monkey knowledgeframe and select the latest by date from each group? Sorting values by `date` (ascending is True), and then grouping by `id`",
        "retrieved_APIs": {
            "API_1": "grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy': Group the KnowledgeFrame by a set of columns or group keys.",
            "API_2": "sorting_index(self, axis: 'Axis' = 0, level: 'Level | None' = None, ascending: 'bool | int | Sequence[bool | int]' = True, inplace: 'bool' = False, kind: 'str' = 'quicksort', na_position: 'str' = 'final_item', sort_remaining: 'bool' = True, ignore_index: 'bool' = False, key: 'IndexKeyFunc' = None): Return object sorted by labels along the specified axis.",
            "API_3": "adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options.",
            "API_4": "concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion': Concatenate monkey objects along one axis, using set logic on the other axes if needed.",
            "API_5": "sort_the_values(self, return_indexer: 'bool' = False, ascending: 'bool' = True, na_position: 'str_t' = 'final_item', key: 'Ctotal_allable | None' = None): Return the index as a sorted clone."
        },
        "code_prefix": "import monkey as mk\nkf = mk.KnowledgeFrame({\n    'id': [220, 220, 220, 826, 826, 826, 901, 901, 901],\n    'product': [6647, 6647, 6647, 3380, 3380, 3380, 4555, 4555, 4555],\n    'date': ['2014-09-01', '2014-09-03', '2014-10-16', '2014-11-11', '2014-12-09', '2015-05-19', '2014-09-01', '2014-10-05', '2014-11-01']\n})\n\nfinal_item_kf =",
        "code_completion": [
            " kf.sort_the_values('date', ascending=True)\nfinal_item_kf = final_item_kf.grouper('id').final_item()"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'sort_values_groupby_last'\n}\n\n\ndef check():\n    assert last_df.equals(df.sort_values('date', ascending=True).groupby('id').last())\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "How to group values and select the latest by date from each group?",
            "How to group values and select the latest by date?",
            "How to group values and select the latest?",
            "How to group values and select?",
            "How to group values?"
        ],
        "gold_APIs": {
            "1200012": "grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy':\nGroup KnowledgeFrame using a mappingper or by a Collections of columns.\n\nA grouper operation involves some combination of splitting the\nobject, employing a function, and combining the results. This can be\nused to group large amounts of data and compute operations on these\ngroups.\n\nParameters\n----------\nby : mappingping, function, label, or list of labels\n    Used to detergetting_mine the groups for the grouper.\n    If ``by`` is a function, it's ctotal_alled on each value of the object's\n    index. If a dict or Collections is passed, the Collections or dict VALUES\n    will be used to detergetting_mine the groups (the Collections' values are first\n    aligned; see ``.align()`` method). If an ndarray is passed, the\n    values are used as-is to detergetting_mine the groups. A label or list of\n    labels may be passed to group by the columns in ``self``. Notice\n    that a tuple is interpreted as a (single) key.\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    Split along rows (0) or columns (1).\nlevel : int, level name, or sequence of such, default None\n    If the axis is a MultiIndex (hierarchical), group by a particular\n    level or levels.\nas_index : bool, default True\n    For aggregated output, return object with group labels as the\n    index. Only relevant for KnowledgeFrame input. as_index=False is\n    effectively \"SQL-style\" grouped output.\nsort : bool, default True\n    Sort group keys. Get better performance by turning this off.\n    Note this does not influence the order of observations within each\n    group. Groupby preserves the order of rows within each group.\ngroup_keys : bool, default True\n    When ctotal_alling employ, add group keys to index to identify pieces.\nsqueeze : bool, default False\n    Reduce the dimensionality of the return type if possible,\n    otherwise return a consistent type.\n\n    .. deprecated:: 1.1.0\n\nobserved : bool, default False\n    This only applies if whatever of the groupers are Categoricals.\n    If True: only show observed values for categorical groupers.\n    If False: show total_all values for categorical groupers.\nsipna : bool, default True\n    If True, and if group keys contain NA values, NA values togettingher\n    with row/column will be sipped.\n    If False, NA values will also be treated as the key in groups\n\n    .. versionadded:: 1.1.0\n\nReturns\n-------\nKnowledgeFrameGroupBy\n    Returns a grouper object that contains informatingion about the groups.\n\nSee Also\n--------\nresample_by_num : Convenience method for frequency conversion and resampling\n    of time collections.\n\nNotes\n-----\nSee the `user guide\n<https://monkey.pydata.org/monkey-docs/stable/grouper.html>`__ for more.\n\nExamples\n--------\n>>> kf = mk.KnowledgeFrame({'Animal': ['Falcon', 'Falcon',\n...                               'Parrot', 'Parrot'],\n...                    'Max Speed': [380., 370., 24., 26.]})\n>>> kf\n   Animal  Max Speed\n0  Falcon      380.0\n1  Falcon      370.0\n2  Parrot       24.0\n3  Parrot       26.0\n>>> kf.grouper(['Animal']).average()\n        Max Speed\nAnimal\nFalcon      375.0\nParrot       25.0\n\n**Hierarchical Indexes**\n\nWe can grouper different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = mk.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> kf = mk.KnowledgeFrame({'Max Speed': [390., 350., 30., 20.]},\n...                   index=index)\n>>> kf\n                Max Speed\nAnimal Type\nFalcon Captive      390.0\n       Wild         350.0\nParrot Captive       30.0\n       Wild          20.0\n>>> kf.grouper(level=0).average()\n        Max Speed\nAnimal\nFalcon      370.0\nParrot       25.0\n>>> kf.grouper(level=\"Type\").average()\n         Max Speed\nType\nCaptive      210.0\nWild         185.0\n\nWe can also choose to include NA in group keys or not by setting\n`sipna` parameter, the default setting is `True`:\n\n>>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\n>>> kf = mk.KnowledgeFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> kf.grouper(by=[\"b\"]).total_sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\n\n>>> kf.grouper(by=[\"b\"], sipna=False).total_sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\nNaN 1   4\n\n>>> l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]]\n>>> kf = mk.KnowledgeFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> kf.grouper(by=\"a\").total_sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\n\n>>> kf.grouper(by=\"a\", sipna=False).total_sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\nNaN 12.3   33.0\n",
            "1200024": "sort_the_values(self, return_indexer: 'bool' = False, ascending: 'bool' = True, na_position: 'str_t' = 'final_item', key: 'Ctotal_allable | None' = None):\n        Return a sorted clone of the index.\n\n        Return a sorted clone of the index, and optiontotal_ally return the indices\n        that sorted the index itself.\n\n        Parameters\n        ----------\n        return_indexer : bool, default False\n            Should the indices that would sort the index be returned.\n        ascending : bool, default True\n            Should the index values be sorted in an ascending order.\n        na_position : {'first' or 'final_item'}, default 'final_item'\n            Argument 'first' puts NaNs at the beginning, 'final_item' puts NaNs at\n            the end.\n\n            .. versionadded:: 1.2.0\n\n        key : ctotal_allable, optional\n            If not None, employ the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        sorted_index : monkey.Index\n            Sorted clone of the index.\n        indexer : numpy.ndarray, optional\n            The indices that the index itself was sorted by.\n\n        See Also\n        --------\n        Collections.sort_the_values : Sort values of a Collections.\n        KnowledgeFrame.sort_the_values : Sort values in a KnowledgeFrame.\n\n        Examples\n        --------\n        >>> idx = mk.Index([10, 100, 1, 1000])\n        >>> idx\n        Int64Index([10, 100, 1, 1000], dtype='int64')\n\n        Sort values in ascending order (default behavior).\n\n        >>> idx.sort_the_values()\n        Int64Index([1, 10, 100, 1000], dtype='int64')\n\n        Sort values in descending order, and also getting the indices `idx` was\n        sorted by.\n\n        >>> idx.sort_the_values(ascending=False, return_indexer=True)\n        (Int64Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))\n        "
        }
    },
    "PandasEval/38": {
        "original_query": "i want to sip 2 rows in the knowledgeframe if zero comes in the column if 0 comes on odd index sip previous row as well as current row using monkey Assuming your knowledgeframe is indexed starting from 0 Rows with column2 = 0 and on odd index The rows above them A new knowledgeframe with those rows removed",
        "retrieved_APIs": {
            "API_1": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.",
            "API_2": "reseting_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, sip: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'KnowledgeFrame | None': Reset the index of the KnowledgeFrame, and use the default one instead.",
            "API_3": "sipna(self): Return an ExtensionArray that is devoid of NA values.",
            "API_4": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_5": "adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options."
        },
        "code_prefix": "import monkey as mk\n\ndef sip2rows_zero(kf):\n                    idx = kf[(kf['column2'] == 0) & (kf.index % 2 == 1)].index\n        idx = idx.adding(idx-1)",
        "code_completion": ["result = kf.sip(idx)\n    return result"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'append_odd_drop'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [1, 0, 2, 3, 7, 10]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [2, 3, 7, 10]}, index=[2, 3, 4, 5]))\n    assert candidate(pd.DataFrame({'column1': ['m', 's', 'r', 'a', 'z', 'a'],'column2': [8, 7, 2, 5, 6, 1]})).equals(pd.DataFrame({'column1': ['m', 's', 'r', 'a', 'z', 'a'],'column2': [8, 7, 2, 5, 6, 1]}))\n    assert candidate(pd.DataFrame({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [2, 0, 2, 3, 7, 10]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [2, 3, 7, 10]}, index=[2, 3, 4, 5]))\n    assert candidate({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [2, 0, 2, 3, 7, 11]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [2, 3, 7, 11]}, index=[2, 3, 4, 5]))\n    assert candidate({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [2, 0, 2, 3, 8, 11]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [2, 3, 8, 11]}, index=[2, 3, 4, 5]))\n    assert candidate({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [2, 0, 2, 4, 8, 11]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [2, 4, 8, 11]}, index=[2, 3, 4, 5]))\n    assert candidate({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [2, 0, 3, 4, 8, 11]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [3, 4, 8, 11]}, index=[2, 3, 4, 5]))\n    assert candidate({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [2, 0, 3, 4, 9, 11]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [3, 4, 9, 11]}, index=[2, 3, 4, 5]))\n    assert candidate({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [2, 0, 3, 4, 9, 18]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [3, 4, 9, 18]}, index=[2, 3, 4, 5]))\n    assert candidate({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [2, 0, 3, 4, 12, 18]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [3, 4, 12, 18]}, index=[2, 3, 4, 5]))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "I want to sip rows in the knowledgeframe if zero comes in the column.",
            "Assuming your knowledgeframe is indexed starting from 0, rows with column2 = 0 and on odd index.",
            "A new knowledgeframe with those rows removed."
        ],
        "gold_APIs": {
            "1200003": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index':\n        Make new Index with passed list of labels deleted.\n\n        Parameters\n        ----------\n        labels : array-like\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and existing labels are sipped.\n\n        Returns\n        -------\n        sipped : Index\n            Will be same type as self, except for RangeIndex.\n\n        Raises\n        ------\n        KeyError\n            If not total_all of the labels are found in the selected axis\n        "
        }
    },
    "PandasEval/39": {
        "original_query": "Shift column in monkey knowledgeframe up by one? In detail, in 'gdp' column, shift up by one and return knowledgeframe with the changed gdp column.",
        "retrieved_APIs": {
            "API_1": "shifting(self, periods=1, freq=None): Increase the number of time frequency increments by the required number.",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options.",
            "API_4": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns.",
            "API_5": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\ndef shift_column_up_by_one(kf):",
        "code_completion": ["kf['gdp'] = kf['gdp'].shifting(1)\n    return kf"],
        "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'shift'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'y': [1,2],'gdp': [2.0,4.0],'cap': [8, 7]})).equals(pd.DataFrame({'y': [1,2],'gdp': [np.nan,2.0],'cap': [8, 7]}))\n    assert candidate(pd.DataFrame({'y': [1,2],'gdp': [2.0,4.0],'cap': [9, 7]})).equals(pd.DataFrame({'y': [1,2],'gdp': [np.nan,2.0],'cap': [9, 7]}))\n    assert candidate(pd.DataFrame({'y': [1,2],'gdp': [2.0,4.0],'cap': [9, 3]})).equals(pd.DataFrame({'y': [1,2],'gdp': [np.nan,2.0],'cap': [9, 3]}))\n    assert candidate(pd.DataFrame({'y': [1,2],'gdp': [3.0,4.0],'cap': [9, 3]})).equals(pd.DataFrame({'y': [1,2],'gdp': [np.nan,3.0],'cap': [9, 3]}))\n    assert candidate(pd.DataFrame({'y': [5,2],'gdp': [3.0,4.0],'cap': [9, 3]})).equals(pd.DataFrame({'y': [5,2],'gdp': [np.nan,3.0],'cap': [9, 3]}))\n    assert candidate(pd.DataFrame({'y': [5,1],'gdp': [3.0,4.0],'cap': [9, 3]})).equals(pd.DataFrame({'y': [5,1],'gdp': [np.nan,3.0],'cap': [9, 3]}))\n    assert candidate(pd.DataFrame({'y': [5,1],'gdp': [3.0,8.0],'cap': [9, 3]})).equals(pd.DataFrame({'y': [5,1],'gdp': [np.nan,3.0],'cap': [9, 3]}))\n    assert candidate(pd.DataFrame({'y': [5,1],'gdp': [3.0,8.0],'cap': [19, 3]})).equals(pd.DataFrame({'y': [5,1],'gdp': [np.nan,3.0],'cap': [19, 3]}))\n    assert candidate(pd.DataFrame({'y': [5,1],'gdp': [3.0,8.0],'cap': [19, 13]})).equals(pd.DataFrame({'y': [5,1],'gdp': [np.nan,3.0],'cap': [19, 13]}))\n    assert candidate(pd.DataFrame({'y': [5,1],'gdp': [13.0,8.0],'cap': [19, 13]})).equals(pd.DataFrame({'y': [5,1],'gdp': [np.nan,13.0],'cap': [19, 13]}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Shift column in knowledgeframe up by one?",
            "In detail, shift up by one and return knowledgeframe with the changed column."
        ],
        "gold_APIs": {
            "1200026": "shifting(self, periods=1, freq=None):\n        Shift index by desired number of time frequency increments.\n\n        This method is for shiftinging the values of datetime-like indexes\n        by a specified time increment a given number of times.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Number of periods (or increments) to shifting by,\n            can be positive or negative.\n        freq : monkey.DateOffset, monkey.Timedelta or str, optional\n            Frequency increment to shifting by.\n            If None, the index is shiftinged by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n\n        Returns\n        -------\n        monkey.Index\n            Shifted index.\n\n        See Also\n        --------\n        Collections.shifting : Shift values of Collections.\n\n        Notes\n        -----\n        This method is only implemented for datetime-like index classes,\n        i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.\n\n        Examples\n        --------\n        Put the first 5 month starts of 2011 into an index.\n\n        >>> month_starts = mk.date_range('1/1/2011', periods=5, freq='MS')\n        >>> month_starts\n        DatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01',\n                       '2011-05-01'],\n                      dtype='datetime64[ns]', freq='MS')\n\n        Shift the index by 10 days.\n\n        >>> month_starts.shifting(10, freq='D')\n        DatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',\n                       '2011-05-11'],\n                      dtype='datetime64[ns]', freq=None)\n\n        The default value of `freq` is the `freq` attribute of the index,\n        which is 'MS' (month start) in this example.\n\n        >>> month_starts.shifting(10)\n        DatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',\n                       '2012-03-01'],\n                      dtype='datetime64[ns]', freq='MS')\n        "
        }
    },
    "PandasEval/40": {
        "original_query": "I was wondering if there is an elegant and shorthand way in Monkey KnowledgeFrames to select columns by data type (dtype). i.e. Select only float64 columns from a KnowledgeFrame",
        "retrieved_APIs": {
            "API_1": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.",
            "API_2": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_3": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_4": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_5": "choose_dtypes(self, include=None, exclude=None) -> 'KnowledgeFrame': Extract a collection of colums from the KnowledgeFrame based on their dtypes."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\nkf = mk.KnowledgeFrame([[1, 2.2, 'three']], columns=['A', 'B', 'C'])\nnew_kf =",
        "code_completion": ["kf.choose_dtypes(include=['float64'])"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'select_dtypes'\n}\n\n\ndef check():\n    assert new_df.equals(df.select_dtypes(include=['float64']))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Is there a shorthand way in Monkey KnowledgeFrames to select columns by dtype?",
            "Can you select only float64 columns from a KnowledgeFrame?"
        ],
        "gold_APIs": {
            "1200017": "choose_dtypes(self, include=None, exclude=None) -> 'KnowledgeFrame':\n        Return a subset of the KnowledgeFrame's columns based on the column dtypes.\n\n        Parameters\n        ----------\n        include, exclude : scalar or list-like\n            A selection of dtypes or strings to be included/excluded. At least\n            one of these parameters must be supplied.\n\n        Returns\n        -------\n        KnowledgeFrame\n            The subset of the frame including the dtypes in ``include`` and\n            excluding the dtypes in ``exclude``.\n\n        Raises\n        ------\n        ValueError\n            * If both of ``include`` and ``exclude`` are empty\n            * If ``include`` and ``exclude`` have overlapping elements\n            * If whatever kind of string dtype is passed in.\n\n        See Also\n        --------\n        KnowledgeFrame.dtypes: Return Collections with the data type of each column.\n\n        Notes\n        -----\n        * To select total_all *numeric* types, use ``np.number`` or ``'number'``\n        * To select strings you must use the ``object`` dtype, but note that\n          this will return *total_all* object dtype columns\n        * See the `numpy dtype hierarchy\n          <https://numpy.org/doc/stable/reference/arrays.scalars.html>`__\n        * To select datetimes, use ``np.datetime64``, ``'datetime'`` or\n          ``'datetime64'``\n        * To select timedeltas, use ``np.timedelta64``, ``'timedelta'`` or\n          ``'timedelta64'``\n        * To select Monkey categorical dtypes, use ``'category'``\n        * To select Monkey datetimetz dtypes, use ``'datetimetz'`` (new in\n          0.20.0) or ``'datetime64[ns, tz]'``\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame({'a': [1, 2] * 3,\n        ...                    'b': [True, False] * 3,\n        ...                    'c': [1.0, 2.0] * 3})\n        >>> kf\n                a      b  c\n        0       1   True  1.0\n        1       2  False  2.0\n        2       1   True  1.0\n        3       2  False  2.0\n        4       1   True  1.0\n        5       2  False  2.0\n\n        >>> kf.choose_dtypes(include='bool')\n           b\n        0  True\n        1  False\n        2  True\n        3  False\n        4  True\n        5  False\n\n        >>> kf.choose_dtypes(include=['float64'])\n           c\n        0  1.0\n        1  2.0\n        2  1.0\n        3  2.0\n        4  1.0\n        5  2.0\n\n        >>> kf.choose_dtypes(exclude=['int64'])\n               b    c\n        0   True  1.0\n        1  False  2.0\n        2   True  1.0\n        3  False  2.0\n        4   True  1.0\n        5  False  2.0\n        "
        }
    },
    "PandasEval/41": {
        "original_query": "How to unioner two knowledgeframes with different column names but same number of rows? I have two different data frames in monkey. Example: kf1=a b kf2= c 0 1 1 1 2 2 2 3 3 I want to unioner them so kf1= a b c 0 1 1 1 2 2 2 3 3 In order to unioner two knowledgeframes you can use this two examples. Both returns the same goal Using unioner plus additional arguments instructing it to use the indexes Specially, we can set left_index and right_index to True",
        "retrieved_APIs": {
            "API_1": "concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion': Concatenate monkey objects along one axis, using set logic on the other axes if needed.",
            "API_2": "interst(self, other, sort=False): Create the intersection of two Index objects.",
            "API_3": "unioner(self, right: 'FrameOrCollectionsUnion', how: 'str' = 'inner', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), clone: 'bool' = True, indicator: 'bool' = False, validate: 'str | None' = None) -> 'KnowledgeFrame': Database-style join the named Collections objects or KnowledgeFrame.",
            "API_4": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_5": "adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\ndef unioner_kf(kf1, kf2):",
        "code_completion": [
            "return mk.unioner(kf1, kf2, left_index=True, right_index=True)"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'merge'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a':[0, 1, 2],'b':[1,2,3]}), pd.DataFrame({'c':[1, 2, 3]})).equals(pd.DataFrame({'a':[0, 1, 2],'b':[1,2,3], 'c':[1, 2, 3]}))\n    assert candidate(pd.DataFrame({'m':[7,7,9],'s':[5,3,6]}), pd.DataFrame({'r':[9,9,2]})).equals(pd.DataFrame({'m':[7,7,9],'s':[5,3,6],'r':[9,9,2]}))\n    assert candidate(pd.DataFrame({'a':[0, 2, 2],'b':[1,2,3]}), pd.DataFrame({'c':[1, 2, 3]})).equals(pd.DataFrame({'a':[0, 2, 2],'b':[1,2,3], 'c':[1, 2, 3]}))\n    assert candidate(pd.DataFrame({'a':[0, 2, 4],'b':[1,2,3]}), pd.DataFrame({'c':[1, 2, 3]})).equals(pd.DataFrame({'a':[0, 2, 4],'b':[1,2,3], 'c':[1, 2, 3]}))\n    assert candidate(pd.DataFrame({'a':[0, 2, 4],'b':[4,2,3]}), pd.DataFrame({'c':[1, 2, 3]})).equals(pd.DataFrame({'a':[0, 2, 4],'b':[4,2,3], 'c':[1, 2, 3]}))\n    assert candidate(pd.DataFrame({'a':[0, 2, 4],'b':[4,2,3]}), pd.DataFrame({'c':[14, 2, 3]})).equals(pd.DataFrame({'a':[0, 2, 4],'b':[4,2,3], 'c':[14, 2, 3]}))\n    assert candidate(pd.DataFrame({'a':[0, 2, 4],'b':[4,2,3]}), pd.DataFrame({'c':[14, 12, 3]})).equals(pd.DataFrame({'a':[0, 2, 4],'b':[4,2,3], 'c':[14, 12, 3]}))\n    assert candidate(pd.DataFrame({'a':[0, 2, 4],'b':[4,2,3]}), pd.DataFrame({'c':[14, 12, 13]})).equals(pd.DataFrame({'a':[0, 2, 4],'b':[4,2,3], 'c':[14, 12, 13]}))\n    assert candidate(pd.DataFrame({'a':[0, 2, 4],'b':[14,2,3]}), pd.DataFrame({'c':[14, 12, 13]})).equals(pd.DataFrame({'a':[0, 2, 4],'b':[14,2,3], 'c':[14, 12, 13]}))\n    assert candidate(pd.DataFrame({'a':[0, 2, 4],'b':[14,2,13]}), pd.DataFrame({'c':[14, 12, 13]})).equals(pd.DataFrame({'a':[0, 2, 4],'b':[14,2,13], 'c':[14, 12, 13]}))\n\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "How to union two knowledgeframes with different column names but same number of rows?",
            "How to union two knowledgeframes with same number of rows?"
        ],
        "gold_APIs": {
            "1200027": "unioner(self, right: 'FrameOrCollectionsUnion', how: 'str' = 'inner', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), clone: 'bool' = True, indicator: 'bool' = False, validate: 'str | None' = None) -> 'KnowledgeFrame':\nMerge KnowledgeFrame or named Collections objects with a database-style join.\n\nA named Collections object is treated as a KnowledgeFrame with a single named column.\n\nThe join is done on columns or indexes. If joining columns on\ncolumns, the KnowledgeFrame indexes *will be ignored*. Otherwise if joining indexes\non indexes or indexes on a column or columns, the index will be passed on.\nWhen perforgetting_ming a cross unioner, no column specifications to unioner on are\ntotal_allowed.\n\nParameters\n----------\nright : KnowledgeFrame or named Collections\n    Object to unioner with.\nhow : {'left', 'right', 'outer', 'inner', 'cross'}, default 'inner'\n    Type of unioner to be performed.\n\n    * left: use only keys from left frame, similar to a SQL left outer join;\n      preserve key order.\n    * right: use only keys from right frame, similar to a SQL right outer join;\n      preserve key order.\n    * outer: use union of keys from both frames, similar to a SQL full outer\n      join; sort keys lexicographictotal_ally.\n    * inner: use interst of keys from both frames, similar to a SQL inner\n      join; preserve the order of the left keys.\n    * cross: creates the cartesian product from both frames, preserves the order\n      of the left keys.\n\n      .. versionadded:: 1.2.0\n\non : label or list\n    Column or index level names to join on. These must be found in both\n    KnowledgeFrames. If `on` is None and not merging on indexes then this defaults\n    to the interst of the columns in both KnowledgeFrames.\nleft_on : label or list, or array-like\n    Column or index level names to join on in the left KnowledgeFrame. Can also\n    be an array or list of arrays of the lengthgth of the left KnowledgeFrame.\n    These arrays are treated as if they are columns.\nright_on : label or list, or array-like\n    Column or index level names to join on in the right KnowledgeFrame. Can also\n    be an array or list of arrays of the lengthgth of the right KnowledgeFrame.\n    These arrays are treated as if they are columns.\nleft_index : bool, default False\n    Use the index from the left KnowledgeFrame as the join key(s). If it is a\n    MultiIndex, the number of keys in the other KnowledgeFrame (either the index\n    or a number of columns) must match the number of levels.\nright_index : bool, default False\n    Use the index from the right KnowledgeFrame as the join key. Same caveats as\n    left_index.\nsort : bool, default False\n    Sort the join keys lexicographictotal_ally in the result KnowledgeFrame. If False,\n    the order of the join keys depends on the join type (how keyword).\nsuffixes : list-like, default is (\"_x\", \"_y\")\n    A lengthgth-2 sequence where each element is optiontotal_ally a string\n    indicating the suffix to add to overlapping column names in\n    `left` and `right` respectively. Pass a value of `None` instead\n    of a string to indicate that the column name from `left` or\n    `right` should be left as-is, with no suffix. At least one of the\n    values must not be None.\nclone : bool, default True\n    If False, avoid clone if possible.\nindicator : bool or str, default False\n    If True, adds a column to the output KnowledgeFrame ctotal_alled \"_unioner\" with\n    informatingion on the source of each row. The column can be given a different\n    name by providing a string argument. The column will have a Categorical\n    type with the value of \"left_only\" for observations whose unioner key only\n    appears in the left KnowledgeFrame, \"right_only\" for observations\n    whose unioner key only appears in the right KnowledgeFrame, and \"both\"\n    if the observation's unioner key is found in both KnowledgeFrames.\n\nvalidate : str, optional\n    If specified, checks if unioner is of specified type.\n\n    * \"one_to_one\" or \"1:1\": check if unioner keys are distinctive in both\n      left and right datasets.\n    * \"one_to_mwhatever\" or \"1:m\": check if unioner keys are distinctive in left\n      dataset.\n    * \"mwhatever_to_one\" or \"m:1\": check if unioner keys are distinctive in right\n      dataset.\n    * \"mwhatever_to_mwhatever\" or \"m:m\": total_allowed, but does not result in checks.\n\nReturns\n-------\nKnowledgeFrame\n    A KnowledgeFrame of the two unionerd objects.\n\nSee Also\n--------\nunioner_ordered : Merge with optional filling/interpolation.\nunioner_asof : Merge on nearest keys.\nKnowledgeFrame.join : Similar method using indices.\n\nNotes\n-----\nSupport for specifying index levels as the `on`, `left_on`, and\n`right_on` parameters was added in version 0.23.0\nSupport for merging named Collections objects was added in version 0.24.0\n\nExamples\n--------\n>>> kf1 = mk.KnowledgeFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [1, 2, 3, 5]})\n>>> kf2 = mk.KnowledgeFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [5, 6, 7, 8]})\n>>> kf1\n    lkey value\n0   foo      1\n1   bar      2\n2   baz      3\n3   foo      5\n>>> kf2\n    rkey value\n0   foo      5\n1   bar      6\n2   baz      7\n3   foo      8\n\nMerge kf1 and kf2 on the lkey and rkey columns. The value columns have\nthe default suffixes, _x and _y, addinged.\n\n>>> kf1.unioner(kf2, left_on='lkey', right_on='rkey')\n  lkey  value_x rkey  value_y\n0  foo        1  foo        5\n1  foo        1  foo        8\n2  foo        5  foo        5\n3  foo        5  foo        8\n4  bar        2  bar        6\n5  baz        3  baz        7\n\nMerge KnowledgeFrames kf1 and kf2 with specified left and right suffixes\naddinged to whatever overlapping columns.\n\n>>> kf1.unioner(kf2, left_on='lkey', right_on='rkey',\n...           suffixes=('_left', '_right'))\n  lkey  value_left rkey  value_right\n0  foo           1  foo            5\n1  foo           1  foo            8\n2  foo           5  foo            5\n3  foo           5  foo            8\n4  bar           2  bar            6\n5  baz           3  baz            7\n\nMerge KnowledgeFrames kf1 and kf2, but raise an exception if the KnowledgeFrames have\nwhatever overlapping columns.\n\n>>> kf1.unioner(kf2, left_on='lkey', right_on='rkey', suffixes=(False, False))\nTraceback (most recent ctotal_all final_item):\n...\nValueError: columns overlap but no suffix specified:\n    Index(['value'], dtype='object')\n\n>>> kf1 = mk.KnowledgeFrame({'a': ['foo', 'bar'], 'b': [1, 2]})\n>>> kf2 = mk.KnowledgeFrame({'a': ['foo', 'baz'], 'c': [3, 4]})\n>>> kf1\n      a  b\n0   foo  1\n1   bar  2\n>>> kf2\n      a  c\n0   foo  3\n1   baz  4\n\n>>> kf1.unioner(kf2, how='inner', on='a')\n      a  b  c\n0   foo  1  3\n\n>>> kf1.unioner(kf2, how='left', on='a')\n      a  b  c\n0   foo  1  3.0\n1   bar  2  NaN\n\n>>> kf1 = mk.KnowledgeFrame({'left': ['foo', 'bar']})\n>>> kf2 = mk.KnowledgeFrame({'right': [7, 8]})\n>>> kf1\n    left\n0   foo\n1   bar\n>>> kf2\n    right\n0   7\n1   8\n\n>>> kf1.unioner(kf2, how='cross')\n   left  right\n0   foo      7\n1   foo      8\n2   bar      7\n3   bar      8\n"
        }
    },
    "PandasEval/42": {
        "original_query": "How can I delete multiple columns in one pass? In detail, I would like to delete columns A and C, but I don't know how to do it in one pass.",
        "retrieved_APIs": {
            "API_1": "remove_duplicates(self: '_IndexT', keep: 'str_t | bool' = 'first') -> '_IndexT': Remove the duplicate values of the Index.",
            "API_2": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.",
            "API_3": "renaming(self, name, inplace=False): Change the name of the Index or MultiIndex.",
            "API_4": "sipna(self): Return an ExtensionArray that is devoid of NA values.",
            "API_5": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes."
        },
        "code_prefix": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'A': [1, 2, 3],'B': [100, 300, 500],'C': list('abc')})\n\nnew_kf =",
        "code_completion": [" kf.sip(['A', 'C'], axis=1)"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'drop'\n}\n\n\ndef check():\n    assert new_df.equals(pd.DataFrame({'B': [100, 300, 500]}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "How can I delete columns in one pass?",
            "How can I delete multiple columns?",
            "How can I delete columns A and C?",
            "How can I delete columns in one go?",
            "How can I delete multiple columns at once?"
        ],
        "gold_APIs": {
            "1200003": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index':\n        Make new Index with passed list of labels deleted.\n\n        Parameters\n        ----------\n        labels : array-like\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and existing labels are sipped.\n\n        Returns\n        -------\n        sipped : Index\n            Will be same type as self, except for RangeIndex.\n\n        Raises\n        ------\n        KeyError\n            If not total_all of the labels are found in the selected axis\n        "
        }
    },
    "PandasEval/43": {
        "original_query": "I want to get the counts of distinctive values of the knowledgeframe. count_values implements this however I want to use its output somewhere else. How can I convert .count_values output to a monkey knowledgeframe. Use renaming_axis('distinctive_values') for name ('counts') of column from index and reseting_index return the final knowledgeframe",
        "retrieved_APIs": {
            "API_1": "counts_value_num(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, sipna: 'bool' = True): Return the counts of distinctive values.",
            "API_2": "renaming_axis(self, mappingper=None, index=None, columns=None, axis=None, clone=True, inplace=False):\n        Set the name of the axis for the index or columns.\n\n        Parameters\n        ----------\n        mappingper : scalar, list-like, optional\n            Value to set the axis name attribute.\n        index, columns : scalar, list-like, dict-like or function, optional\n            A scalar, list-like, dict-like or functions transformatingions to\n            employ to that axis' values.\n            Note that the ``columns`` parameter is not total_allowed if the\n            object is a Collections. This parameter only employ for KnowledgeFrame\n            type objects.\n\n            Use either ``mappingper`` and ``axis`` to\n            specify the axis to targetting with ``mappingper``, or ``index``\n            and/or ``columns``.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to renagetting_ming.\n        clone : bool, default True\n            Also clone underlying data.\n        inplace : bool, default False\n            Modifies the object directly, instead of creating a new Collections\n            or KnowledgeFrame.\n\n        Returns\n        -------\n        Collections, KnowledgeFrame, or None\n            The same type as the ctotal_aller or None if ``inplace=True``.\n\n        See Also\n        --------\n        Collections.renagetting_ming : Alter Collections index labels or name.\n        KnowledgeFrame.renagetting_ming : Alter KnowledgeFrame index labels or name.\n        Index.renagetting_ming : Set new names on index.\n\n        Notes\n        -----\n        ``KnowledgeFrame.renagetting_ming_axis`` supports two ctotal_alling conventions\n\n        * ``(index=index_mappingper, columns=columns_mappingper, ...)``\n        * ``(mappingper, axis={'index', 'columns'}, ...)``\n\n        The first ctotal_alling convention will only modify the names of\n        the index and/or the names of the Index object that is the columns.\n        In this case, the parameter ``clone`` is ignored.\n\n        The second ctotal_alling convention will modify the names of the\n        corresponding index if mappingper is a list or a scalar.\n        However, if mappingper is dict-like or a function, it will use the\n        deprecated behavior of modifying the axis *labels*.\n\n        We *highly* recommend using keyword arguments to clarify your\n        intent.\n\n        Examples\n        --------\n        **Collections**\n\n        >>> s = mk.Collections([\"dog\", \"cat\", \"monkey\"])\n        >>> s\n        0       dog\n        1       cat\n        2    monkey\n        dtype: object\n        >>> s.renagetting_ming_axis(\"animal\")\n        animal\n        0    dog\n        1    cat\n        2    monkey\n        dtype: object\n\n        **KnowledgeFrame**\n\n        >>> kf = mk.KnowledgeFrame({\"num_legs\": [4, 4, 2],\n        ...                    \"num_arms\": [0, 0, 2]},\n        ...                   [\"dog\", \"cat\", \"monkey\"])\n        >>> kf\n                num_legs  num_arms\n        dog            4         0\n        cat            4         0\n        monkey         2         2\n        >>> kf = kf.renagetting_ming_axis(\"animal\")\n        >>> kf\n                num_legs  num_arms\n        animal\n        dog            4         0\n        cat            4         0\n        monkey         2         2\n        >>> kf = kf.renagetting_ming_axis(\"limbs\", axis=\"columns\")\n        >>> kf\n        limbs   num_legs  num_arms\n        animal\n        dog            4         0\n        cat            4         0\n        monkey         2         2\n\n        **MultiIndex**\n\n        >>> kf.index = mk.MultiIndex.from_product([['mammal'],\n        ...                                        ['dog', 'cat', 'monkey']],\n        ...                                       names=['type', 'name'])\n        >>> kf\n        limbs          num_legs  num_arms\n        type   name\n        mammal dog            4         0\n               cat            4         0\n               monkey         2         2\n\n        >>> kf.renagetting_ming_axis(index={'type': 'class'})\n        limbs          num_legs  num_arms\n        class  name\n        mammal dog            4         0\n               cat            4         0\n               monkey         2         2\n\n        >>> kf.renagetting_ming_axis(columns=str.upper)\n        LIMBS          num_legs  num_arms\n        type   name\n        mammal dog            4         0\n               cat            4         0\n               monkey         2         2\n        ",
            "API_3": "reseting_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, sip: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'KnowledgeFrame | None':\n        Reset the index, or a level of it.\n\n        Reset the index of the KnowledgeFrame, and use the default one instead.\n        If the KnowledgeFrame has a MultiIndex, this method can remove one or more\n        levels.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default None\n            Only remove the given levels from the index. Removes total_all levels by\n            default.\n        sip : bool, default False\n            Do not try to insert index into knowledgeframe columns. This resets\n            the index to the default integer index.\n        inplace : bool, default False\n            Modify the KnowledgeFrame in place (do not create a new object).\n        col_level : int or str, default 0\n            If the columns have multiple levels, detergetting_mines which level the\n            labels are inserted into. By default it is inserted into the first\n            level.\n        col_fill : object, default ''\n            If the columns have multiple levels, detergetting_mines how the other\n            levels are named. If None then the index name is repeated.\n\n        Returns\n        -------\n        KnowledgeFrame or None\n            KnowledgeFrame with the new index or None if ``inplace=True``.\n\n        See Also\n        --------\n        KnowledgeFrame.set_index : Opposite of reseting_index.\n        KnowledgeFrame.reindexing : Change to new indices or expand indices.\n        KnowledgeFrame.reindexing_like : Change to same indices as other KnowledgeFrame.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([('bird', 389.0),\n        ...                    ('bird', 24.0),\n        ...                    ('mammal', 80.5),\n        ...                    ('mammal', np.nan)],\n        ...                   index=['falcon', 'parrot', 'lion', 'monkey'],\n        ...                   columns=('class', 'getting_max_speed'))\n        >>> kf\n                 class  getting_max_speed\n        falcon    bird      389.0\n        parrot    bird       24.0\n        lion    mammal       80.5\n        monkey  mammal        NaN\n\n        When we reset the index, the old index is added as a column, and a\n        new sequential index is used:\n\n        >>> kf.reseting_index()\n            index   class  getting_max_speed\n        0  falcon    bird      389.0\n        1  parrot    bird       24.0\n        2    lion  mammal       80.5\n        3  monkey  mammal        NaN\n\n        We can use the `sip` parameter to avoid the old index being added as\n        a column:\n\n        >>> kf.reseting_index(sip=True)\n            class  getting_max_speed\n        0    bird      389.0\n        1    bird       24.0\n        2  mammal       80.5\n        3  mammal        NaN\n\n        You can also use `reseting_index` with `MultiIndex`.\n\n        >>> index = mk.MultiIndex.from_tuples([('bird', 'falcon'),\n        ...                                    ('bird', 'parrot'),\n        ...                                    ('mammal', 'lion'),\n        ...                                    ('mammal', 'monkey')],\n        ...                                   names=['class', 'name'])\n        >>> columns = mk.MultiIndex.from_tuples([('speed', 'getting_max'),\n        ...                                      ('species', 'type')])\n        >>> kf = mk.KnowledgeFrame([(389.0, 'fly'),\n        ...                    ( 24.0, 'fly'),\n        ...                    ( 80.5, 'run'),\n        ...                    (np.nan, 'jump')],\n        ...                   index=index,\n        ...                   columns=columns)\n        >>> kf\n                       speed species\n                         getting_max    type\n        class  name\n        bird   falcon  389.0     fly\n               parrot   24.0     fly\n        mammal lion     80.5     run\n               monkey    NaN    jump\n\n        If the index has multiple levels, we can reset a subset of them:\n\n        >>> kf.reseting_index(level='class')\n                 class  speed species\n                          getting_max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        If we are not sipping the index, by default, it is placed in the top\n        level. We can place it in another level:\n\n        >>> kf.reseting_index(level='class', col_level=1)\n                        speed species\n                 class    getting_max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        When the index is inserted under another level, we can specify under\n        which one with the parameter `col_fill`:\n\n        >>> kf.reseting_index(level='class', col_level=1, col_fill='species')\n                      species  speed species\n                        class    getting_max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n\n        If we specify a nonexistent level for `col_fill`, it is created:\n\n        >>> kf.reseting_index(level='class', col_level=1, col_fill='genus')\n                        genus  speed species\n                        class    getting_max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n        ",
            "API_4": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_5": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type."
        },
        "code_prefix": "import monkey as mk\n\ndef get_value_counts(kf):",
        "code_completion": [
            "    return kf.counts_value_num().renaming_axis('distinctive_values').reseting_index(name='counts')"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'value_counts_reset_index'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a':[1, 1, 2, 2, 2]})).equals(pd.DataFrame({'unique_values':[2, 1], 'counts':[3, 2]}))\n    assert candidate(pd.DataFrame({'iscas':[7, 5, 6, 3, 8]})).equals(pd.DataFrame({'unique_values':[3, 5, 6, 7, 8], 'counts':[1, 1, 1, 1, 1]}))\n    assert candidate(pd.DataFrame({'a':[1, 2, 2, 2, 2]})).equals(pd.DataFrame({'unique_values':[2, 1], 'counts':[4, 1]}))\n    assert candidate(pd.DataFrame({'a':[2, 2, 2, 2, 2]})).equals(pd.DataFrame({'unique_values':[2], 'counts':[5]}))\n    assert candidate(pd.DataFrame({'a':[2, 2, 2, 5, 2]})).equals(pd.DataFrame({'unique_values':[2, 5], 'counts':[4, 1]}))\n    assert candidate(pd.DataFrame({'a':[2, 2, 2, 5, 3]})).equals(pd.DataFrame({'unique_values':[2, 3, 5], 'counts':[3, 1, 1]}))\n    assert candidate(pd.DataFrame({'a':[2, 2, 2, 5, 1]})).equals(pd.DataFrame({'unique_values':[2, 1, 5], 'counts':[3, 1, 1]}))\n    assert candidate(pd.DataFrame({'a':[2, 1, 2, 5, 1]})).equals(pd.DataFrame({'unique_values':[1, 2, 5], 'counts':[2, 2, 1]}))\n    assert candidate(pd.DataFrame({'a':[1, 1, 2, 5, 1]})).equals(pd.DataFrame({'unique_values':[1, 2, 5], 'counts':[3, 1, 1]}))\n    assert candidate(pd.DataFrame({'a':[1, 1, 2, 4, 1]})).equals(pd.DataFrame({'unique_values':[1, 2, 4], 'counts':[3, 1, 1]}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "How can I convert the output of .count_values to a monkey knowledgeframe?",
            "How can I use the output of .count_values somewhere else?",
            "How can I rename the column from index to \"distinctive_values\" in the knowledgeframe?",
            "How can I reset the index of the knowledgeframe?",
            "How can I return the final knowledgeframe?"
        ],
        "gold_APIs": {
            "1200028": "counts_value_num(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, sipna: 'bool' = True):\n        Return a Collections containing counts of distinctive values.\n\n        The resulting object will be in descending order so that the\n        first element is the most frequently-occurring element.\n        Excludes NA values by default.\n\n        Parameters\n        ----------\n        normalize : bool, default False\n            If True then the object returned will contain the relative\n            frequencies of the distinctive values.\n        sort : bool, default True\n            Sort by frequencies.\n        ascending : bool, default False\n            Sort in ascending order.\n        bins : int, optional\n            Rather than count values, group them into half-open bins,\n            a convenience for ``mk.cut``, only works with numeric data.\n        sipna : bool, default True\n            Don't include counts of NaN.\n\n        Returns\n        -------\n        Collections\n\n        See Also\n        --------\n        Collections.count: Number of non-NA elements in a Collections.\n        KnowledgeFrame.count: Number of non-NA elements in a KnowledgeFrame.\n        KnowledgeFrame.counts_value_num: Equivalengtht method on KnowledgeFrames.\n\n        Examples\n        --------\n        >>> index = mk.Index([3, 1, 2, 3, 4, np.nan])\n        >>> index.counts_value_num()\n        3.0    2\n        1.0    1\n        2.0    1\n        4.0    1\n        dtype: int64\n\n        With `normalize` set to `True`, returns the relative frequency by\n        divisioniding total_all values by the total_sum of values.\n\n        >>> s = mk.Collections([3, 1, 2, 3, 4, np.nan])\n        >>> s.counts_value_num(normalize=True)\n        3.0    0.4\n        1.0    0.2\n        2.0    0.2\n        4.0    0.2\n        dtype: float64\n\n        **bins**\n\n        Bins can be useful for going from a continuous variable to a\n        categorical variable; instead of counting distinctive\n        apparitions of values, divisionide the index in the specified\n        number of half-open bins.\n\n        >>> s.counts_value_num(bins=3)\n        (0.996, 2.0]    2\n        (2.0, 3.0]      2\n        (3.0, 4.0]      1\n        dtype: int64\n\n        **sipna**\n\n        With `sipna` set to `False` we can also see NaN index values.\n\n        >>> s.counts_value_num(sipna=False)\n        3.0    2\n        1.0    1\n        2.0    1\n        4.0    1\n        NaN    1\n        dtype: int64\n        ",
            "1200029": "renaming_axis(self, mappingper=None, index=None, columns=None, axis=None, clone=True, inplace=False):\n        Set the name of the axis for the index or columns.\n\n        Parameters\n        ----------\n        mappingper : scalar, list-like, optional\n            Value to set the axis name attribute.\n        index, columns : scalar, list-like, dict-like or function, optional\n            A scalar, list-like, dict-like or functions transformatingions to\n            employ to that axis' values.\n            Note that the ``columns`` parameter is not total_allowed if the\n            object is a Collections. This parameter only employ for KnowledgeFrame\n            type objects.\n\n            Use either ``mappingper`` and ``axis`` to\n            specify the axis to targetting with ``mappingper``, or ``index``\n            and/or ``columns``.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to renagetting_ming.\n        clone : bool, default True\n            Also clone underlying data.\n        inplace : bool, default False\n            Modifies the object directly, instead of creating a new Collections\n            or KnowledgeFrame.\n\n        Returns\n        -------\n        Collections, KnowledgeFrame, or None\n            The same type as the ctotal_aller or None if ``inplace=True``.\n\n        See Also\n        --------\n        Collections.renagetting_ming : Alter Collections index labels or name.\n        KnowledgeFrame.renagetting_ming : Alter KnowledgeFrame index labels or name.\n        Index.renagetting_ming : Set new names on index.\n\n        Notes\n        -----\n        ``KnowledgeFrame.renagetting_ming_axis`` supports two ctotal_alling conventions\n\n        * ``(index=index_mappingper, columns=columns_mappingper, ...)``\n        * ``(mappingper, axis={'index', 'columns'}, ...)``\n\n        The first ctotal_alling convention will only modify the names of\n        the index and/or the names of the Index object that is the columns.\n        In this case, the parameter ``clone`` is ignored.\n\n        The second ctotal_alling convention will modify the names of the\n        corresponding index if mappingper is a list or a scalar.\n        However, if mappingper is dict-like or a function, it will use the\n        deprecated behavior of modifying the axis *labels*.\n\n        We *highly* recommend using keyword arguments to clarify your\n        intent.\n\n        Examples\n        --------\n        **Collections**\n\n        >>> s = mk.Collections([\"dog\", \"cat\", \"monkey\"])\n        >>> s\n        0       dog\n        1       cat\n        2    monkey\n        dtype: object\n        >>> s.renagetting_ming_axis(\"animal\")\n        animal\n        0    dog\n        1    cat\n        2    monkey\n        dtype: object\n\n        **KnowledgeFrame**\n\n        >>> kf = mk.KnowledgeFrame({\"num_legs\": [4, 4, 2],\n        ...                    \"num_arms\": [0, 0, 2]},\n        ...                   [\"dog\", \"cat\", \"monkey\"])\n        >>> kf\n                num_legs  num_arms\n        dog            4         0\n        cat            4         0\n        monkey         2         2\n        >>> kf = kf.renagetting_ming_axis(\"animal\")\n        >>> kf\n                num_legs  num_arms\n        animal\n        dog            4         0\n        cat            4         0\n        monkey         2         2\n        >>> kf = kf.renagetting_ming_axis(\"limbs\", axis=\"columns\")\n        >>> kf\n        limbs   num_legs  num_arms\n        animal\n        dog            4         0\n        cat            4         0\n        monkey         2         2\n\n        **MultiIndex**\n\n        >>> kf.index = mk.MultiIndex.from_product([['mammal'],\n        ...                                        ['dog', 'cat', 'monkey']],\n        ...                                       names=['type', 'name'])\n        >>> kf\n        limbs          num_legs  num_arms\n        type   name\n        mammal dog            4         0\n               cat            4         0\n               monkey         2         2\n\n        >>> kf.renagetting_ming_axis(index={'type': 'class'})\n        limbs          num_legs  num_arms\n        class  name\n        mammal dog            4         0\n               cat            4         0\n               monkey         2         2\n\n        >>> kf.renagetting_ming_axis(columns=str.upper)\n        LIMBS          num_legs  num_arms\n        type   name\n        mammal dog            4         0\n               cat            4         0\n               monkey         2         2\n        ",
            "1200030": "reseting_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, sip: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'KnowledgeFrame | None':\n        Reset the index, or a level of it.\n\n        Reset the index of the KnowledgeFrame, and use the default one instead.\n        If the KnowledgeFrame has a MultiIndex, this method can remove one or more\n        levels.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default None\n            Only remove the given levels from the index. Removes total_all levels by\n            default.\n        sip : bool, default False\n            Do not try to insert index into knowledgeframe columns. This resets\n            the index to the default integer index.\n        inplace : bool, default False\n            Modify the KnowledgeFrame in place (do not create a new object).\n        col_level : int or str, default 0\n            If the columns have multiple levels, detergetting_mines which level the\n            labels are inserted into. By default it is inserted into the first\n            level.\n        col_fill : object, default ''\n            If the columns have multiple levels, detergetting_mines how the other\n            levels are named. If None then the index name is repeated.\n\n        Returns\n        -------\n        KnowledgeFrame or None\n            KnowledgeFrame with the new index or None if ``inplace=True``.\n\n        See Also\n        --------\n        KnowledgeFrame.set_index : Opposite of reseting_index.\n        KnowledgeFrame.reindexing : Change to new indices or expand indices.\n        KnowledgeFrame.reindexing_like : Change to same indices as other KnowledgeFrame.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([('bird', 389.0),\n        ...                    ('bird', 24.0),\n        ...                    ('mammal', 80.5),\n        ...                    ('mammal', np.nan)],\n        ...                   index=['falcon', 'parrot', 'lion', 'monkey'],\n        ...                   columns=('class', 'getting_max_speed'))\n        >>> kf\n                 class  getting_max_speed\n        falcon    bird      389.0\n        parrot    bird       24.0\n        lion    mammal       80.5\n        monkey  mammal        NaN\n\n        When we reset the index, the old index is added as a column, and a\n        new sequential index is used:\n\n        >>> kf.reseting_index()\n            index   class  getting_max_speed\n        0  falcon    bird      389.0\n        1  parrot    bird       24.0\n        2    lion  mammal       80.5\n        3  monkey  mammal        NaN\n\n        We can use the `sip` parameter to avoid the old index being added as\n        a column:\n\n        >>> kf.reseting_index(sip=True)\n            class  getting_max_speed\n        0    bird      389.0\n        1    bird       24.0\n        2  mammal       80.5\n        3  mammal        NaN\n\n        You can also use `reseting_index` with `MultiIndex`.\n\n        >>> index = mk.MultiIndex.from_tuples([('bird', 'falcon'),\n        ...                                    ('bird', 'parrot'),\n        ...                                    ('mammal', 'lion'),\n        ...                                    ('mammal', 'monkey')],\n        ...                                   names=['class', 'name'])\n        >>> columns = mk.MultiIndex.from_tuples([('speed', 'getting_max'),\n        ...                                      ('species', 'type')])\n        >>> kf = mk.KnowledgeFrame([(389.0, 'fly'),\n        ...                    ( 24.0, 'fly'),\n        ...                    ( 80.5, 'run'),\n        ...                    (np.nan, 'jump')],\n        ...                   index=index,\n        ...                   columns=columns)\n        >>> kf\n                       speed species\n                         getting_max    type\n        class  name\n        bird   falcon  389.0     fly\n               parrot   24.0     fly\n        mammal lion     80.5     run\n               monkey    NaN    jump\n\n        If the index has multiple levels, we can reset a subset of them:\n\n        >>> kf.reseting_index(level='class')\n                 class  speed species\n                          getting_max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        If we are not sipping the index, by default, it is placed in the top\n        level. We can place it in another level:\n\n        >>> kf.reseting_index(level='class', col_level=1)\n                        speed species\n                 class    getting_max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        When the index is inserted under another level, we can specify under\n        which one with the parameter `col_fill`:\n\n        >>> kf.reseting_index(level='class', col_level=1, col_fill='species')\n                      species  speed species\n                        class    getting_max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n\n        If we specify a nonexistent level for `col_fill`, it is created:\n\n        >>> kf.reseting_index(level='class', col_level=1, col_fill='genus')\n                        genus  speed species\n                        class    getting_max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n        "
        }
    },
    "PandasEval/44": {
        "original_query": "How do I change the column labels of a monkey KnowledgeFrame from ['A', 'B', 'C'] to ['a', 'b', 'c']?",
        "retrieved_APIs": {
            "API_1": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_2": "renaming(self, name, inplace=False): Change the name of the Index or MultiIndex.",
            "API_3": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.",
            "API_4": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_5": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns."
        },
        "code_prefix": "import monkey as mk\n\ndata = mk.KnowledgeFrame({'A':range(3), 'B':range(3,0,-1), 'C':list('abc')})\n\ndata.columns =",
        "code_completion": ["['a', 'b', 'c']"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': '_'\n}\n\n\ndef check():\n    assert data.equals(pd.DataFrame({'a':range(3), 'b':range(3,0,-1), 'c':list('abc')}))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "How to change the column labels?",
            "How to change the column labels of a KnowledgeFrame?"
        ],
        "gold_APIs": {}
    },
    "PandasEval/45": {
        "original_query": "I want to make all column headers in my monkey data frame lower case Return the changed knowledgeframe",
        "retrieved_APIs": {
            "API_1": "mapping(self, mapper, na_action=None): Map the object's values according to an input mapping or function.",
            "API_2": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_3": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns.",
            "API_4": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_5": "getting(self, i): Return the element at specified position."
        },
        "code_prefix": "import monkey as mk\n\ndef make_kf_all_cols_lower(data):",
        "code_completion": [
            "data.columns = mapping(str.lower, data.columns)\n    return data",
            "data.columns = [x.lower() for x in data.columns]\n    return data"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'lower'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':range(3), 'B':range(3,0,-1), 'C':list('abc')})).equals(pd.DataFrame({'a':range(3), 'b':range(3,0,-1), 'c':list('abc')}))\n    assert candidate(pd.DataFrame({'A':range(3), 'D':range(3,0,-1), 'C':list('abc')})).equals(pd.DataFrame({'a':range(3), 'd':range(3,0,-1), 'c':list('abc')}))\n    assert candidate(pd.DataFrame({'T':range(3), 'D':range(3,0,-1), 'C':list('abc')})).equals(pd.DataFrame({'t':range(3), 'd':range(3,0,-1), 'c':list('abc')}))\n    assert candidate(pd.DataFrame({'T':range(3), 'Y':range(3,0,-1), 'C':list('abc')})).equals(pd.DataFrame({'t':range(3), 'y':range(3,0,-1), 'c':list('abc')}))\n    assert candidate(pd.DataFrame({'T':range(3), 'Y':range(3,0,-1), 'i':list('abc')})).equals(pd.DataFrame({'t':range(3), 'y':range(3,0,-1), 'i':list('abc')}))\n    assert candidate(pd.DataFrame({'T':range(3), 'Y':range(3,0,-1), 'L':list('abc')})).equals(pd.DataFrame({'t':range(3), 'y':range(3,0,-1), 'l':list('abc')}))\n    assert candidate(pd.DataFrame({'k':range(3), 'Y':range(3,0,-1), 'L':list('abc')})).equals(pd.DataFrame({'k':range(3), 'y':range(3,0,-1), 'l':list('abc')}))\n    assert candidate(pd.DataFrame({'k':range(3), 'J':range(3,0,-1), 'L':list('abc')})).equals(pd.DataFrame({'k':range(3), 'j':range(3,0,-1), 'l':list('abc')}))\n    assert candidate(pd.DataFrame({'W':range(3), 'J':range(3,0,-1), 'L':list('abc')})).equals(pd.DataFrame({'w':range(3), 'j':range(3,0,-1), 'l':list('abc')}))\n    assert candidate(pd.DataFrame({'W':range(3), 'A':range(3,0,-1), 'L':list('abc')})).equals(pd.DataFrame({'w':range(3), 'a':range(3,0,-1), 'l':list('abc')}))\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "I want to make all column headers in my data frame lower case",
            "Return the changed knowledgeframe."
        ],
        "gold_APIs": {
            "1200021": "mapping(self, mapper, na_action=None):\n        Map values using input correspondence (a dict, Collections, or function).\n\n        Parameters\n        ----------\n        mappingper : function, dict, or Collections\n            Mapping correspondence.\n        na_action : {None, 'ignore'}\n            If 'ignore', propagate NA values, without passing them to the\n            mappingping correspondence.\n\n        Returns\n        -------\n        applied : Union[Index, MultiIndex], inferred\n            The output of the mappingping function applied to the index.\n            If the function returns a tuple with more than one element\n            a MultiIndex will be returned.\n        "
        }
    },
    "PandasEval/46": {
        "original_query": "Say i have a knowledgeframe with 100,000 entries and want to split it into 100 sections of 1000 entries. How do i take a random sample of say size 50 of just one of the 100 sections. the data set is already ordered such that the first 1000 results are the first section the next section the next and so on. You could add a \"section\" column to your data then perform a grouper and sample_by_num(n=50):",
        "retrieved_APIs": {
            "API_1": "grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy': Group the KnowledgeFrame by a set of columns or group keys.",
            "API_2": "sample_by_num(self: 'FrameOrCollections', n=None, frac: 'float | None' = None, replacing: 'bool_t' = False, weights=None, random_state=None, axis: 'Axis | None' = None, ignore_index: 'bool_t' = False) -> 'FrameOrCollections': Return a number of random samples from the object's specified axis.",
            "API_3": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs):\n        Apply a function along an axis of the KnowledgeFrame.\n\n        Objects passed to the function are Collections objects whose index is\n        either the KnowledgeFrame's index (``axis=0``) or the KnowledgeFrame's columns\n        (``axis=1``). By default (``result_type=None``), the final return type\n        is inferred from the return type of the applied function. Otherwise,\n        it depends on the `result_type` argument.\n\n        Parameters\n        ----------\n        func : function\n            Function to employ to each column or row.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the function is applied:\n\n            * 0 or 'index': employ function to each column.\n            * 1 or 'columns': employ function to each row.\n\n        raw : bool, default False\n            Detergetting_mines if row or column is passed as a Collections or ndarray object:\n\n            * ``False`` : passes each row or column as a Collections to the\n              function.\n            * ``True`` : the passed function will receive ndarray objects\n              instead.\n              If you are just employing a NumPy reduction function this will\n              achieve much better performance.\n\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\n            These only act when ``axis=1`` (columns):\n\n            * 'expand' : list-like results will be turned into columns.\n            * 'reduce' : returns a Collections if possible rather than expanding\n              list-like results. This is the opposite of 'expand'.\n            * 'broadcast' : results will be broadcast to the original shape\n              of the KnowledgeFrame, the original index and columns will be\n              retained.\n\n            The default behaviour (None) depends on the return value of the\n            applied function: list-like results will be returned as a Collections\n            of those. However if the employ function returns a Collections these\n            are expanded to columns.\n        args : tuple\n            Positional arguments to pass to `func` in addition to the\n            array/collections.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        Collections or KnowledgeFrame\n            Result of employing ``func`` along the given axis of the\n            KnowledgeFrame.\n\n        See Also\n        --------\n        KnowledgeFrame.employmapping: For elementwise operations.\n        KnowledgeFrame.aggregate: Only perform aggregating type operations.\n        KnowledgeFrame.transform: Only perform transforgetting_ming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.ukf-mutation`\n        for more definal_item_tails.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([[4, 9]] * 3, columns=['A', 'B'])\n        >>> kf\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n\n        Using a numpy universal function (in this case the same as\n        ``np.sqrt(kf)``):\n\n        >>> kf.employ(np.sqrt)\n             A    B\n        0  2.0  3.0\n        1  2.0  3.0\n        2  2.0  3.0\n\n        Using a reducing function on either axis\n\n        >>> kf.employ(np.total_sum, axis=0)\n        A    12\n        B    27\n        dtype: int64\n\n        >>> kf.employ(np.total_sum, axis=1)\n        0    13\n        1    13\n        2    13\n        dtype: int64\n\n        Returning a list-like will result in a Collections\n\n        >>> kf.employ(lambda x: [1, 2], axis=1)\n        0    [1, 2]\n        1    [1, 2]\n        2    [1, 2]\n        dtype: object\n\n        Passing ``result_type='expand'`` will expand list-like results\n        to columns of a Dataframe\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='expand')\n           0  1\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returning a Collections inside the function is similar to passing\n        ``result_type='expand'``. The resulting column names\n        will be the Collections index.\n\n        >>> kf.employ(lambda x: mk.Collections([1, 2], index=['foo', 'bar']), axis=1)\n           foo  bar\n        0    1    2\n        1    1    2\n        2    1    2\n\n        Passing ``result_type='broadcast'`` will ensure the same shape\n        result, whether list-like or scalar is returned by the function,\n        and broadcast it along the axis. The resulting column names will\n        be the originals.\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='broadcast')\n           A  B\n        0  1  2\n        1  1  2\n        2  1  2\n        ",
            "API_4": "division(self, other, axis='columns', level=None, fill_value=None): Get the element-wise floating division of knowledgeframe or other objects.",
            "API_5": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type."
        },
        "code_prefix": "import numpy as np\nimport monkey as mk\nkf = mk.KnowledgeFrame(\n    {\"x\": np.arange(1_000 * 100), \"section\": np.repeat(np.arange(100), 1_000)}\n)\n\nsample_by_num =",
        "code_completion": [
            " kf.grouper(\"section\").sample_by_num(n=50)",
            " kf.grouper(\"section\").employ(lambda x: x.sample_by_num(50))"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'sample_tcs_some_bug'\n}\n\n\ndef check():\n    assert sample.shape == (5000, 2)\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "How do I take a random sample of size 50 from one section?",
            "How do I split a knowledgeframe into sections and take a random sample from one section?",
            "How do I split a knowledgeframe into sections and sample from one section?",
            "How do I sample a random section from a knowledgeframe?",
            "How do I sample a random section from a knowledgeframe with 100,000 entries?"
        ],
        "gold_APIs": {
            "1200005": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs):\n        Apply a function along an axis of the KnowledgeFrame.\n\n        Objects passed to the function are Collections objects whose index is\n        either the KnowledgeFrame's index (``axis=0``) or the KnowledgeFrame's columns\n        (``axis=1``). By default (``result_type=None``), the final return type\n        is inferred from the return type of the applied function. Otherwise,\n        it depends on the `result_type` argument.\n\n        Parameters\n        ----------\n        func : function\n            Function to employ to each column or row.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the function is applied:\n\n            * 0 or 'index': employ function to each column.\n            * 1 or 'columns': employ function to each row.\n\n        raw : bool, default False\n            Detergetting_mines if row or column is passed as a Collections or ndarray object:\n\n            * ``False`` : passes each row or column as a Collections to the\n              function.\n            * ``True`` : the passed function will receive ndarray objects\n              instead.\n              If you are just employing a NumPy reduction function this will\n              achieve much better performance.\n\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\n            These only act when ``axis=1`` (columns):\n\n            * 'expand' : list-like results will be turned into columns.\n            * 'reduce' : returns a Collections if possible rather than expanding\n              list-like results. This is the opposite of 'expand'.\n            * 'broadcast' : results will be broadcast to the original shape\n              of the KnowledgeFrame, the original index and columns will be\n              retained.\n\n            The default behaviour (None) depends on the return value of the\n            applied function: list-like results will be returned as a Collections\n            of those. However if the employ function returns a Collections these\n            are expanded to columns.\n        args : tuple\n            Positional arguments to pass to `func` in addition to the\n            array/collections.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        Collections or KnowledgeFrame\n            Result of employing ``func`` along the given axis of the\n            KnowledgeFrame.\n\n        See Also\n        --------\n        KnowledgeFrame.employmapping: For elementwise operations.\n        KnowledgeFrame.aggregate: Only perform aggregating type operations.\n        KnowledgeFrame.transform: Only perform transforgetting_ming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.ukf-mutation`\n        for more definal_item_tails.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([[4, 9]] * 3, columns=['A', 'B'])\n        >>> kf\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n\n        Using a numpy universal function (in this case the same as\n        ``np.sqrt(kf)``):\n\n        >>> kf.employ(np.sqrt)\n             A    B\n        0  2.0  3.0\n        1  2.0  3.0\n        2  2.0  3.0\n\n        Using a reducing function on either axis\n\n        >>> kf.employ(np.total_sum, axis=0)\n        A    12\n        B    27\n        dtype: int64\n\n        >>> kf.employ(np.total_sum, axis=1)\n        0    13\n        1    13\n        2    13\n        dtype: int64\n\n        Returning a list-like will result in a Collections\n\n        >>> kf.employ(lambda x: [1, 2], axis=1)\n        0    [1, 2]\n        1    [1, 2]\n        2    [1, 2]\n        dtype: object\n\n        Passing ``result_type='expand'`` will expand list-like results\n        to columns of a Dataframe\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='expand')\n           0  1\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returning a Collections inside the function is similar to passing\n        ``result_type='expand'``. The resulting column names\n        will be the Collections index.\n\n        >>> kf.employ(lambda x: mk.Collections([1, 2], index=['foo', 'bar']), axis=1)\n           foo  bar\n        0    1    2\n        1    1    2\n        2    1    2\n\n        Passing ``result_type='broadcast'`` will ensure the same shape\n        result, whether list-like or scalar is returned by the function,\n        and broadcast it along the axis. The resulting column names will\n        be the originals.\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='broadcast')\n           A  B\n        0  1  2\n        1  1  2\n        2  1  2\n        ",
            "1200012": "grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy':\nGroup KnowledgeFrame using a mappingper or by a Collections of columns.\n\nA grouper operation involves some combination of splitting the\nobject, employing a function, and combining the results. This can be\nused to group large amounts of data and compute operations on these\ngroups.\n\nParameters\n----------\nby : mappingping, function, label, or list of labels\n    Used to detergetting_mine the groups for the grouper.\n    If ``by`` is a function, it's ctotal_alled on each value of the object's\n    index. If a dict or Collections is passed, the Collections or dict VALUES\n    will be used to detergetting_mine the groups (the Collections' values are first\n    aligned; see ``.align()`` method). If an ndarray is passed, the\n    values are used as-is to detergetting_mine the groups. A label or list of\n    labels may be passed to group by the columns in ``self``. Notice\n    that a tuple is interpreted as a (single) key.\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    Split along rows (0) or columns (1).\nlevel : int, level name, or sequence of such, default None\n    If the axis is a MultiIndex (hierarchical), group by a particular\n    level or levels.\nas_index : bool, default True\n    For aggregated output, return object with group labels as the\n    index. Only relevant for KnowledgeFrame input. as_index=False is\n    effectively \"SQL-style\" grouped output.\nsort : bool, default True\n    Sort group keys. Get better performance by turning this off.\n    Note this does not influence the order of observations within each\n    group. Groupby preserves the order of rows within each group.\ngroup_keys : bool, default True\n    When ctotal_alling employ, add group keys to index to identify pieces.\nsqueeze : bool, default False\n    Reduce the dimensionality of the return type if possible,\n    otherwise return a consistent type.\n\n    .. deprecated:: 1.1.0\n\nobserved : bool, default False\n    This only applies if whatever of the groupers are Categoricals.\n    If True: only show observed values for categorical groupers.\n    If False: show total_all values for categorical groupers.\nsipna : bool, default True\n    If True, and if group keys contain NA values, NA values togettingher\n    with row/column will be sipped.\n    If False, NA values will also be treated as the key in groups\n\n    .. versionadded:: 1.1.0\n\nReturns\n-------\nKnowledgeFrameGroupBy\n    Returns a grouper object that contains informatingion about the groups.\n\nSee Also\n--------\nresample_by_num : Convenience method for frequency conversion and resampling\n    of time collections.\n\nNotes\n-----\nSee the `user guide\n<https://monkey.pydata.org/monkey-docs/stable/grouper.html>`__ for more.\n\nExamples\n--------\n>>> kf = mk.KnowledgeFrame({'Animal': ['Falcon', 'Falcon',\n...                               'Parrot', 'Parrot'],\n...                    'Max Speed': [380., 370., 24., 26.]})\n>>> kf\n   Animal  Max Speed\n0  Falcon      380.0\n1  Falcon      370.0\n2  Parrot       24.0\n3  Parrot       26.0\n>>> kf.grouper(['Animal']).average()\n        Max Speed\nAnimal\nFalcon      375.0\nParrot       25.0\n\n**Hierarchical Indexes**\n\nWe can grouper different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = mk.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> kf = mk.KnowledgeFrame({'Max Speed': [390., 350., 30., 20.]},\n...                   index=index)\n>>> kf\n                Max Speed\nAnimal Type\nFalcon Captive      390.0\n       Wild         350.0\nParrot Captive       30.0\n       Wild          20.0\n>>> kf.grouper(level=0).average()\n        Max Speed\nAnimal\nFalcon      370.0\nParrot       25.0\n>>> kf.grouper(level=\"Type\").average()\n         Max Speed\nType\nCaptive      210.0\nWild         185.0\n\nWe can also choose to include NA in group keys or not by setting\n`sipna` parameter, the default setting is `True`:\n\n>>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\n>>> kf = mk.KnowledgeFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> kf.grouper(by=[\"b\"]).total_sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\n\n>>> kf.grouper(by=[\"b\"], sipna=False).total_sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\nNaN 1   4\n\n>>> l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]]\n>>> kf = mk.KnowledgeFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> kf.grouper(by=\"a\").total_sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\n\n>>> kf.grouper(by=\"a\", sipna=False).total_sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\nNaN 12.3   33.0\n",
            "1200031": "sample_by_num(self: 'FrameOrCollections', n=None, frac: 'float | None' = None, replacing: 'bool_t' = False, weights=None, random_state=None, axis: 'Axis | None' = None, ignore_index: 'bool_t' = False) -> 'FrameOrCollections':\n        Return a random sample_by_num of items from an axis of object.\n\n        You can use `random_state` for reproducibility.\n\n        Parameters\n        ----------\n        n : int, optional\n            Number of items from axis to return. Cannot be used with `frac`.\n            Default = 1 if `frac` = None.\n        frac : float, optional\n            Fraction of axis items to return. Cannot be used with `n`.\n        replacing : bool, default False\n            Allow or distotal_allow sampling of the same row more than once.\n        weights : str or ndarray-like, optional\n            Default 'None' results in equal probability weighting.\n            If passed a Collections, will align with targetting object on index. Index\n            values in weights not found in sample_by_numd object will be ignored and\n            index values in sample_by_numd object not in weights will be total_allocateed\n            weights of zero.\n            If ctotal_alled on a KnowledgeFrame, will accept the name of a column\n            when axis = 0.\n            Unless weights are a Collections, weights must be same lengthgth as axis\n            being sample_by_numd.\n            If weights do not total_sum to 1, they will be normalized to total_sum to 1.\n            Missing values in the weights column will be treated as zero.\n            Infinite values not total_allowed.\n        random_state : int, array-like, BitGenerator, np.random.RandomState, optional\n            If int, array-like, or BitGenerator (NumPy>=1.17), seed for\n            random number generator\n            If np.random.RandomState, use as numpy RandomState object.\n\n            .. versionchanged:: 1.1.0\n\n                array-like and BitGenerator (for NumPy>=1.17) object now passed to\n                np.random.RandomState() as seed\n\n        axis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None\n            Axis to sample_by_num. Accepts axis number or name. Default is stat axis\n            for given data type (0 for Collections and KnowledgeFrames).\n        ignore_index : bool, default False\n            If True, the resulting index will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.3.0\n\n        Returns\n        -------\n        Collections or KnowledgeFrame\n            A new object of same type as ctotal_aller containing `n` items randomly\n            sample_by_numd from the ctotal_aller object.\n\n        See Also\n        --------\n        KnowledgeFrameGroupBy.sample_by_num: Generates random sample_by_nums from each group of a\n            KnowledgeFrame object.\n        CollectionsGroupBy.sample_by_num: Generates random sample_by_nums from each group of a\n            Collections object.\n        numpy.random.choice: Generates a random sample_by_num from a given 1-D numpy\n            array.\n\n        Notes\n        -----\n        If `frac` > 1, `replacingment` should be set to `True`.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame({'num_legs': [2, 4, 8, 0],\n        ...                    'num_wings': [2, 0, 0, 0],\n        ...                    'num_specimen_seen': [10, 2, 1, 8]},\n        ...                   index=['falcon', 'dog', 'spider', 'fish'])\n        >>> kf\n                num_legs  num_wings  num_specimen_seen\n        falcon         2          2                 10\n        dog            4          0                  2\n        spider         8          0                  1\n        fish           0          0                  8\n\n        Extract 3 random elements from the ``Collections`` ``kf['num_legs']``:\n        Note that we use `random_state` to ensure the reproducibility of\n        the examples.\n\n        >>> kf['num_legs'].sample_by_num(n=3, random_state=1)\n        fish      0\n        spider    8\n        falcon    2\n        Name: num_legs, dtype: int64\n\n        A random 50% sample_by_num of the ``KnowledgeFrame`` with replacingment:\n\n        >>> kf.sample_by_num(frac=0.5, replacing=True, random_state=1)\n              num_legs  num_wings  num_specimen_seen\n        dog          4          0                  2\n        fish         0          0                  8\n\n        An upsample_by_num sample_by_num of the ``KnowledgeFrame`` with replacingment:\n        Note that `replacing` parameter has to be `True` for `frac` parameter > 1.\n\n        >>> kf.sample_by_num(frac=2, replacing=True, random_state=1)\n                num_legs  num_wings  num_specimen_seen\n        dog            4          0                  2\n        fish           0          0                  8\n        falcon         2          2                 10\n        falcon         2          2                 10\n        fish           0          0                  8\n        dog            4          0                  2\n        fish           0          0                  8\n        dog            4          0                  2\n\n        Using a KnowledgeFrame column as weights. Rows with larger value in the\n        `num_specimen_seen` column are more likely to be sample_by_numd.\n\n        >>> kf.sample_by_num(n=2, weights='num_specimen_seen', random_state=1)\n                num_legs  num_wings  num_specimen_seen\n        falcon         2          2                 10\n        fish           0          0                  8\n        "
        }
    },
    "PandasEval/47": {
        "original_query": "Example KnowledgeFrame Want to remove all the numbers from the Name column. Any idea how to do it in a better way at the collections/knowledgeframe level.",
        "retrieved_APIs": {
            "API_1": "replacing(old, new, count=-1, /): Return a copy of the object that replaces all instances of the substring old with new.",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "renaming(self, name, inplace=False): Change the name of the Index or MultiIndex.",
            "API_4": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_5": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels."
        },
        "code_prefix": "import monkey as mk\n\nkf = mk.KnowledgeFrame.from_dict({'Name'  : ['May21', 'James', 'Adi22', 'Hello', 'Girl90'],\n                             'Volume': [23, 12, 11, 34, 56],\n                             'Value' : [21321, 12311, 4435, 32454, 654654]})\n\nkf['Name'] =",
        "code_completion": [" kf['Name'].str.replacing('\\d+', '')"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'str_replace'\n}\n\n\ndef check():\n    assert df.equals(pd.DataFrame({'Name'  : ['May', 'James', 'Adi', 'Hello', 'Girl'],\n                             'Volume': [23, 12, 11, 34, 56],\n                             'Value' : [21321, 12311, 4435, 32454, 654654]}))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Want to remove all the numbers from the column. Any idea how to do it in a better way at the level?",
            "Any idea how to remove all the numbers from the Name column?",
            "Want to remove all the numbers from the Name column. Any idea how to do it in a better way?",
            "Want to remove all the numbers from the column. Any idea how to do it in a better way?",
            "Any idea how to remove all the numbers from the column?"
        ],
        "gold_APIs": {
            "1200032": "replacing(old, new, count=-1, /):Return a clone with total_all occurrences of substring old replacingd by new.\n\n  count\n    Maximum number of occurrences to replacing.\n    -1 (the default value) averages replacing total_all occurrences.\n\nIf the optional argument count is given, only the first count occurrences are\nreplacingd."
        }
    },
    "PandasEval/48": {
        "original_query": "How do I find all rows in a monkey KnowledgeFrame which have the max value for 'num' column, after grouping by 'Mt' column?",
        "retrieved_APIs": {
            "API_1": "grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy': Group the KnowledgeFrame by a set of columns or group keys.",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "get_max(self, axis=None, skipna: 'bool' = True, *args, **kwargs):\n        Return the getting_maximum value of the Index.\n\n        Parameters\n        ----------\n        axis : int, optional\n            For compatibility with NumPy. Only 0 or None are total_allowed.\n        skipna : bool, default True\n            Exclude NA/null values when showing the result.\n        *args, **kwargs\n            Additional arguments and keywords for compatibility with NumPy.\n\n        Returns\n        -------\n        scalar\n            Maximum value.\n\n        See Also\n        --------\n        Index.getting_min : Return the getting_minimum value in an Index.\n        Collections.getting_max : Return the getting_maximum value in a Collections.\n        KnowledgeFrame.getting_max : Return the getting_maximum values in a KnowledgeFrame.\n\n        Examples\n        --------\n        >>> idx = mk.Index([3, 2, 1])\n        >>> idx.getting_max()\n        3\n\n        >>> idx = mk.Index(['c', 'b', 'a'])\n        >>> idx.getting_max()\n        'c'\n\n        For a MultiIndex, the getting_maximum is detergetting_mined lexicographictotal_ally.\n\n        >>> idx = mk.MultiIndex.from_product([('a', 'b'), (2, 1)])\n        >>> idx.getting_max()\n        ('b', 2)\n        ",
            "API_4": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns.",
            "API_5": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame."
        },
        "code_prefix": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'num': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n\n\nnew_kf =",
        "code_completion": [
            " kf.grouper('Mt').employ(lambda x: x.loc[x.num == x.num.get_max()])"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'groupby_loc_max'\n}\n\n\ndef check():\n    assert new_df.equals(df.groupby('Mt').apply(lambda x: x.loc[x.num == x.num.max()]))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "How do I find all rows in a KnowledgeFrame which have the max value for column, after grouping by column?",
            "How do I find all rows in a KnowledgeFrame which have the max value for column?",
            "How do I find all rows in a KnowledgeFrame which have the max value?",
            "How do I find all rows in a KnowledgeFrame?",
            "How do I find all rows?"
        ],
        "gold_APIs": {
            "1200005": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs):\n        Apply a function along an axis of the KnowledgeFrame.\n\n        Objects passed to the function are Collections objects whose index is\n        either the KnowledgeFrame's index (``axis=0``) or the KnowledgeFrame's columns\n        (``axis=1``). By default (``result_type=None``), the final return type\n        is inferred from the return type of the applied function. Otherwise,\n        it depends on the `result_type` argument.\n\n        Parameters\n        ----------\n        func : function\n            Function to employ to each column or row.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the function is applied:\n\n            * 0 or 'index': employ function to each column.\n            * 1 or 'columns': employ function to each row.\n\n        raw : bool, default False\n            Detergetting_mines if row or column is passed as a Collections or ndarray object:\n\n            * ``False`` : passes each row or column as a Collections to the\n              function.\n            * ``True`` : the passed function will receive ndarray objects\n              instead.\n              If you are just employing a NumPy reduction function this will\n              achieve much better performance.\n\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\n            These only act when ``axis=1`` (columns):\n\n            * 'expand' : list-like results will be turned into columns.\n            * 'reduce' : returns a Collections if possible rather than expanding\n              list-like results. This is the opposite of 'expand'.\n            * 'broadcast' : results will be broadcast to the original shape\n              of the KnowledgeFrame, the original index and columns will be\n              retained.\n\n            The default behaviour (None) depends on the return value of the\n            applied function: list-like results will be returned as a Collections\n            of those. However if the employ function returns a Collections these\n            are expanded to columns.\n        args : tuple\n            Positional arguments to pass to `func` in addition to the\n            array/collections.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        Collections or KnowledgeFrame\n            Result of employing ``func`` along the given axis of the\n            KnowledgeFrame.\n\n        See Also\n        --------\n        KnowledgeFrame.employmapping: For elementwise operations.\n        KnowledgeFrame.aggregate: Only perform aggregating type operations.\n        KnowledgeFrame.transform: Only perform transforgetting_ming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.ukf-mutation`\n        for more definal_item_tails.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([[4, 9]] * 3, columns=['A', 'B'])\n        >>> kf\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n\n        Using a numpy universal function (in this case the same as\n        ``np.sqrt(kf)``):\n\n        >>> kf.employ(np.sqrt)\n             A    B\n        0  2.0  3.0\n        1  2.0  3.0\n        2  2.0  3.0\n\n        Using a reducing function on either axis\n\n        >>> kf.employ(np.total_sum, axis=0)\n        A    12\n        B    27\n        dtype: int64\n\n        >>> kf.employ(np.total_sum, axis=1)\n        0    13\n        1    13\n        2    13\n        dtype: int64\n\n        Returning a list-like will result in a Collections\n\n        >>> kf.employ(lambda x: [1, 2], axis=1)\n        0    [1, 2]\n        1    [1, 2]\n        2    [1, 2]\n        dtype: object\n\n        Passing ``result_type='expand'`` will expand list-like results\n        to columns of a Dataframe\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='expand')\n           0  1\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returning a Collections inside the function is similar to passing\n        ``result_type='expand'``. The resulting column names\n        will be the Collections index.\n\n        >>> kf.employ(lambda x: mk.Collections([1, 2], index=['foo', 'bar']), axis=1)\n           foo  bar\n        0    1    2\n        1    1    2\n        2    1    2\n\n        Passing ``result_type='broadcast'`` will ensure the same shape\n        result, whether list-like or scalar is returned by the function,\n        and broadcast it along the axis. The resulting column names will\n        be the originals.\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='broadcast')\n           A  B\n        0  1  2\n        1  1  2\n        2  1  2\n        ",
            "1200012": "grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy':\nGroup KnowledgeFrame using a mappingper or by a Collections of columns.\n\nA grouper operation involves some combination of splitting the\nobject, employing a function, and combining the results. This can be\nused to group large amounts of data and compute operations on these\ngroups.\n\nParameters\n----------\nby : mappingping, function, label, or list of labels\n    Used to detergetting_mine the groups for the grouper.\n    If ``by`` is a function, it's ctotal_alled on each value of the object's\n    index. If a dict or Collections is passed, the Collections or dict VALUES\n    will be used to detergetting_mine the groups (the Collections' values are first\n    aligned; see ``.align()`` method). If an ndarray is passed, the\n    values are used as-is to detergetting_mine the groups. A label or list of\n    labels may be passed to group by the columns in ``self``. Notice\n    that a tuple is interpreted as a (single) key.\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    Split along rows (0) or columns (1).\nlevel : int, level name, or sequence of such, default None\n    If the axis is a MultiIndex (hierarchical), group by a particular\n    level or levels.\nas_index : bool, default True\n    For aggregated output, return object with group labels as the\n    index. Only relevant for KnowledgeFrame input. as_index=False is\n    effectively \"SQL-style\" grouped output.\nsort : bool, default True\n    Sort group keys. Get better performance by turning this off.\n    Note this does not influence the order of observations within each\n    group. Groupby preserves the order of rows within each group.\ngroup_keys : bool, default True\n    When ctotal_alling employ, add group keys to index to identify pieces.\nsqueeze : bool, default False\n    Reduce the dimensionality of the return type if possible,\n    otherwise return a consistent type.\n\n    .. deprecated:: 1.1.0\n\nobserved : bool, default False\n    This only applies if whatever of the groupers are Categoricals.\n    If True: only show observed values for categorical groupers.\n    If False: show total_all values for categorical groupers.\nsipna : bool, default True\n    If True, and if group keys contain NA values, NA values togettingher\n    with row/column will be sipped.\n    If False, NA values will also be treated as the key in groups\n\n    .. versionadded:: 1.1.0\n\nReturns\n-------\nKnowledgeFrameGroupBy\n    Returns a grouper object that contains informatingion about the groups.\n\nSee Also\n--------\nresample_by_num : Convenience method for frequency conversion and resampling\n    of time collections.\n\nNotes\n-----\nSee the `user guide\n<https://monkey.pydata.org/monkey-docs/stable/grouper.html>`__ for more.\n\nExamples\n--------\n>>> kf = mk.KnowledgeFrame({'Animal': ['Falcon', 'Falcon',\n...                               'Parrot', 'Parrot'],\n...                    'Max Speed': [380., 370., 24., 26.]})\n>>> kf\n   Animal  Max Speed\n0  Falcon      380.0\n1  Falcon      370.0\n2  Parrot       24.0\n3  Parrot       26.0\n>>> kf.grouper(['Animal']).average()\n        Max Speed\nAnimal\nFalcon      375.0\nParrot       25.0\n\n**Hierarchical Indexes**\n\nWe can grouper different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = mk.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> kf = mk.KnowledgeFrame({'Max Speed': [390., 350., 30., 20.]},\n...                   index=index)\n>>> kf\n                Max Speed\nAnimal Type\nFalcon Captive      390.0\n       Wild         350.0\nParrot Captive       30.0\n       Wild          20.0\n>>> kf.grouper(level=0).average()\n        Max Speed\nAnimal\nFalcon      370.0\nParrot       25.0\n>>> kf.grouper(level=\"Type\").average()\n         Max Speed\nType\nCaptive      210.0\nWild         185.0\n\nWe can also choose to include NA in group keys or not by setting\n`sipna` parameter, the default setting is `True`:\n\n>>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\n>>> kf = mk.KnowledgeFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> kf.grouper(by=[\"b\"]).total_sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\n\n>>> kf.grouper(by=[\"b\"], sipna=False).total_sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\nNaN 1   4\n\n>>> l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]]\n>>> kf = mk.KnowledgeFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> kf.grouper(by=\"a\").total_sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\n\n>>> kf.grouper(by=\"a\", sipna=False).total_sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\nNaN 12.3   33.0\n",
            "1200020": "get_max(self, axis=None, skipna: 'bool' = True, *args, **kwargs):\n        Return the getting_maximum value of the Index.\n\n        Parameters\n        ----------\n        axis : int, optional\n            For compatibility with NumPy. Only 0 or None are total_allowed.\n        skipna : bool, default True\n            Exclude NA/null values when showing the result.\n        *args, **kwargs\n            Additional arguments and keywords for compatibility with NumPy.\n\n        Returns\n        -------\n        scalar\n            Maximum value.\n\n        See Also\n        --------\n        Index.getting_min : Return the getting_minimum value in an Index.\n        Collections.getting_max : Return the getting_maximum value in a Collections.\n        KnowledgeFrame.getting_max : Return the getting_maximum values in a KnowledgeFrame.\n\n        Examples\n        --------\n        >>> idx = mk.Index([3, 2, 1])\n        >>> idx.getting_max()\n        3\n\n        >>> idx = mk.Index(['c', 'b', 'a'])\n        >>> idx.getting_max()\n        'c'\n\n        For a MultiIndex, the getting_maximum is detergetting_mined lexicographictotal_ally.\n\n        >>> idx = mk.MultiIndex.from_product([('a', 'b'), (2, 1)])\n        >>> idx.getting_max()\n        ('b', 2)\n        "
        }
    },
    "PandasEval/49": {
        "original_query": "transfer column date to datetime type when there is a string that is not capable of beeing turned into datetime format, skip that row, use errors='coerce' for this",
        "retrieved_APIs": {
            "API_1": "convert_datetime(arg: 'DatetimeScalarOrArrayConvertible', errors: 'str' = 'raise', dayfirst: 'bool' = False, yearfirst: 'bool' = False, utc: 'bool | None' = None, formating: 'str | None' = None, exact: 'bool' = True, unit: 'str | None' = None, infer_datetime_formating: 'bool' = False, origin='unix', cache: 'bool' = True) -> 'DatetimeIndex | Collections | DatetimeScalar | NaTType | None': Map the format of the argument to datetime.",
            "API_2": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_3": "convert_pydatetime(*args, **kwargs): Return the native datetime object in Python.",
            "API_4": "to_num(arg, errors='raise', downcast=None): Transform the the argumemt to the numeric type.",
            "API_5": "final_item(self: 'FrameOrCollections', offset) -> 'FrameOrCollections': Using a date offset to get the last periods of time collections data."
        },
        "code_prefix": "import monkey as mk\n\nkf = mk.KnowledgeFrame({\n'date': [\"2022-01-01\", \"2022-01-02\", \"2022-01-03\", \"friday\"],\n'value': [1, 2, 3, 4]\n})\n\nkf['date'] =",
        "code_completion": [
            " mk.convert_datetime(kf['date'], errors='coerce')"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'to_datetime'\n}\n\n\ndef check():\n    tmp = pd.DataFrame({'date': [\"2022-01-01\", \"2022-01-02\", \"2022-01-03\", \"friday\"],'value': [1, 2, 3, 4]})\n    tmp['date'] = pd.to_datetime(tmp['date'], errors='coerce')\n    df.equals(tmp)\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Transfer column to datetime type when there is a string that is not capable of beeing turned into datetime format?",
            "Skip that row?",
            "Use errors='coerce' for this?"
        ],
        "gold_APIs": {
            "1200033": "convert_datetime(arg: 'DatetimeScalarOrArrayConvertible', errors: 'str' = 'raise', dayfirst: 'bool' = False, yearfirst: 'bool' = False, utc: 'bool | None' = None, formating: 'str | None' = None, exact: 'bool' = True, unit: 'str | None' = None, infer_datetime_formating: 'bool' = False, origin='unix', cache: 'bool' = True) -> 'DatetimeIndex | Collections | DatetimeScalar | NaTType | None':\n    Convert argument to datetime.\n\n    Parameters\n    ----------\n    arg : int, float, str, datetime, list, tuple, 1-d array, Collections, KnowledgeFrame/dict-like\n        The object to convert to a datetime.\n    errors : {'ignore', 'raise', 'coerce'}, default 'raise'\n        - If 'raise', then invalid parsing will raise an exception.\n        - If 'coerce', then invalid parsing will be set as NaT.\n        - If 'ignore', then invalid parsing will return the input.\n    dayfirst : bool, default False\n        Specify a date parse order if `arg` is str or its list-likes.\n        If True, parses dates with the day first, eg 10/11/12 is parsed as\n        2012-11-10.\n        Warning: dayfirst=True is not strict, but will prefer to parse\n        with day first (this is a known bug, based on dateutil behavior).\n    yearfirst : bool, default False\n        Specify a date parse order if `arg` is str or its list-likes.\n\n        - If True parses dates with the year first, eg 10/11/12 is parsed as\n          2010-11-12.\n        - If both dayfirst and yearfirst are True, yearfirst is preceded (same\n          as dateutil).\n\n        Warning: yearfirst=True is not strict, but will prefer to parse\n        with year first (this is a known bug, based on dateutil behavior).\n    utc : bool, default None\n        Return UTC DatetimeIndex if True (converting whatever tz-aware\n        datetime.datetime objects as well).\n    formating : str, default None\n        The strftime to parse time, eg \"%d/%m/%Y\", note that \"%f\" will parse\n        total_all the way up to nanoseconds.\n        See strftime documentation for more informatingion on choices:\n        https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.\n    exact : bool, True by default\n        Behaves as:\n        - If True, require an exact formating match.\n        - If False, total_allow the formating to match whateverwhere in the targetting string.\n\n    unit : str, default 'ns'\n        The unit of the arg (D,s,ms,us,ns) denote the unit, which is an\n        integer or float number. This will be based off the origin.\n        Example, with unit='ms' and origin='unix' (the default), this\n        would calculate the number of milliseconds to the unix epoch start.\n    infer_datetime_formating : bool, default False\n        If True and no `formating` is given, attempt to infer the formating of the\n        datetime strings based on the first non-NaN element,\n        and if it can be inferred, switch to a faster method of parsing them.\n        In some cases this can increase the parsing speed by ~5-10x.\n    origin : scalar, default 'unix'\n        Define the reference date. The numeric values would be parsed as number\n        of units (defined by `unit`) since this reference date.\n\n        - If 'unix' (or POSIX) time; origin is set to 1970-01-01.\n        - If 'julian', unit must be 'D', and origin is set to beginning of\n          Julian Calengthdar. Julian day number 0 is total_allocateed to the day starting\n          at noon on January 1, 4713 BC.\n        - If Timestamp convertible, origin is set to Timestamp identified by\n          origin.\n    cache : bool, default True\n        If True, use a cache of distinctive, converted dates to employ the datetime\n        conversion. May produce significant speed-up when parsing duplicate\n        date strings, especitotal_ally ones with timezone offsets. The cache is only\n        used when there are at least 50 values. The presence of out-of-bounds\n        values will render the cache unusable and may slow down parsing.\n\n        .. versionchanged:: 0.25.0\n            - changed default value from False to True.\n\n    Returns\n    -------\n    datetime\n        If parsing succeeded.\n        Return type depends on input:\n\n        - list-like: DatetimeIndex\n        - Collections: Collections of datetime64 dtype\n        - scalar: Timestamp\n\n        In case when it is not possible to return designated types (e.g. when\n        whatever element of input is before Timestamp.getting_min or after Timestamp.getting_max)\n        return will have datetime.datetime type (or corresponding\n        array/Collections).\n\n    See Also\n    --------\n    KnowledgeFrame.totype : Cast argument to a specified dtype.\n    to_timedelta : Convert argument to timedelta.\n    convert_dtypes : Convert dtypes.\n\n    Examples\n    --------\n    Assembling a datetime from multiple columns of a KnowledgeFrame. The keys can be\n    common abbreviations like ['year', 'month', 'day', 'getting_minute', 'second',\n    'ms', 'us', 'ns']) or plurals of the same\n\n    >>> kf = mk.KnowledgeFrame({'year': [2015, 2016],\n    ...                    'month': [2, 3],\n    ...                    'day': [4, 5]})\n    >>> mk.convert_datetime(kf)\n    0   2015-02-04\n    1   2016-03-05\n    dtype: datetime64[ns]\n\n    If a date does not meet the `timestamp limitations\n    <https://monkey.pydata.org/monkey-docs/stable/user_guide/timecollections.html\n    #timecollections-timestamp-limits>`_, passing errors='ignore'\n    will return the original input instead of raincontaing whatever exception.\n\n    Passing errors='coerce' will force an out-of-bounds date to NaT,\n    in addition to forcing non-dates (or non-parseable dates) to NaT.\n\n    >>> mk.convert_datetime('13000101', formating='%Y%m%d', errors='ignore')\n    datetime.datetime(1300, 1, 1, 0, 0)\n    >>> mk.convert_datetime('13000101', formating='%Y%m%d', errors='coerce')\n    NaT\n\n    Passing infer_datetime_formating=True can often-times speedup a parsing\n    if its not an ISO8601 formating exactly, but in a regular formating.\n\n    >>> s = mk.Collections(['3/11/2000', '3/12/2000', '3/13/2000'] * 1000)\n    >>> s.header_num()\n    0    3/11/2000\n    1    3/12/2000\n    2    3/13/2000\n    3    3/11/2000\n    4    3/12/2000\n    dtype: object\n\n    >>> %timeit mk.convert_datetime(s, infer_datetime_formating=True)  # doctest: +SKIP\n    100 loops, best of 3: 10.4 ms per loop\n\n    >>> %timeit mk.convert_datetime(s, infer_datetime_formating=False)  # doctest: +SKIP\n    1 loop, best of 3: 471 ms per loop\n\n    Using a unix epoch time\n\n    >>> mk.convert_datetime(1490195805, unit='s')\n    Timestamp('2017-03-22 15:16:45')\n    >>> mk.convert_datetime(1490195805433502912, unit='ns')\n    Timestamp('2017-03-22 15:16:45.433502912')\n\n    .. warning:: For float arg, precision value_rounding might happen. To prevent\n        unexpected behavior use a fixed-width exact type.\n\n    Using a non-unix epoch origin\n\n    >>> mk.convert_datetime([1, 2, 3], unit='D',\n    ...                origin=mk.Timestamp('1960-01-01'))\n    DatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'],\n                  dtype='datetime64[ns]', freq=None)\n\n    In case input is list-like and the elements of input are of mixed\n    timezones, return will have object type Index if utc=False.\n\n    >>> mk.convert_datetime(['2018-10-26 12:00 -0530', '2018-10-26 12:00 -0500'])\n    Index([2018-10-26 12:00:00-05:30, 2018-10-26 12:00:00-05:00], dtype='object')\n\n    >>> mk.convert_datetime(['2018-10-26 12:00 -0530', '2018-10-26 12:00 -0500'],\n    ...                utc=True)\n    DatetimeIndex(['2018-10-26 17:30:00+00:00', '2018-10-26 17:00:00+00:00'],\n                  dtype='datetime64[ns, UTC]', freq=None)\n    "
        }
    },
    "PandasEval/50": {
        "original_query": "How to check if any value is NaN in a Monkey KnowledgeFrame? Return the result.",
        "retrieved_APIs": {
            "API_1": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.",
            "API_4": "sipna(self): Return an ExtensionArray that is devoid of NA values.",
            "API_5": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\ndef if_any_value_is_nan(kf):",
        "code_completion": ["return kf.ifnull().values.whatever()"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'isnull_value_any'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [np.nan, 2, 3], 'B': [1, 2, 3], 'C': [1, 2, 3]})) == True\n    assert candidate(pd.DataFrame({'A': [np.nan, np.nan, 3], 'B': [1, 2, 3], 'C': [1, 2, 3]})) == True\n    assert candidate(pd.DataFrame({'A': [np.nan, np.nan, 3], 'B': [1, np.nan, 3], 'C': [1, 2, 3]})) == True\n    assert candidate(pd.DataFrame({'A': [np.nan, np.nan, 3], 'B': [1, np.nan, 8], 'C': [1, 2, 3]})) == True\n    assert candidate(pd.DataFrame({'A': [7, 6, 3], 'B': [1, np.nan, 8], 'C': [1, 2, 3]})) == True\n    assert candidate(pd.DataFrame({'A': [7, 8, 3], 'B': [1, np.nan, 8], 'C': [1, 2, 18]})) == True\n    assert candidate(pd.DataFrame({'A': [7, 6, 3], 'B': [1, np.nan, 8], 'C': [1, 2, 18]})) == True\n    assert candidate(pd.DataFrame({'A': [7, 8, 3], 'B': [1, 8, 8], 'C': [1, 2, 18]})) == False\n    assert candidate(pd.DataFrame({'A': [7, 8, 3], 'B': [81, 5, 8], 'C': [1, 2, 18]})) == False\n    assert candidate(pd.DataFrame({'A': [7, 94, 3], 'B': [81, 5, 8], 'C': [1, 2, 18]})) == False\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "How to check if any value is NaN in a KnowledgeFrame?",
            "Return the result."
        ],
        "gold_APIs": {
            "1200015": "ifnull(self) -> 'np.ndarray':\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`mk.NaT`, getting\n        mappingped to ``True`` values.\n        Everything else getting mappingped to ``False`` values. Characters such as\n        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``monkey.options.mode.use_inf_as_na = True``).\n\n        Returns\n        -------\n        numpy.ndarray[bool]\n            A boolean array of whether my values are NA.\n\n        See Also\n        --------\n        Index.notna : Boolean inverse of ifna.\n        Index.sipna : Omit entries with missing values.\n        ifna : Top-level ifna.\n        Collections.ifna : Detect missing values in Collections object.\n\n        Examples\n        --------\n        Show which entries in a monkey.Index are NA. The result is an\n        array.\n\n        >>> idx = mk.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.ifna()\n        array([False, False,  True])\n\n        Empty strings are not considered NA values. None is considered an NA\n        value.\n\n        >>> idx = mk.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.ifna()\n        array([False, False, False,  True])\n\n        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n\n        >>> idx = mk.DatetimeIndex([mk.Timestamp('1940-04-25'),\n        ...                         mk.Timestamp(''), None, mk.NaT])\n        >>> idx\n        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n                      dtype='datetime64[ns]', freq=None)\n        >>> idx.ifna()\n        array([False,  True,  True,  True])\n        ",
            "1200034": "whatever(self, *args, **kwargs):\n        Return whether whatever element is Truthy.\n\n        Parameters\n        ----------\n        *args\n            Required for compatibility with numpy.\n        **kwargs\n            Required for compatibility with numpy.\n\n        Returns\n        -------\n        whatever : bool or array-like (if axis is specified)\n            A single element array-like may be converted to bool.\n\n        See Also\n        --------\n        Index.total_all : Return whether total_all elements are True.\n        Collections.total_all : Return whether total_all elements are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        >>> index = mk.Index([0, 1, 2])\n        >>> index.whatever()\n        True\n\n        >>> index = mk.Index([0, 0, 0])\n        >>> index.whatever()\n        False\n        "
        }
    },
    "PandasEval/51": {
        "original_query": "Sorting columns in monkey knowledgeframe based on column name Note that axis is one",
        "retrieved_APIs": {
            "API_1": "sorting_index(self, axis: 'Axis' = 0, level: 'Level | None' = None, ascending: 'bool | int | Sequence[bool | int]' = True, inplace: 'bool' = False, kind: 'str' = 'quicksort', na_position: 'str' = 'final_item', sort_remaining: 'bool' = True, ignore_index: 'bool' = False, key: 'IndexKeyFunc' = None): Return object sorted by labels along the specified axis.",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "division(self, other, axis='columns', level=None, fill_value=None): Get the element-wise floating division of knowledgeframe or other objects.",
            "API_4": "concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion': Concatenate monkey objects along one axis, using set logic on the other axes if needed.",
            "API_5": "reindexing(self, target, method=None, level=None, limit=None, tolerance=None) -> 'tuple[MultiIndex, np.ndarray | None]': Create an index conditioned on the values of target (move, add, or remove values as needed)."
        },
        "code_prefix": "import monkey as mk\n\ndef sorting_columns_based_on_column_name(kf):",
        "code_completion": [
            "    return kf.reindexing(sorted(kf.columns), axis=1)"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'reindex_sorted'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'Q1.1': [1, 2, 3], 'Q1.3': [4, 5, 6], 'Q1.2': [7, 8, 9]})).equals(pd.DataFrame({'Q1.1': [1, 2, 3], 'Q1.2': [7, 8, 9], 'Q1.3': [4, 5, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [1, 2, 4], 'Q1.3': [4, 5, 6], 'Q1.2': [7, 8, 9]})).equals(pd.DataFrame({'Q1.1': [1, 2, 4], 'Q1.2': [7, 8, 9], 'Q1.3': [4, 5, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [1, 2, 5], 'Q1.3': [4, 5, 6], 'Q1.2': [7, 8, 9]})).equals(pd.DataFrame({'Q1.1': [1, 2, 5], 'Q1.2': [7, 8, 9], 'Q1.3': [4, 5, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [1, 3, 5], 'Q1.3': [4, 5, 6], 'Q1.2': [7, 8, 9]})).equals(pd.DataFrame({'Q1.1': [1, 3, 5], 'Q1.2': [7, 8, 9], 'Q1.3': [4, 5, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [2, 3, 5], 'Q1.3': [4, 5, 6], 'Q1.2': [7, 8, 9]})).equals(pd.DataFrame({'Q1.1': [2, 3, 5], 'Q1.2': [7, 8, 9], 'Q1.3': [4, 5, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [2, 3, 5], 'Q1.3': [3, 5, 6], 'Q1.2': [7, 8, 9]})).equals(pd.DataFrame({'Q1.1': [2, 3, 5], 'Q1.2': [7, 8, 9], 'Q1.3': [3, 5, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [2, 3, 5], 'Q1.3': [3, 3, 6], 'Q1.2': [7, 8, 9]})).equals(pd.DataFrame({'Q1.1': [2, 3, 5], 'Q1.2': [7, 8, 9], 'Q1.3': [3, 3, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [4, 4, 5], 'Q1.3': [3, 3, 6], 'Q1.2': [7, 8, 9]})).equals(pd.DataFrame({'Q1.1': [4, 4, 5], 'Q1.2': [7, 8, 9], 'Q1.3': [3, 3, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [4, 4, 5], 'Q1.3': [3, 3, 6], 'Q1.2': [7, 4, 9]})).equals(pd.DataFrame({'Q1.1': [4, 4, 5], 'Q1.2': [7, 4, 9], 'Q1.3': [3, 3, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [4, 4, 5], 'Q1.3': [3, 3, 6], 'Q1.2': [7, 3, 9]})).equals(pd.DataFrame({'Q1.1': [4, 4, 5], 'Q1.2': [7, 3, 9], 'Q1.3': [3, 3, 6]}))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Sorting columns in monkey knowledgeframe based on",
            "Sorting columns in monkey knowledgeframe based on column name Note that",
            "Sorting columns in monkey knowledgeframe based on column name Note that axis is",
            "Sorting columns in monkey knowledgeframe based on column name Note that axis is one"
        ],
        "gold_APIs": {
            "1200035": "reindexing(self, target, method=None, level=None, limit=None, tolerance=None) -> 'tuple[MultiIndex, np.ndarray | None]':\n        Create index with targetting's values (move/add/delete values as necessary)\n\n        Returns\n        -------\n        new_index : mk.MultiIndex\n            Resulting index\n        indexer : np.ndarray[np.intp] or None\n            Indices of output values in original index.\n\n        "
        }
    },
    "PandasEval/52": {
        "original_query": "How can I get the values of column `A` when column `B`=3?",
        "retrieved_APIs": {
            "API_1": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_2": "ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.",
            "API_3": "incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values.",
            "API_4": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_5": "to_num(arg, errors='raise', downcast=None): Transform the the argumemt to the numeric type."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\ndef get_value_when_condition(kf):",
        "code_completion": ["    return kf[kf['B'] == 3]['A'].values"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'condition'\n}\n\n\ndef check(candidate):\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p1', 'p1', 'p2', 'p2', 'p3'], 'B': [1, 2, 3, 4, 5]})), np.array(['p2']))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p1', 'p1', 'p2', 'p2', 'p3'], 'B': [1, 4, 3, 4, 5]})), np.array(['p2']))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p1', 'p1', 'p2', 'p2', 'p3'], 'B': [1, 4, 8, 4, 5]})), np.array([]))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p1', 'p1', 'p2', 'p2', 'p5'], 'B': [2, 4, 8, 4, 5]})), np.array([]))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p1', 'p1', 'p2', 'p2', 'p3'], 'B': [2, 3, 4, 4, 5]})), np.array(['p1']))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p2', 'p1', 'p2', 'p2', 'p3'], 'B': [2, 3, 4, 4, 5]})), np.array(['p1']))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p2', 'p1', 'p2', 'p3', 'p3'], 'B': [2, 3, 4, 5, 5]})), np.array(['p1']))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p2', 'p1', 'p1', 'p3', 'p3'], 'B': [2, 3, 4, 5, 5]})), np.array(['p1']))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p2', 'p1', 'p1', 'p1', 'p3'], 'B': [2, 3, 4, 5, 5]})), np.array(['p1']))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p2', 'p1', 'p1', 'p1', 'p2'], 'B': [2, 3, 4, 5, 5]})), np.array(['p1']))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "How can I get the values of when =3?",
            "How can I get the values of column `A` when column `B`?",
            "How can I get the values of column `A` when =3?",
            "How can I get the values of column `A` when?",
            "How can I get the values of column `A` when column `B`=3?"
        ],
        "gold_APIs": {}
    },
    "PandasEval/53": {
        "original_query": "return the column average/mean",
        "retrieved_APIs": {
            "API_1": "average(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs): Return the average value along the specified axis.",
            "API_2": "standard(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs): Return the standard deviation across the requested axis.",
            "API_3": "cumulative_sum(self, axis=None, skipna=True, *args, **kwargs): Return the cumulative total of an axis in the KnowledgeFrame or Collections.",
            "API_4": "total_sum(self, axis=None, skipna=None, level=None, numeric_only=None, getting_min_count=0, **kwargs): Return the summed value of the specified axis.",
            "API_5": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values."
        },
        "code_prefix": "import monkey as mk\n\ndef get_average_in_column(kf, col_name):",
        "code_completion": ["    return kf[col_name].average()"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'condition'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({\"A\": [1, 2, 3, 4, 5]}), \"A\") == 3.0\n    assert candidate(pd.DataFrame({\"A\": [1, 2, 3, 4, 5, 3]}), \"A\") == 3.0\n    assert candidate(pd.DataFrame({'B': [1, 2, 3, 4, 5, 3]}), 'B') == 3.0\n    assert candidate(pd.DataFrame({'A': [1, 2, 3, 3, 4, 5, 3]}), 'A') == 3.0\n    assert candidate(pd.DataFrame({'A': [1, 3, 2, 3, 4, 5, 3]}), 'A') == 3.0\n    assert candidate(pd.DataFrame({'T': [1, 2, 3, 4, 5, 3]}), 'T') == 3.0\n    assert candidate(pd.DataFrame({'A': [1, 2, 3, 4, 5, 3, 3, 3]}), 'A') == 3.0\n    assert candidate(pd.DataFrame({'A': [1, 2, 1, 3, 5, 4, 5, 3]}), 'A') == 3.0\n    assert candidate(pd.DataFrame({'A': [1, 2, 3, 4, 5, 3, 2, 4]}), 'A') == 3.0\n    assert candidate(pd.DataFrame({'A': [1, 2, 3, 4, 5, 3, 1, 5]}), 'A') == 3.0\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Return average/mean",
            "What is the column average/mean?",
            "What is the average/mean?",
            "What is the average/mean of the column?"
        ],
        "gold_APIs": {
            "1200000": "average(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs):\nReturn the average of the values over the requested axis.\n\nParameters\n----------\naxis : {index (0)}\n    Axis for the function to be applied on.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a scalar.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Collections.\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nscalar or Collections (if level specified)\n"
        }
    },
    "PandasEval/54": {
        "original_query": "How do I combine two knowledgeframes with ignore index? Return the concated knowledgeframe.",
        "retrieved_APIs": {
            "API_1": "adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options.",
            "API_2": "concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion':\n    Concatenate monkey objects along a particular axis with optional set logic\n    along the other axes.\n\n    Can also add a layer of hierarchical indexing on the concatingenation axis,\n    which may be useful if the labels are the same (or overlapping) on\n    the passed axis number.\n\n    Parameters\n    ----------\n    objs : a sequence or mappingping of Collections or KnowledgeFrame objects\n        If a mappingping is passed, the sorted keys will be used as the `keys`\n        argument, unless it is passed, in which case the values will be\n        selected (see below). Any None objects will be sipped silengthtly unless\n        they are total_all None in which case a ValueError will be raised.\n    axis : {0/'index', 1/'columns'}, default 0\n        The axis to concatingenate along.\n    join : {'inner', 'outer'}, default 'outer'\n        How to handle indexes on other axis (or axes).\n    ignore_index : bool, default False\n        If True, do not use the index values along the concatingenation axis. The\n        resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n        concatingenating objects where the concatingenation axis does not have\n        averageingful indexing informatingion. Note the index values on the other\n        axes are still respected in the join.\n    keys : sequence, default None\n        If multiple levels passed, should contain tuples. Construct\n        hierarchical index using the passed keys as the outermost level.\n    levels : list of sequences, default None\n        Specific levels (distinctive values) to use for constructing a\n        MultiIndex. Otherwise they will be inferred from the keys.\n    names : list, default None\n        Names for the levels in the resulting hierarchical index.\n    verify_integrity : bool, default False\n        Check whether the new concatingenated axis contains duplicates. This can\n        be very expensive relative to the actual data concatingenation.\n    sort : bool, default False\n        Sort non-concatingenation axis if it is not already aligned when `join`\n        is 'outer'.\n        This has no effect when ``join='inner'``, which already preserves\n        the order of the non-concatingenation axis.\n\n        .. versionchanged:: 1.0.0\n\n           Changed to not sort by default.\n\n    clone : bool, default True\n        If False, do not clone data unnecessarily.\n\n    Returns\n    -------\n    object, type of objs\n        When concatingenating total_all ``Collections`` along the index (axis=0), a\n        ``Collections`` is returned. When ``objs`` contains at least one\n        ``KnowledgeFrame``, a ``KnowledgeFrame`` is returned. When concatingenating along\n        the columns (axis=1), a ``KnowledgeFrame`` is returned.\n\n    See Also\n    --------\n    Collections.adding : Concatenate Collections.\n    KnowledgeFrame.adding : Concatenate KnowledgeFrames.\n    KnowledgeFrame.join : Join KnowledgeFrames using indexes.\n    KnowledgeFrame.unioner : Merge KnowledgeFrames by indexes or columns.\n\n    Notes\n    -----\n    The keys, levels, and names arguments are total_all optional.\n\n    A walkthrough of how this method fits in with other tools for combining\n    monkey objects can be found `here\n    <https://monkey.pydata.org/monkey-docs/stable/user_guide/merging.html>`__.\n\n    Examples\n    --------\n    Combine two ``Collections``.\n\n    >>> s1 = mk.Collections(['a', 'b'])\n    >>> s2 = mk.Collections(['c', 'd'])\n    >>> mk.concating([s1, s2])\n    0    a\n    1    b\n    0    c\n    1    d\n    dtype: object\n\n    Clear the existing index and reset it in the result\n    by setting the ``ignore_index`` option to ``True``.\n\n    >>> mk.concating([s1, s2], ignore_index=True)\n    0    a\n    1    b\n    2    c\n    3    d\n    dtype: object\n\n    Add a hierarchical index at the outermost level of\n    the data with the ``keys`` option.\n\n    >>> mk.concating([s1, s2], keys=['s1', 's2'])\n    s1  0    a\n        1    b\n    s2  0    c\n        1    d\n    dtype: object\n\n    Label the index keys you create with the ``names`` option.\n\n    >>> mk.concating([s1, s2], keys=['s1', 's2'],\n    ...           names=['Collections name', 'Row ID'])\n    Collections name  Row ID\n    s1           0         a\n                 1         b\n    s2           0         c\n                 1         d\n    dtype: object\n\n    Combine two ``KnowledgeFrame`` objects with identical columns.\n\n    >>> kf1 = mk.KnowledgeFrame([['a', 1], ['b', 2]],\n    ...                    columns=['letter', 'number'])\n    >>> kf1\n      letter  number\n    0      a       1\n    1      b       2\n    >>> kf2 = mk.KnowledgeFrame([['c', 3], ['d', 4]],\n    ...                    columns=['letter', 'number'])\n    >>> kf2\n      letter  number\n    0      c       3\n    1      d       4\n    >>> mk.concating([kf1, kf2])\n      letter  number\n    0      a       1\n    1      b       2\n    0      c       3\n    1      d       4\n\n    Combine ``KnowledgeFrame`` objects with overlapping columns\n    and return everything. Columns outside the interst will\n    be filled with ``NaN`` values.\n\n    >>> kf3 = mk.KnowledgeFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n    ...                    columns=['letter', 'number', 'animal'])\n    >>> kf3\n      letter  number animal\n    0      c       3    cat\n    1      d       4    dog\n    >>> mk.concating([kf1, kf3], sort=False)\n      letter  number animal\n    0      a       1    NaN\n    1      b       2    NaN\n    0      c       3    cat\n    1      d       4    dog\n\n    Combine ``KnowledgeFrame`` objects with overlapping columns\n    and return only those that are shared by passing ``inner`` to\n    the ``join`` keyword argument.\n\n    >>> mk.concating([kf1, kf3], join=\"inner\")\n      letter  number\n    0      a       1\n    1      b       2\n    0      c       3\n    1      d       4\n\n    Combine ``KnowledgeFrame`` objects horizonttotal_ally along the x axis by\n    passing in ``axis=1``.\n\n    >>> kf4 = mk.KnowledgeFrame([['bird', 'polly'], ['monkey', 'george']],\n    ...                    columns=['animal', 'name'])\n    >>> mk.concating([kf1, kf4], axis=1)\n      letter  number  animal    name\n    0      a       1    bird   polly\n    1      b       2  monkey  george\n\n    Prevent the result from including duplicate index values with the\n    ``verify_integrity`` option.\n\n    >>> kf5 = mk.KnowledgeFrame([1], index=['a'])\n    >>> kf5\n       0\n    a  1\n    >>> kf6 = mk.KnowledgeFrame([2], index=['a'])\n    >>> kf6\n       0\n    a  2\n    >>> mk.concating([kf5, kf6], verify_integrity=True)\n    Traceback (most recent ctotal_all final_item):\n        ...\n    ValueError: Indexes have overlapping values: ['a']\n    ",
            "API_3": "reindexing(self, target, method=None, level=None, limit=None, tolerance=None) -> 'tuple[MultiIndex, np.ndarray | None]': Create an index conditioned on the values of target (move, add, or remove values as needed).",
            "API_4": "interst(self, other, sort=False): Create the intersection of two Index objects.",
            "API_5": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous."
        },
        "code_prefix": "import monkey as mk\n\ndef combine_kf(kf1, kf2):",
        "code_completion": [
            "    return kf1.adding(kf2, ignore_index=True)",
            "    return mk.concating([kf1, kf2], ignore_index=True)"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'append_concat'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 2]}), pd.DataFrame({'A': [4, 5]})).equals(pd.DataFrame({'A': [1, 2, 4, 5]}))\n    assert candidate(pd.DataFrame({'A': [1, 2]}), pd.DataFrame({'A': [4, 6]})).equals(pd.DataFrame({'A': [1, 2, 4, 6]}))\n    assert candidate(pd.DataFrame({'A': [1, 4]}), pd.DataFrame({'A': [4, 5]})).equals(pd.DataFrame({'A': [1, 4, 4, 5]}))\n    assert candidate(pd.DataFrame({'A': [3, 4]}), pd.DataFrame({'A': [4, 5]})).equals(pd.DataFrame({'A': [3, 4, 4, 5]}))\n    assert candidate(pd.DataFrame({'A': [5, 2]}), pd.DataFrame({'A': [4, 5]})).equals(pd.DataFrame({'A': [5, 2, 4, 5]}))\n    assert candidate(pd.DataFrame({'A': [4, 2]}), pd.DataFrame({'A': [4, 5]})).equals(pd.DataFrame({'A': [4, 2, 4, 5]}))\n    assert candidate(pd.DataFrame({'A': [6, 2]}), pd.DataFrame({'A': [9, 5]})).equals(pd.DataFrame({'A': [6, 2, 9, 5]}))\n    assert candidate(pd.DataFrame({'A': [1, 7]}), pd.DataFrame({'A': [4, 5]})).equals(pd.DataFrame({'A': [1, 7, 4, 5]}))\n    assert candidate(pd.DataFrame({'A': [6, 2]}), pd.DataFrame({'A': [4, 56]})).equals(pd.DataFrame({'A': [6, 2, 4, 56]}))\n    assert candidate(pd.DataFrame({'A': [11, 22]}), pd.DataFrame({'A': [4, 5]})).equals(pd.DataFrame({'A': [11, 22, 4, 5]}))\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "How do I combine two knowledgeframes?",
            "Return the concated knowledgeframe."
        ],
        "gold_APIs": {
            "1200009": "adding(self, other: 'Index | Sequence[Index]') -> 'Index':\n        Append a collection of Index options togettingher.\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n\n        Returns\n        -------\n        Index\n        ",
            "1200036": "concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion':\n    Concatenate monkey objects along a particular axis with optional set logic\n    along the other axes.\n\n    Can also add a layer of hierarchical indexing on the concatingenation axis,\n    which may be useful if the labels are the same (or overlapping) on\n    the passed axis number.\n\n    Parameters\n    ----------\n    objs : a sequence or mappingping of Collections or KnowledgeFrame objects\n        If a mappingping is passed, the sorted keys will be used as the `keys`\n        argument, unless it is passed, in which case the values will be\n        selected (see below). Any None objects will be sipped silengthtly unless\n        they are total_all None in which case a ValueError will be raised.\n    axis : {0/'index', 1/'columns'}, default 0\n        The axis to concatingenate along.\n    join : {'inner', 'outer'}, default 'outer'\n        How to handle indexes on other axis (or axes).\n    ignore_index : bool, default False\n        If True, do not use the index values along the concatingenation axis. The\n        resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n        concatingenating objects where the concatingenation axis does not have\n        averageingful indexing informatingion. Note the index values on the other\n        axes are still respected in the join.\n    keys : sequence, default None\n        If multiple levels passed, should contain tuples. Construct\n        hierarchical index using the passed keys as the outermost level.\n    levels : list of sequences, default None\n        Specific levels (distinctive values) to use for constructing a\n        MultiIndex. Otherwise they will be inferred from the keys.\n    names : list, default None\n        Names for the levels in the resulting hierarchical index.\n    verify_integrity : bool, default False\n        Check whether the new concatingenated axis contains duplicates. This can\n        be very expensive relative to the actual data concatingenation.\n    sort : bool, default False\n        Sort non-concatingenation axis if it is not already aligned when `join`\n        is 'outer'.\n        This has no effect when ``join='inner'``, which already preserves\n        the order of the non-concatingenation axis.\n\n        .. versionchanged:: 1.0.0\n\n           Changed to not sort by default.\n\n    clone : bool, default True\n        If False, do not clone data unnecessarily.\n\n    Returns\n    -------\n    object, type of objs\n        When concatingenating total_all ``Collections`` along the index (axis=0), a\n        ``Collections`` is returned. When ``objs`` contains at least one\n        ``KnowledgeFrame``, a ``KnowledgeFrame`` is returned. When concatingenating along\n        the columns (axis=1), a ``KnowledgeFrame`` is returned.\n\n    See Also\n    --------\n    Collections.adding : Concatenate Collections.\n    KnowledgeFrame.adding : Concatenate KnowledgeFrames.\n    KnowledgeFrame.join : Join KnowledgeFrames using indexes.\n    KnowledgeFrame.unioner : Merge KnowledgeFrames by indexes or columns.\n\n    Notes\n    -----\n    The keys, levels, and names arguments are total_all optional.\n\n    A walkthrough of how this method fits in with other tools for combining\n    monkey objects can be found `here\n    <https://monkey.pydata.org/monkey-docs/stable/user_guide/merging.html>`__.\n\n    Examples\n    --------\n    Combine two ``Collections``.\n\n    >>> s1 = mk.Collections(['a', 'b'])\n    >>> s2 = mk.Collections(['c', 'd'])\n    >>> mk.concating([s1, s2])\n    0    a\n    1    b\n    0    c\n    1    d\n    dtype: object\n\n    Clear the existing index and reset it in the result\n    by setting the ``ignore_index`` option to ``True``.\n\n    >>> mk.concating([s1, s2], ignore_index=True)\n    0    a\n    1    b\n    2    c\n    3    d\n    dtype: object\n\n    Add a hierarchical index at the outermost level of\n    the data with the ``keys`` option.\n\n    >>> mk.concating([s1, s2], keys=['s1', 's2'])\n    s1  0    a\n        1    b\n    s2  0    c\n        1    d\n    dtype: object\n\n    Label the index keys you create with the ``names`` option.\n\n    >>> mk.concating([s1, s2], keys=['s1', 's2'],\n    ...           names=['Collections name', 'Row ID'])\n    Collections name  Row ID\n    s1           0         a\n                 1         b\n    s2           0         c\n                 1         d\n    dtype: object\n\n    Combine two ``KnowledgeFrame`` objects with identical columns.\n\n    >>> kf1 = mk.KnowledgeFrame([['a', 1], ['b', 2]],\n    ...                    columns=['letter', 'number'])\n    >>> kf1\n      letter  number\n    0      a       1\n    1      b       2\n    >>> kf2 = mk.KnowledgeFrame([['c', 3], ['d', 4]],\n    ...                    columns=['letter', 'number'])\n    >>> kf2\n      letter  number\n    0      c       3\n    1      d       4\n    >>> mk.concating([kf1, kf2])\n      letter  number\n    0      a       1\n    1      b       2\n    0      c       3\n    1      d       4\n\n    Combine ``KnowledgeFrame`` objects with overlapping columns\n    and return everything. Columns outside the interst will\n    be filled with ``NaN`` values.\n\n    >>> kf3 = mk.KnowledgeFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n    ...                    columns=['letter', 'number', 'animal'])\n    >>> kf3\n      letter  number animal\n    0      c       3    cat\n    1      d       4    dog\n    >>> mk.concating([kf1, kf3], sort=False)\n      letter  number animal\n    0      a       1    NaN\n    1      b       2    NaN\n    0      c       3    cat\n    1      d       4    dog\n\n    Combine ``KnowledgeFrame`` objects with overlapping columns\n    and return only those that are shared by passing ``inner`` to\n    the ``join`` keyword argument.\n\n    >>> mk.concating([kf1, kf3], join=\"inner\")\n      letter  number\n    0      a       1\n    1      b       2\n    0      c       3\n    1      d       4\n\n    Combine ``KnowledgeFrame`` objects horizonttotal_ally along the x axis by\n    passing in ``axis=1``.\n\n    >>> kf4 = mk.KnowledgeFrame([['bird', 'polly'], ['monkey', 'george']],\n    ...                    columns=['animal', 'name'])\n    >>> mk.concating([kf1, kf4], axis=1)\n      letter  number  animal    name\n    0      a       1    bird   polly\n    1      b       2  monkey  george\n\n    Prevent the result from including duplicate index values with the\n    ``verify_integrity`` option.\n\n    >>> kf5 = mk.KnowledgeFrame([1], index=['a'])\n    >>> kf5\n       0\n    a  1\n    >>> kf6 = mk.KnowledgeFrame([2], index=['a'])\n    >>> kf6\n       0\n    a  2\n    >>> mk.concating([kf5, kf6], verify_integrity=True)\n    Traceback (most recent ctotal_all final_item):\n        ...\n    ValueError: Indexes have overlapping values: ['a']\n    "
        }
    },
    "PandasEval/55": {
        "original_query": "This is my KnowledgeFrame that should be repeated for 5 times: I haven't found anything practical, including those like np.repeat ---- it just doesn't work on a KnowledgeFrame. You can use the concating function:",
        "retrieved_APIs": {
            "API_1": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_2": "concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion': Concatenate monkey objects along one axis, using set logic on the other axes if needed.",
            "API_3": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_4": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_5": "reindexing(self, target, method=None, level=None, limit=None, tolerance=None) -> 'tuple[MultiIndex, np.ndarray | None]': Create an index conditioned on the values of target (move, add, or remove values as needed)."
        },
        "code_prefix": "import monkey as mk\n\nx = mk.KnowledgeFrame({'a':1,'b':2}, index = range(1))\nrepeated_x =",
        "code_completion": [" mk.concating([x]*5)"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'concat'\n}\n\n\ndef check():\n    assert repeated_x.equals(pd.concat([x]*5))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "This is my KnowledgeFrame that should be repeated for 5 times:",
            "I haven't found anything practical, including those like np.repeat ---- it just doesn't work on a KnowledgeFrame.",
            "You can use the concating function:",
            "What is the function to repeat a KnowledgeFrame?",
            "How can I repeat a KnowledgeFrame?"
        ],
        "gold_APIs": {
            "1200036": "concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion':\n    Concatenate monkey objects along a particular axis with optional set logic\n    along the other axes.\n\n    Can also add a layer of hierarchical indexing on the concatingenation axis,\n    which may be useful if the labels are the same (or overlapping) on\n    the passed axis number.\n\n    Parameters\n    ----------\n    objs : a sequence or mappingping of Collections or KnowledgeFrame objects\n        If a mappingping is passed, the sorted keys will be used as the `keys`\n        argument, unless it is passed, in which case the values will be\n        selected (see below). Any None objects will be sipped silengthtly unless\n        they are total_all None in which case a ValueError will be raised.\n    axis : {0/'index', 1/'columns'}, default 0\n        The axis to concatingenate along.\n    join : {'inner', 'outer'}, default 'outer'\n        How to handle indexes on other axis (or axes).\n    ignore_index : bool, default False\n        If True, do not use the index values along the concatingenation axis. The\n        resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n        concatingenating objects where the concatingenation axis does not have\n        averageingful indexing informatingion. Note the index values on the other\n        axes are still respected in the join.\n    keys : sequence, default None\n        If multiple levels passed, should contain tuples. Construct\n        hierarchical index using the passed keys as the outermost level.\n    levels : list of sequences, default None\n        Specific levels (distinctive values) to use for constructing a\n        MultiIndex. Otherwise they will be inferred from the keys.\n    names : list, default None\n        Names for the levels in the resulting hierarchical index.\n    verify_integrity : bool, default False\n        Check whether the new concatingenated axis contains duplicates. This can\n        be very expensive relative to the actual data concatingenation.\n    sort : bool, default False\n        Sort non-concatingenation axis if it is not already aligned when `join`\n        is 'outer'.\n        This has no effect when ``join='inner'``, which already preserves\n        the order of the non-concatingenation axis.\n\n        .. versionchanged:: 1.0.0\n\n           Changed to not sort by default.\n\n    clone : bool, default True\n        If False, do not clone data unnecessarily.\n\n    Returns\n    -------\n    object, type of objs\n        When concatingenating total_all ``Collections`` along the index (axis=0), a\n        ``Collections`` is returned. When ``objs`` contains at least one\n        ``KnowledgeFrame``, a ``KnowledgeFrame`` is returned. When concatingenating along\n        the columns (axis=1), a ``KnowledgeFrame`` is returned.\n\n    See Also\n    --------\n    Collections.adding : Concatenate Collections.\n    KnowledgeFrame.adding : Concatenate KnowledgeFrames.\n    KnowledgeFrame.join : Join KnowledgeFrames using indexes.\n    KnowledgeFrame.unioner : Merge KnowledgeFrames by indexes or columns.\n\n    Notes\n    -----\n    The keys, levels, and names arguments are total_all optional.\n\n    A walkthrough of how this method fits in with other tools for combining\n    monkey objects can be found `here\n    <https://monkey.pydata.org/monkey-docs/stable/user_guide/merging.html>`__.\n\n    Examples\n    --------\n    Combine two ``Collections``.\n\n    >>> s1 = mk.Collections(['a', 'b'])\n    >>> s2 = mk.Collections(['c', 'd'])\n    >>> mk.concating([s1, s2])\n    0    a\n    1    b\n    0    c\n    1    d\n    dtype: object\n\n    Clear the existing index and reset it in the result\n    by setting the ``ignore_index`` option to ``True``.\n\n    >>> mk.concating([s1, s2], ignore_index=True)\n    0    a\n    1    b\n    2    c\n    3    d\n    dtype: object\n\n    Add a hierarchical index at the outermost level of\n    the data with the ``keys`` option.\n\n    >>> mk.concating([s1, s2], keys=['s1', 's2'])\n    s1  0    a\n        1    b\n    s2  0    c\n        1    d\n    dtype: object\n\n    Label the index keys you create with the ``names`` option.\n\n    >>> mk.concating([s1, s2], keys=['s1', 's2'],\n    ...           names=['Collections name', 'Row ID'])\n    Collections name  Row ID\n    s1           0         a\n                 1         b\n    s2           0         c\n                 1         d\n    dtype: object\n\n    Combine two ``KnowledgeFrame`` objects with identical columns.\n\n    >>> kf1 = mk.KnowledgeFrame([['a', 1], ['b', 2]],\n    ...                    columns=['letter', 'number'])\n    >>> kf1\n      letter  number\n    0      a       1\n    1      b       2\n    >>> kf2 = mk.KnowledgeFrame([['c', 3], ['d', 4]],\n    ...                    columns=['letter', 'number'])\n    >>> kf2\n      letter  number\n    0      c       3\n    1      d       4\n    >>> mk.concating([kf1, kf2])\n      letter  number\n    0      a       1\n    1      b       2\n    0      c       3\n    1      d       4\n\n    Combine ``KnowledgeFrame`` objects with overlapping columns\n    and return everything. Columns outside the interst will\n    be filled with ``NaN`` values.\n\n    >>> kf3 = mk.KnowledgeFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n    ...                    columns=['letter', 'number', 'animal'])\n    >>> kf3\n      letter  number animal\n    0      c       3    cat\n    1      d       4    dog\n    >>> mk.concating([kf1, kf3], sort=False)\n      letter  number animal\n    0      a       1    NaN\n    1      b       2    NaN\n    0      c       3    cat\n    1      d       4    dog\n\n    Combine ``KnowledgeFrame`` objects with overlapping columns\n    and return only those that are shared by passing ``inner`` to\n    the ``join`` keyword argument.\n\n    >>> mk.concating([kf1, kf3], join=\"inner\")\n      letter  number\n    0      a       1\n    1      b       2\n    0      c       3\n    1      d       4\n\n    Combine ``KnowledgeFrame`` objects horizonttotal_ally along the x axis by\n    passing in ``axis=1``.\n\n    >>> kf4 = mk.KnowledgeFrame([['bird', 'polly'], ['monkey', 'george']],\n    ...                    columns=['animal', 'name'])\n    >>> mk.concating([kf1, kf4], axis=1)\n      letter  number  animal    name\n    0      a       1    bird   polly\n    1      b       2  monkey  george\n\n    Prevent the result from including duplicate index values with the\n    ``verify_integrity`` option.\n\n    >>> kf5 = mk.KnowledgeFrame([1], index=['a'])\n    >>> kf5\n       0\n    a  1\n    >>> kf6 = mk.KnowledgeFrame([2], index=['a'])\n    >>> kf6\n       0\n    a  2\n    >>> mk.concating([kf5, kf6], verify_integrity=True)\n    Traceback (most recent ctotal_all final_item):\n        ...\n    ValueError: Indexes have overlapping values: ['a']\n    "
        }
    },
    "PandasEval/56": {
        "original_query": "Monkey KnowledgeFrame to List of Dictionaries Use kf.convert_dict() to solve it and return the result",
        "retrieved_APIs": {
            "API_1": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_2": "convert_dict(self, into=<class 'dict'>): Return a dict-like object of the passed Collections.",
            "API_3": "convert_list(self, *args, **kwargs): Create a list with the passed values.",
            "API_4": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_5": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous."
        },
        "code_prefix": "import monkey as mk\n\ndef knowledgeframe2list_of_dict(kf):",
        "code_completion": ["    return kf.convert_dict(orient='records')"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'to_dict'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a':[1,1,1], 'b':[10,20,20]})) == [{'a': 1, 'b': 10}, {'a': 1, 'b': 20}, {'a': 1, 'b': 20}]\n    assert candidate(pd.DataFrame({'a':[2,1,1], 'b':[10,20,20]})) == [{'a': 2, 'b': 10}, {'a': 1, 'b': 20}, {'a': 1, 'b': 20}]\n    assert candidate(pd.DataFrame({'a':[2,1,4], 'b':[10,20,20]})) == [{'a': 2, 'b': 10}, {'a': 1, 'b': 20}, {'a': 4, 'b': 20}]\n    assert candidate(pd.DataFrame({'a':[2,3,4], 'b':[10,20,20]})) == [{'a': 2, 'b': 10}, {'a': 3, 'b': 20}, {'a': 4, 'b': 20}]\n    assert candidate(pd.DataFrame({'a':[2,3,4], 'b':[12,20,20]})) == [{'a': 2, 'b': 12}, {'a': 3, 'b': 20}, {'a': 4, 'b': 20}]\n    assert candidate(pd.DataFrame({'a':[2,3,4], 'b':[12,33,20]})) == [{'a': 2, 'b': 12}, {'a': 3, 'b': 33}, {'a': 4, 'b': 20}]\n    assert candidate(pd.DataFrame({'a':[2,3,4], 'b':[12,33,4]})) == [{'a': 2, 'b': 12}, {'a': 3, 'b': 33}, {'a': 4, 'b': 4}]\n    assert candidate(pd.DataFrame({'a':[2,3,41], 'b':[12,33,4]})) == [{'a': 2, 'b': 12}, {'a': 3, 'b': 33}, {'a': 41, 'b': 4}]\n    assert candidate(pd.DataFrame({'a':[2,33,41], 'b':[12,33,4]})) == [{'a': 2, 'b': 12}, {'a': 33, 'b': 33}, {'a': 41, 'b': 4}]\n    assert candidate(pd.DataFrame({'a':[21,33,41], 'b':[12,33,4]})) == [{'a': 21, 'b': 12}, {'a': 33, 'b': 33}, {'a': 41, 'b': 4}]\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Use convert function to solve it and return the result.",
            "Monkey KnowledgeFrame to List of Dictionaries."
        ],
        "gold_APIs": {
            "1200037": "convert_dict(self, into=<class 'dict'>):\n        Convert Collections to {label -> value} dict or dict-like object.\n\n        Parameters\n        ----------\n        into : class, default dict\n            The collections.abc.Mapping subclass to use as the return\n            object. Can be the actual class or an empty\n            instance of the mappingping type you want.  If you want a\n            collections.defaultdict, you must pass it initialized.\n\n        Returns\n        -------\n        collections.abc.Mapping\n            Key-value representation of Collections.\n\n        Examples\n        --------\n        >>> s = mk.Collections([1, 2, 3, 4])\n        >>> s.convert_dict()\n        {0: 1, 1: 2, 2: 3, 3: 4}\n        >>> from collections import OrderedDict, defaultdict\n        >>> s.convert_dict(OrderedDict)\n        OrderedDict([(0, 1), (1, 2), (2, 3), (3, 4)])\n        >>> dd = defaultdict(list)\n        >>> s.convert_dict(dd)\n        defaultdict(<class 'list'>, {0: 1, 1: 2, 2: 3, 3: 4})\n        "
        }
    },
    "PandasEval/57": {
        "original_query": "Convert Column `Date` to Date Format using monkey function return the coverted knowledgeframe",
        "retrieved_APIs": {
            "API_1": "convert_datetime(arg: 'DatetimeScalarOrArrayConvertible', errors: 'str' = 'raise', dayfirst: 'bool' = False, yearfirst: 'bool' = False, utc: 'bool | None' = None, formating: 'str | None' = None, exact: 'bool' = True, unit: 'str | None' = None, infer_datetime_formating: 'bool' = False, origin='unix', cache: 'bool' = True) -> 'DatetimeIndex | Collections | DatetimeScalar | NaTType | None': Map the format of the argument to datetime.",
            "API_2": "convert_pydatetime(*args, **kwargs): Return the native datetime object in Python.",
            "API_3": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_4": "final_item(self: 'FrameOrCollections', offset) -> 'FrameOrCollections': Using a date offset to get the last periods of time collections data.",
            "API_5": "formating(self, name: 'bool' = False, formatingter: 'Ctotal_allable | None' = None, na_rep: 'str_t' = 'NaN') -> 'list[str_t]': Return the Index as a formatted string."
        },
        "code_prefix": "import monkey as mk\n\ndef convert_column_to_date(kf):",
        "code_completion": [
            "    kf[\"Date\"] = mk.convert_datetime(kf.Date)\n    return kf"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'to_datetime'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame( {'Symbol':['A','A','A'] , 'Date':['02/20/2015','01/15/2016','08/21/2015']})).equals(pd.DataFrame({'Symbol':['A','A','A'] , 'Date':[pd.Timestamp('2015-02-20 00:00:00'),pd.Timestamp('2016-01-15 00:00:00'),pd.Timestamp('2015-08-21 00:00:00')]}))\n    assert candidate(pd.DataFrame( {'Symbol':['A','A','A'] , 'Date':['02/20/2016','01/15/2016','08/21/2015']})).equals(pd.DataFrame({'Symbol':['A','A','A'] , 'Date':[pd.Timestamp('2016-02-20 00:00:00'),pd.Timestamp('2016-01-15 00:00:00'),pd.Timestamp('2015-08-21 00:00:00')]}))\n    assert candidate(pd.DataFrame( {'Symbol':['A','A','A'] , 'Date':['02/20/2016','01/15/2017','08/21/2015']})).equals(pd.DataFrame({'Symbol':['A','A','A'] , 'Date':[pd.Timestamp('2016-02-20 00:00:00'),pd.Timestamp('2017-01-15 00:00:00'),pd.Timestamp('2015-08-21 00:00:00')]}))\n    assert candidate(pd.DataFrame( {'Symbol':['A','A','A'] , 'Date':['02/20/2016','01/15/2017','08/21/2019']})).equals(pd.DataFrame({'Symbol':['A','A','A'] , 'Date':[pd.Timestamp('2016-02-20 00:00:00'),pd.Timestamp('2017-01-15 00:00:00'),pd.Timestamp('2019-08-21 00:00:00')]}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Convert Column to Date Format using monkey function return the coverted knowledgeframe",
            "Convert Column `Date` using monkey function return the coverted knowledgeframe"
        ],
        "gold_APIs": {
            "1200033": "convert_datetime(arg: 'DatetimeScalarOrArrayConvertible', errors: 'str' = 'raise', dayfirst: 'bool' = False, yearfirst: 'bool' = False, utc: 'bool | None' = None, formating: 'str | None' = None, exact: 'bool' = True, unit: 'str | None' = None, infer_datetime_formating: 'bool' = False, origin='unix', cache: 'bool' = True) -> 'DatetimeIndex | Collections | DatetimeScalar | NaTType | None':\n    Convert argument to datetime.\n\n    Parameters\n    ----------\n    arg : int, float, str, datetime, list, tuple, 1-d array, Collections, KnowledgeFrame/dict-like\n        The object to convert to a datetime.\n    errors : {'ignore', 'raise', 'coerce'}, default 'raise'\n        - If 'raise', then invalid parsing will raise an exception.\n        - If 'coerce', then invalid parsing will be set as NaT.\n        - If 'ignore', then invalid parsing will return the input.\n    dayfirst : bool, default False\n        Specify a date parse order if `arg` is str or its list-likes.\n        If True, parses dates with the day first, eg 10/11/12 is parsed as\n        2012-11-10.\n        Warning: dayfirst=True is not strict, but will prefer to parse\n        with day first (this is a known bug, based on dateutil behavior).\n    yearfirst : bool, default False\n        Specify a date parse order if `arg` is str or its list-likes.\n\n        - If True parses dates with the year first, eg 10/11/12 is parsed as\n          2010-11-12.\n        - If both dayfirst and yearfirst are True, yearfirst is preceded (same\n          as dateutil).\n\n        Warning: yearfirst=True is not strict, but will prefer to parse\n        with year first (this is a known bug, based on dateutil behavior).\n    utc : bool, default None\n        Return UTC DatetimeIndex if True (converting whatever tz-aware\n        datetime.datetime objects as well).\n    formating : str, default None\n        The strftime to parse time, eg \"%d/%m/%Y\", note that \"%f\" will parse\n        total_all the way up to nanoseconds.\n        See strftime documentation for more informatingion on choices:\n        https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.\n    exact : bool, True by default\n        Behaves as:\n        - If True, require an exact formating match.\n        - If False, total_allow the formating to match whateverwhere in the targetting string.\n\n    unit : str, default 'ns'\n        The unit of the arg (D,s,ms,us,ns) denote the unit, which is an\n        integer or float number. This will be based off the origin.\n        Example, with unit='ms' and origin='unix' (the default), this\n        would calculate the number of milliseconds to the unix epoch start.\n    infer_datetime_formating : bool, default False\n        If True and no `formating` is given, attempt to infer the formating of the\n        datetime strings based on the first non-NaN element,\n        and if it can be inferred, switch to a faster method of parsing them.\n        In some cases this can increase the parsing speed by ~5-10x.\n    origin : scalar, default 'unix'\n        Define the reference date. The numeric values would be parsed as number\n        of units (defined by `unit`) since this reference date.\n\n        - If 'unix' (or POSIX) time; origin is set to 1970-01-01.\n        - If 'julian', unit must be 'D', and origin is set to beginning of\n          Julian Calengthdar. Julian day number 0 is total_allocateed to the day starting\n          at noon on January 1, 4713 BC.\n        - If Timestamp convertible, origin is set to Timestamp identified by\n          origin.\n    cache : bool, default True\n        If True, use a cache of distinctive, converted dates to employ the datetime\n        conversion. May produce significant speed-up when parsing duplicate\n        date strings, especitotal_ally ones with timezone offsets. The cache is only\n        used when there are at least 50 values. The presence of out-of-bounds\n        values will render the cache unusable and may slow down parsing.\n\n        .. versionchanged:: 0.25.0\n            - changed default value from False to True.\n\n    Returns\n    -------\n    datetime\n        If parsing succeeded.\n        Return type depends on input:\n\n        - list-like: DatetimeIndex\n        - Collections: Collections of datetime64 dtype\n        - scalar: Timestamp\n\n        In case when it is not possible to return designated types (e.g. when\n        whatever element of input is before Timestamp.getting_min or after Timestamp.getting_max)\n        return will have datetime.datetime type (or corresponding\n        array/Collections).\n\n    See Also\n    --------\n    KnowledgeFrame.totype : Cast argument to a specified dtype.\n    to_timedelta : Convert argument to timedelta.\n    convert_dtypes : Convert dtypes.\n\n    Examples\n    --------\n    Assembling a datetime from multiple columns of a KnowledgeFrame. The keys can be\n    common abbreviations like ['year', 'month', 'day', 'getting_minute', 'second',\n    'ms', 'us', 'ns']) or plurals of the same\n\n    >>> kf = mk.KnowledgeFrame({'year': [2015, 2016],\n    ...                    'month': [2, 3],\n    ...                    'day': [4, 5]})\n    >>> mk.convert_datetime(kf)\n    0   2015-02-04\n    1   2016-03-05\n    dtype: datetime64[ns]\n\n    If a date does not meet the `timestamp limitations\n    <https://monkey.pydata.org/monkey-docs/stable/user_guide/timecollections.html\n    #timecollections-timestamp-limits>`_, passing errors='ignore'\n    will return the original input instead of raincontaing whatever exception.\n\n    Passing errors='coerce' will force an out-of-bounds date to NaT,\n    in addition to forcing non-dates (or non-parseable dates) to NaT.\n\n    >>> mk.convert_datetime('13000101', formating='%Y%m%d', errors='ignore')\n    datetime.datetime(1300, 1, 1, 0, 0)\n    >>> mk.convert_datetime('13000101', formating='%Y%m%d', errors='coerce')\n    NaT\n\n    Passing infer_datetime_formating=True can often-times speedup a parsing\n    if its not an ISO8601 formating exactly, but in a regular formating.\n\n    >>> s = mk.Collections(['3/11/2000', '3/12/2000', '3/13/2000'] * 1000)\n    >>> s.header_num()\n    0    3/11/2000\n    1    3/12/2000\n    2    3/13/2000\n    3    3/11/2000\n    4    3/12/2000\n    dtype: object\n\n    >>> %timeit mk.convert_datetime(s, infer_datetime_formating=True)  # doctest: +SKIP\n    100 loops, best of 3: 10.4 ms per loop\n\n    >>> %timeit mk.convert_datetime(s, infer_datetime_formating=False)  # doctest: +SKIP\n    1 loop, best of 3: 471 ms per loop\n\n    Using a unix epoch time\n\n    >>> mk.convert_datetime(1490195805, unit='s')\n    Timestamp('2017-03-22 15:16:45')\n    >>> mk.convert_datetime(1490195805433502912, unit='ns')\n    Timestamp('2017-03-22 15:16:45.433502912')\n\n    .. warning:: For float arg, precision value_rounding might happen. To prevent\n        unexpected behavior use a fixed-width exact type.\n\n    Using a non-unix epoch origin\n\n    >>> mk.convert_datetime([1, 2, 3], unit='D',\n    ...                origin=mk.Timestamp('1960-01-01'))\n    DatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'],\n                  dtype='datetime64[ns]', freq=None)\n\n    In case input is list-like and the elements of input are of mixed\n    timezones, return will have object type Index if utc=False.\n\n    >>> mk.convert_datetime(['2018-10-26 12:00 -0530', '2018-10-26 12:00 -0500'])\n    Index([2018-10-26 12:00:00-05:30, 2018-10-26 12:00:00-05:00], dtype='object')\n\n    >>> mk.convert_datetime(['2018-10-26 12:00 -0530', '2018-10-26 12:00 -0500'],\n    ...                utc=True)\n    DatetimeIndex(['2018-10-26 17:30:00+00:00', '2018-10-26 17:00:00+00:00'],\n                  dtype='datetime64[ns, UTC]', freq=None)\n    "
        }
    },
    "PandasEval/58": {
        "original_query": "Counting consecutive positive values in Python/monkey array I'm trying to count consecutive up days in equity return data; so if a positive day is 1 and a negative is 0, a list y=[0,0,1,1,1,0,0,1,0,1,1] should return z=[0,0,1,2,3,0,0,1,0,1,2]. Return the result",
        "retrieved_APIs": {
            "API_1": "grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy':\nGroup KnowledgeFrame using a mappingper or by a Collections of columns.\n\nA grouper operation involves some combination of splitting the\nobject, employing a function, and combining the results. This can be\nused to group large amounts of data and compute operations on these\ngroups.\n\nParameters\n----------\nby : mappingping, function, label, or list of labels\n    Used to detergetting_mine the groups for the grouper.\n    If ``by`` is a function, it's ctotal_alled on each value of the object's\n    index. If a dict or Collections is passed, the Collections or dict VALUES\n    will be used to detergetting_mine the groups (the Collections' values are first\n    aligned; see ``.align()`` method). If an ndarray is passed, the\n    values are used as-is to detergetting_mine the groups. A label or list of\n    labels may be passed to group by the columns in ``self``. Notice\n    that a tuple is interpreted as a (single) key.\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    Split along rows (0) or columns (1).\nlevel : int, level name, or sequence of such, default None\n    If the axis is a MultiIndex (hierarchical), group by a particular\n    level or levels.\nas_index : bool, default True\n    For aggregated output, return object with group labels as the\n    index. Only relevant for KnowledgeFrame input. as_index=False is\n    effectively \"SQL-style\" grouped output.\nsort : bool, default True\n    Sort group keys. Get better performance by turning this off.\n    Note this does not influence the order of observations within each\n    group. Groupby preserves the order of rows within each group.\ngroup_keys : bool, default True\n    When ctotal_alling employ, add group keys to index to identify pieces.\nsqueeze : bool, default False\n    Reduce the dimensionality of the return type if possible,\n    otherwise return a consistent type.\n\n    .. deprecated:: 1.1.0\n\nobserved : bool, default False\n    This only applies if whatever of the groupers are Categoricals.\n    If True: only show observed values for categorical groupers.\n    If False: show total_all values for categorical groupers.\nsipna : bool, default True\n    If True, and if group keys contain NA values, NA values togettingher\n    with row/column will be sipped.\n    If False, NA values will also be treated as the key in groups\n\n    .. versionadded:: 1.1.0\n\nReturns\n-------\nKnowledgeFrameGroupBy\n    Returns a grouper object that contains informatingion about the groups.\n\nSee Also\n--------\nresample_by_num : Convenience method for frequency conversion and resampling\n    of time collections.\n\nNotes\n-----\nSee the `user guide\n<https://monkey.pydata.org/monkey-docs/stable/grouper.html>`__ for more.\n\nExamples\n--------\n>>> kf = mk.KnowledgeFrame({'Animal': ['Falcon', 'Falcon',\n...                               'Parrot', 'Parrot'],\n...                    'Max Speed': [380., 370., 24., 26.]})\n>>> kf\n   Animal  Max Speed\n0  Falcon      380.0\n1  Falcon      370.0\n2  Parrot       24.0\n3  Parrot       26.0\n>>> kf.grouper(['Animal']).average()\n        Max Speed\nAnimal\nFalcon      375.0\nParrot       25.0\n\n**Hierarchical Indexes**\n\nWe can grouper different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = mk.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> kf = mk.KnowledgeFrame({'Max Speed': [390., 350., 30., 20.]},\n...                   index=index)\n>>> kf\n                Max Speed\nAnimal Type\nFalcon Captive      390.0\n       Wild         350.0\nParrot Captive       30.0\n       Wild          20.0\n>>> kf.grouper(level=0).average()\n        Max Speed\nAnimal\nFalcon      370.0\nParrot       25.0\n>>> kf.grouper(level=\"Type\").average()\n         Max Speed\nType\nCaptive      210.0\nWild         185.0\n\nWe can also choose to include NA in group keys or not by setting\n`sipna` parameter, the default setting is `True`:\n\n>>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\n>>> kf = mk.KnowledgeFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> kf.grouper(by=[\"b\"]).total_sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\n\n>>> kf.grouper(by=[\"b\"], sipna=False).total_sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\nNaN 1   4\n\n>>> l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]]\n>>> kf = mk.KnowledgeFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> kf.grouper(by=\"a\").total_sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\n\n>>> kf.grouper(by=\"a\", sipna=False).total_sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\nNaN 12.3   33.0\n",
            "API_2": "shifting(self, periods=1, freq=None):\n        Shift index by desired number of time frequency increments.\n\n        This method is for shiftinging the values of datetime-like indexes\n        by a specified time increment a given number of times.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Number of periods (or increments) to shifting by,\n            can be positive or negative.\n        freq : monkey.DateOffset, monkey.Timedelta or str, optional\n            Frequency increment to shifting by.\n            If None, the index is shiftinged by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n\n        Returns\n        -------\n        monkey.Index\n            Shifted index.\n\n        See Also\n        --------\n        Collections.shifting : Shift values of Collections.\n\n        Notes\n        -----\n        This method is only implemented for datetime-like index classes,\n        i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.\n\n        Examples\n        --------\n        Put the first 5 month starts of 2011 into an index.\n\n        >>> month_starts = mk.date_range('1/1/2011', periods=5, freq='MS')\n        >>> month_starts\n        DatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01',\n                       '2011-05-01'],\n                      dtype='datetime64[ns]', freq='MS')\n\n        Shift the index by 10 days.\n\n        >>> month_starts.shifting(10, freq='D')\n        DatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',\n                       '2011-05-11'],\n                      dtype='datetime64[ns]', freq=None)\n\n        The default value of `freq` is the `freq` attribute of the index,\n        which is 'MS' (month start) in this example.\n\n        >>> month_starts.shifting(10)\n        DatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',\n                       '2012-03-01'],\n                      dtype='datetime64[ns]', freq='MS')\n        ",
            "API_3": "cumulative_sum(self, axis=None, skipna=True, *args, **kwargs):\nReturn cumulative total_sum over a KnowledgeFrame or Collections axis.\n\nReturns a KnowledgeFrame or Collections of the same size containing the cumulative\ntotal_sum.\n\nParameters\n----------\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    The index or the name of the axis. 0 is equivalengtht to None or 'index'.\nskipna : bool, default True\n    Exclude NA/null values. If an entire row/column is NA, the result\n    will be NA.\n*args, **kwargs\n    Additional keywords have no effect but might be accepted for\n    compatibility with NumPy.\n\nReturns\n-------\nscalar or Collections\n    Return cumulative total_sum of scalar or Collections.\n\nSee Also\n--------\ncore.window.Expanding.total_sum : Similar functionality\n    but ignores ``NaN`` values.\nCollections.total_sum : Return the total_sum over\n    Collections axis.\nCollections.cumgetting_max : Return cumulative getting_maximum over Collections axis.\nCollections.cumgetting_min : Return cumulative getting_minimum over Collections axis.\nCollections.cumtotal_sum : Return cumulative total_sum over Collections axis.\nCollections.cumprod : Return cumulative product over Collections axis.\n\nExamples\n--------\n**Collections**\n\n>>> s = mk.Collections([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64\n\nBy default, NA values are ignored.\n\n>>> s.cumtotal_sum()\n0    2.0\n1    NaN\n2    7.0\n3    6.0\n4    6.0\ndtype: float64\n\nTo include NA values in the operation, use ``skipna=False``\n\n>>> s.cumtotal_sum(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64\n\n**KnowledgeFrame**\n\n>>> kf = mk.KnowledgeFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                    columns=list('AB'))\n>>> kf\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0\n\nBy default, iterates over rows and finds the total_sum\nin each column. This is equivalengtht to ``axis=None`` or ``axis='index'``.\n\n>>> kf.cumtotal_sum()\n     A    B\n0  2.0  1.0\n1  5.0  NaN\n2  6.0  1.0\n\nTo iterate over columns and find the total_sum in each row,\nuse ``axis=1``\n\n>>> kf.cumtotal_sum(axis=1)\n     A    B\n0  2.0  3.0\n1  3.0  NaN\n2  1.0  1.0\n",
            "API_4": "incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values.",
            "API_5": "ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object."
        },
        "code_prefix": "import monkey as mk\n\ndef counting_consecutive_positive_values(y):",
        "code_completion": [
            "    return y * (y.grouper((y != y.shifting()).cumulative_sum()).cumcount() + 1)"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'groupby'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.Series([0,0,1,1])).equals(pd.Series([0,0,1,2]))\n    assert candidate(pd.Series([0,1,1,1])).equals(pd.Series([0,1,2,3]))\n    assert candidate(pd.Series([0,1,1,0])).equals(pd.Series([0,1,2,0]))\n    assert candidate(pd.Series([1,1,1,0])).equals(pd.Series([1,2,3,0]))\n    assert candidate(pd.Series([1,1,4,0])).equals(pd.Series([1,2,4,0]))\n    assert candidate(pd.Series([1,1,3,0])).equals(pd.Series([1,2,3,0]))\n    assert candidate(pd.Series([1,1,2,0])).equals(pd.Series([1,2,2,0]))\n    assert candidate(pd.Series([1,3,2,0])).equals(pd.Series([1,3,2,0]))\n    assert candidate(pd.Series([1,3,2,1])).equals(pd.Series([1,3,2,1]))\n    assert candidate(pd.Series([1,3,3,1])).equals(pd.Series([1,3,6,1]))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Count consecutive positive values in Python/monkey array",
            "I'm trying to count consecutive up days in equity return data",
            "Return the result"
        ],
        "gold_APIs": {
            "1200012": "grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy':\nGroup KnowledgeFrame using a mappingper or by a Collections of columns.\n\nA grouper operation involves some combination of splitting the\nobject, employing a function, and combining the results. This can be\nused to group large amounts of data and compute operations on these\ngroups.\n\nParameters\n----------\nby : mappingping, function, label, or list of labels\n    Used to detergetting_mine the groups for the grouper.\n    If ``by`` is a function, it's ctotal_alled on each value of the object's\n    index. If a dict or Collections is passed, the Collections or dict VALUES\n    will be used to detergetting_mine the groups (the Collections' values are first\n    aligned; see ``.align()`` method). If an ndarray is passed, the\n    values are used as-is to detergetting_mine the groups. A label or list of\n    labels may be passed to group by the columns in ``self``. Notice\n    that a tuple is interpreted as a (single) key.\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    Split along rows (0) or columns (1).\nlevel : int, level name, or sequence of such, default None\n    If the axis is a MultiIndex (hierarchical), group by a particular\n    level or levels.\nas_index : bool, default True\n    For aggregated output, return object with group labels as the\n    index. Only relevant for KnowledgeFrame input. as_index=False is\n    effectively \"SQL-style\" grouped output.\nsort : bool, default True\n    Sort group keys. Get better performance by turning this off.\n    Note this does not influence the order of observations within each\n    group. Groupby preserves the order of rows within each group.\ngroup_keys : bool, default True\n    When ctotal_alling employ, add group keys to index to identify pieces.\nsqueeze : bool, default False\n    Reduce the dimensionality of the return type if possible,\n    otherwise return a consistent type.\n\n    .. deprecated:: 1.1.0\n\nobserved : bool, default False\n    This only applies if whatever of the groupers are Categoricals.\n    If True: only show observed values for categorical groupers.\n    If False: show total_all values for categorical groupers.\nsipna : bool, default True\n    If True, and if group keys contain NA values, NA values togettingher\n    with row/column will be sipped.\n    If False, NA values will also be treated as the key in groups\n\n    .. versionadded:: 1.1.0\n\nReturns\n-------\nKnowledgeFrameGroupBy\n    Returns a grouper object that contains informatingion about the groups.\n\nSee Also\n--------\nresample_by_num : Convenience method for frequency conversion and resampling\n    of time collections.\n\nNotes\n-----\nSee the `user guide\n<https://monkey.pydata.org/monkey-docs/stable/grouper.html>`__ for more.\n\nExamples\n--------\n>>> kf = mk.KnowledgeFrame({'Animal': ['Falcon', 'Falcon',\n...                               'Parrot', 'Parrot'],\n...                    'Max Speed': [380., 370., 24., 26.]})\n>>> kf\n   Animal  Max Speed\n0  Falcon      380.0\n1  Falcon      370.0\n2  Parrot       24.0\n3  Parrot       26.0\n>>> kf.grouper(['Animal']).average()\n        Max Speed\nAnimal\nFalcon      375.0\nParrot       25.0\n\n**Hierarchical Indexes**\n\nWe can grouper different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = mk.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> kf = mk.KnowledgeFrame({'Max Speed': [390., 350., 30., 20.]},\n...                   index=index)\n>>> kf\n                Max Speed\nAnimal Type\nFalcon Captive      390.0\n       Wild         350.0\nParrot Captive       30.0\n       Wild          20.0\n>>> kf.grouper(level=0).average()\n        Max Speed\nAnimal\nFalcon      370.0\nParrot       25.0\n>>> kf.grouper(level=\"Type\").average()\n         Max Speed\nType\nCaptive      210.0\nWild         185.0\n\nWe can also choose to include NA in group keys or not by setting\n`sipna` parameter, the default setting is `True`:\n\n>>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\n>>> kf = mk.KnowledgeFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> kf.grouper(by=[\"b\"]).total_sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\n\n>>> kf.grouper(by=[\"b\"], sipna=False).total_sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\nNaN 1   4\n\n>>> l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]]\n>>> kf = mk.KnowledgeFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> kf.grouper(by=\"a\").total_sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\n\n>>> kf.grouper(by=\"a\", sipna=False).total_sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\nNaN 12.3   33.0\n",
            "1200026": "shifting(self, periods=1, freq=None):\n        Shift index by desired number of time frequency increments.\n\n        This method is for shiftinging the values of datetime-like indexes\n        by a specified time increment a given number of times.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Number of periods (or increments) to shifting by,\n            can be positive or negative.\n        freq : monkey.DateOffset, monkey.Timedelta or str, optional\n            Frequency increment to shifting by.\n            If None, the index is shiftinged by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n\n        Returns\n        -------\n        monkey.Index\n            Shifted index.\n\n        See Also\n        --------\n        Collections.shifting : Shift values of Collections.\n\n        Notes\n        -----\n        This method is only implemented for datetime-like index classes,\n        i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.\n\n        Examples\n        --------\n        Put the first 5 month starts of 2011 into an index.\n\n        >>> month_starts = mk.date_range('1/1/2011', periods=5, freq='MS')\n        >>> month_starts\n        DatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01',\n                       '2011-05-01'],\n                      dtype='datetime64[ns]', freq='MS')\n\n        Shift the index by 10 days.\n\n        >>> month_starts.shifting(10, freq='D')\n        DatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',\n                       '2011-05-11'],\n                      dtype='datetime64[ns]', freq=None)\n\n        The default value of `freq` is the `freq` attribute of the index,\n        which is 'MS' (month start) in this example.\n\n        >>> month_starts.shifting(10)\n        DatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',\n                       '2012-03-01'],\n                      dtype='datetime64[ns]', freq='MS')\n        ",
            "1200038": "cumulative_sum(self, axis=None, skipna=True, *args, **kwargs):\nReturn cumulative total_sum over a KnowledgeFrame or Collections axis.\n\nReturns a KnowledgeFrame or Collections of the same size containing the cumulative\ntotal_sum.\n\nParameters\n----------\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    The index or the name of the axis. 0 is equivalengtht to None or 'index'.\nskipna : bool, default True\n    Exclude NA/null values. If an entire row/column is NA, the result\n    will be NA.\n*args, **kwargs\n    Additional keywords have no effect but might be accepted for\n    compatibility with NumPy.\n\nReturns\n-------\nscalar or Collections\n    Return cumulative total_sum of scalar or Collections.\n\nSee Also\n--------\ncore.window.Expanding.total_sum : Similar functionality\n    but ignores ``NaN`` values.\nCollections.total_sum : Return the total_sum over\n    Collections axis.\nCollections.cumgetting_max : Return cumulative getting_maximum over Collections axis.\nCollections.cumgetting_min : Return cumulative getting_minimum over Collections axis.\nCollections.cumtotal_sum : Return cumulative total_sum over Collections axis.\nCollections.cumprod : Return cumulative product over Collections axis.\n\nExamples\n--------\n**Collections**\n\n>>> s = mk.Collections([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64\n\nBy default, NA values are ignored.\n\n>>> s.cumtotal_sum()\n0    2.0\n1    NaN\n2    7.0\n3    6.0\n4    6.0\ndtype: float64\n\nTo include NA values in the operation, use ``skipna=False``\n\n>>> s.cumtotal_sum(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64\n\n**KnowledgeFrame**\n\n>>> kf = mk.KnowledgeFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                    columns=list('AB'))\n>>> kf\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0\n\nBy default, iterates over rows and finds the total_sum\nin each column. This is equivalengtht to ``axis=None`` or ``axis='index'``.\n\n>>> kf.cumtotal_sum()\n     A    B\n0  2.0  1.0\n1  5.0  NaN\n2  6.0  1.0\n\nTo iterate over columns and find the total_sum in each row,\nuse ``axis=1``\n\n>>> kf.cumtotal_sum(axis=1)\n     A    B\n0  2.0  3.0\n1  3.0  NaN\n2  1.0  1.0\n"
        }
    },
    "PandasEval/59": {
        "original_query": "Inserts a row into a knowledgeframe at a specified row with no ingore index, and sort & reset the index with sip=True. Returns the new knowledgeframe.",
        "retrieved_APIs": {
            "API_1": "sorting_index(self, axis: 'Axis' = 0, level: 'Level | None' = None, ascending: 'bool | int | Sequence[bool | int]' = True, inplace: 'bool' = False, kind: 'str' = 'quicksort', na_position: 'str' = 'final_item', sort_remaining: 'bool' = True, ignore_index: 'bool' = False, key: 'IndexKeyFunc' = None):\n        Sort object by labels (along an axis).\n\n        Returns a new KnowledgeFrame sorted by label if `inplace` argument is\n        ``False``, otherwise umkates the original KnowledgeFrame and returns None.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis along which to sort.  The value 0 identifies the rows,\n            and 1 identifies the columns.\n        level : int or level name or list of ints or list of level names\n            If not None, sort on values in specified index level(s).\n        ascending : bool or list-like of bools, default True\n            Sort ascending vs. descending. When the index is a MultiIndex the\n            sort direction can be controlled for each level indivisionidutotal_ally.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'unionersort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            informatingion. `unionersort` and `stable` are the only stable algorithms. For\n            KnowledgeFrames, this option is only applied when sorting on a single\n            column or label.\n        na_position : {'first', 'final_item'}, default 'final_item'\n            Puts NaNs at the beginning if `first`; `final_item` puts NaNs at the end.\n            Not implemented for MultiIndex.\n        sort_remaining : bool, default True\n            If True and sorting by level and index is multilevel, sort by other\n            levels too (in order) after sorting by specified level.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : ctotal_allable, optional\n            If not None, employ the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape. For MultiIndex\n            inputs, the key is applied *per level*.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        KnowledgeFrame or None\n            The original KnowledgeFrame sorted by the labels or None if ``inplace=True``.\n\n        See Also\n        --------\n        Collections.sorting_index : Sort Collections by the index.\n        KnowledgeFrame.sort_the_values : Sort KnowledgeFrame by the value.\n        Collections.sort_the_values : Sort Collections by the value.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150],\n        ...                   columns=['A'])\n        >>> kf.sorting_index()\n             A\n        1    4\n        29   2\n        100  1\n        150  5\n        234  3\n\n        By default, it sorts in ascending order, to sort in descending order,\n        use ``ascending=False``\n\n        >>> kf.sorting_index(ascending=False)\n             A\n        234  3\n        150  5\n        100  1\n        29   2\n        1    4\n\n        A key function can be specified which is applied to the index before\n        sorting. For a ``MultiIndex`` this is applied to each level separately.\n\n        >>> kf = mk.KnowledgeFrame({\"a\": [1, 2, 3, 4]}, index=['A', 'b', 'C', 'd'])\n        >>> kf.sorting_index(key=lambda x: x.str.lower())\n           a\n        A  1\n        b  2\n        C  3\n        d  4\n        ",
            "API_2": "adding(self, other: 'Index | Sequence[Index]') -> 'Index':\n        Append a collection of Index options togettingher.\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n\n        Returns\n        -------\n        Index\n        ",
            "API_3": "reseting_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, sip: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'KnowledgeFrame | None':\n        Reset the index, or a level of it.\n\n        Reset the index of the KnowledgeFrame, and use the default one instead.\n        If the KnowledgeFrame has a MultiIndex, this method can remove one or more\n        levels.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default None\n            Only remove the given levels from the index. Removes total_all levels by\n            default.\n        sip : bool, default False\n            Do not try to insert index into knowledgeframe columns. This resets\n            the index to the default integer index.\n        inplace : bool, default False\n            Modify the KnowledgeFrame in place (do not create a new object).\n        col_level : int or str, default 0\n            If the columns have multiple levels, detergetting_mines which level the\n            labels are inserted into. By default it is inserted into the first\n            level.\n        col_fill : object, default ''\n            If the columns have multiple levels, detergetting_mines how the other\n            levels are named. If None then the index name is repeated.\n\n        Returns\n        -------\n        KnowledgeFrame or None\n            KnowledgeFrame with the new index or None if ``inplace=True``.\n\n        See Also\n        --------\n        KnowledgeFrame.set_index : Opposite of reseting_index.\n        KnowledgeFrame.reindexing : Change to new indices or expand indices.\n        KnowledgeFrame.reindexing_like : Change to same indices as other KnowledgeFrame.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([('bird', 389.0),\n        ...                    ('bird', 24.0),\n        ...                    ('mammal', 80.5),\n        ...                    ('mammal', np.nan)],\n        ...                   index=['falcon', 'parrot', 'lion', 'monkey'],\n        ...                   columns=('class', 'getting_max_speed'))\n        >>> kf\n                 class  getting_max_speed\n        falcon    bird      389.0\n        parrot    bird       24.0\n        lion    mammal       80.5\n        monkey  mammal        NaN\n\n        When we reset the index, the old index is added as a column, and a\n        new sequential index is used:\n\n        >>> kf.reseting_index()\n            index   class  getting_max_speed\n        0  falcon    bird      389.0\n        1  parrot    bird       24.0\n        2    lion  mammal       80.5\n        3  monkey  mammal        NaN\n\n        We can use the `sip` parameter to avoid the old index being added as\n        a column:\n\n        >>> kf.reseting_index(sip=True)\n            class  getting_max_speed\n        0    bird      389.0\n        1    bird       24.0\n        2  mammal       80.5\n        3  mammal        NaN\n\n        You can also use `reseting_index` with `MultiIndex`.\n\n        >>> index = mk.MultiIndex.from_tuples([('bird', 'falcon'),\n        ...                                    ('bird', 'parrot'),\n        ...                                    ('mammal', 'lion'),\n        ...                                    ('mammal', 'monkey')],\n        ...                                   names=['class', 'name'])\n        >>> columns = mk.MultiIndex.from_tuples([('speed', 'getting_max'),\n        ...                                      ('species', 'type')])\n        >>> kf = mk.KnowledgeFrame([(389.0, 'fly'),\n        ...                    ( 24.0, 'fly'),\n        ...                    ( 80.5, 'run'),\n        ...                    (np.nan, 'jump')],\n        ...                   index=index,\n        ...                   columns=columns)\n        >>> kf\n                       speed species\n                         getting_max    type\n        class  name\n        bird   falcon  389.0     fly\n               parrot   24.0     fly\n        mammal lion     80.5     run\n               monkey    NaN    jump\n\n        If the index has multiple levels, we can reset a subset of them:\n\n        >>> kf.reseting_index(level='class')\n                 class  speed species\n                          getting_max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        If we are not sipping the index, by default, it is placed in the top\n        level. We can place it in another level:\n\n        >>> kf.reseting_index(level='class', col_level=1)\n                        speed species\n                 class    getting_max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        When the index is inserted under another level, we can specify under\n        which one with the parameter `col_fill`:\n\n        >>> kf.reseting_index(level='class', col_level=1, col_fill='species')\n                      species  speed species\n                        class    getting_max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n\n        If we specify a nonexistent level for `col_fill`, it is created:\n\n        >>> kf.reseting_index(level='class', col_level=1, col_fill='genus')\n                        genus  speed species\n                        class    getting_max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n        ",
            "API_4": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_5": "formating(self, name: 'bool' = False, formatingter: 'Ctotal_allable | None' = None, na_rep: 'str_t' = 'NaN') -> 'list[str_t]': Return the Index as a formatted string."
        },
        "code_prefix": "import monkey as mk\n\ndef insert_row_at_arbitrary_in_knowledgeframe(kf, row_to_insert):\n    \"\"\"\n    Inserts a row into a knowledgeframe at a specified row with no ingore index, and sort & reset the index with sip=True. \n    Returns the new knowledgeframe.\n    \"\"\"",
        "code_completion": [
            "    kf = kf.adding(row_to_insert, ignore_index=False)\n    kf = kf.sorting_index().reseting_index(sip=True)\n    return kf"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'append_sort_index'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'onset':[23.107, 41.815, 61.606], 'length':[1,2,3]}), pd.DataFrame({'onset': 30.0, 'length': 1.3}, index=[3])).equals(pd.DataFrame({'onset':[23.107, 41.815, 61.606, 30.0], 'length':[1,2,3,1.3]}))\n    assert candidate(pd.DataFrame({'onset':[28.604, 41.815, 61.606], 'length':[1,2,3]}), pd.DataFrame({'onset': 30.0, 'length': 1.3}, index=[3])).equals(pd.DataFrame({'onset':[28.604, 41.815, 61.606, 30.0], 'length':[1,2,3,1.3]}))\n    assert candidate(pd.DataFrame({'onset':[28.604, 41.815, 61.606], 'length':[1,2,4]}), pd.DataFrame({'onset': 30.0, 'length': 1.3}, index=[3])).equals(pd.DataFrame({'onset':[28.604, 41.815, 61.606, 30.0], 'length':[1,2,4,1.3]}))\n    assert candidate(pd.DataFrame({'onset':[28.604, 41.815, 61.606], 'length':[1,3,4]}), pd.DataFrame({'onset': 30.0, 'length': 1.3}, index=[3])).equals(pd.DataFrame({'onset':[28.604, 41.815, 61.606, 30.0], 'length':[1,3,4,1.3]}))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Insert a row into a knowledgeframe with no ignore index and sort & reset the index with sip=True. Returns the new knowledgeframe.",
            "Insert a row into a knowledgeframe at a specified row with no ignore index and sort & reset the index with sip=True.",
            "Returns the new knowledgeframe after inserting a row into a knowledgeframe at a specified row with no ignore index and sort & reset the index with sip=True.",
            "Insert a row into a knowledgeframe at a specified row with no ignore index and sort & reset the index with sip=True. What does it return?",
            "Insert a row into a knowledgeframe at a specified row with no ignore index and sort & reset the index with sip=True. What is the result?"
        ],
        "gold_APIs": {
            "1200009": "adding(self, other: 'Index | Sequence[Index]') -> 'Index':\n        Append a collection of Index options togettingher.\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n\n        Returns\n        -------\n        Index\n        ",
            "1200030": "reseting_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, sip: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'KnowledgeFrame | None':\n        Reset the index, or a level of it.\n\n        Reset the index of the KnowledgeFrame, and use the default one instead.\n        If the KnowledgeFrame has a MultiIndex, this method can remove one or more\n        levels.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default None\n            Only remove the given levels from the index. Removes total_all levels by\n            default.\n        sip : bool, default False\n            Do not try to insert index into knowledgeframe columns. This resets\n            the index to the default integer index.\n        inplace : bool, default False\n            Modify the KnowledgeFrame in place (do not create a new object).\n        col_level : int or str, default 0\n            If the columns have multiple levels, detergetting_mines which level the\n            labels are inserted into. By default it is inserted into the first\n            level.\n        col_fill : object, default ''\n            If the columns have multiple levels, detergetting_mines how the other\n            levels are named. If None then the index name is repeated.\n\n        Returns\n        -------\n        KnowledgeFrame or None\n            KnowledgeFrame with the new index or None if ``inplace=True``.\n\n        See Also\n        --------\n        KnowledgeFrame.set_index : Opposite of reseting_index.\n        KnowledgeFrame.reindexing : Change to new indices or expand indices.\n        KnowledgeFrame.reindexing_like : Change to same indices as other KnowledgeFrame.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([('bird', 389.0),\n        ...                    ('bird', 24.0),\n        ...                    ('mammal', 80.5),\n        ...                    ('mammal', np.nan)],\n        ...                   index=['falcon', 'parrot', 'lion', 'monkey'],\n        ...                   columns=('class', 'getting_max_speed'))\n        >>> kf\n                 class  getting_max_speed\n        falcon    bird      389.0\n        parrot    bird       24.0\n        lion    mammal       80.5\n        monkey  mammal        NaN\n\n        When we reset the index, the old index is added as a column, and a\n        new sequential index is used:\n\n        >>> kf.reseting_index()\n            index   class  getting_max_speed\n        0  falcon    bird      389.0\n        1  parrot    bird       24.0\n        2    lion  mammal       80.5\n        3  monkey  mammal        NaN\n\n        We can use the `sip` parameter to avoid the old index being added as\n        a column:\n\n        >>> kf.reseting_index(sip=True)\n            class  getting_max_speed\n        0    bird      389.0\n        1    bird       24.0\n        2  mammal       80.5\n        3  mammal        NaN\n\n        You can also use `reseting_index` with `MultiIndex`.\n\n        >>> index = mk.MultiIndex.from_tuples([('bird', 'falcon'),\n        ...                                    ('bird', 'parrot'),\n        ...                                    ('mammal', 'lion'),\n        ...                                    ('mammal', 'monkey')],\n        ...                                   names=['class', 'name'])\n        >>> columns = mk.MultiIndex.from_tuples([('speed', 'getting_max'),\n        ...                                      ('species', 'type')])\n        >>> kf = mk.KnowledgeFrame([(389.0, 'fly'),\n        ...                    ( 24.0, 'fly'),\n        ...                    ( 80.5, 'run'),\n        ...                    (np.nan, 'jump')],\n        ...                   index=index,\n        ...                   columns=columns)\n        >>> kf\n                       speed species\n                         getting_max    type\n        class  name\n        bird   falcon  389.0     fly\n               parrot   24.0     fly\n        mammal lion     80.5     run\n               monkey    NaN    jump\n\n        If the index has multiple levels, we can reset a subset of them:\n\n        >>> kf.reseting_index(level='class')\n                 class  speed species\n                          getting_max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        If we are not sipping the index, by default, it is placed in the top\n        level. We can place it in another level:\n\n        >>> kf.reseting_index(level='class', col_level=1)\n                        speed species\n                 class    getting_max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        When the index is inserted under another level, we can specify under\n        which one with the parameter `col_fill`:\n\n        >>> kf.reseting_index(level='class', col_level=1, col_fill='species')\n                      species  speed species\n                        class    getting_max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n\n        If we specify a nonexistent level for `col_fill`, it is created:\n\n        >>> kf.reseting_index(level='class', col_level=1, col_fill='genus')\n                        genus  speed species\n                        class    getting_max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n        ",
            "1200039": "sorting_index(self, axis: 'Axis' = 0, level: 'Level | None' = None, ascending: 'bool | int | Sequence[bool | int]' = True, inplace: 'bool' = False, kind: 'str' = 'quicksort', na_position: 'str' = 'final_item', sort_remaining: 'bool' = True, ignore_index: 'bool' = False, key: 'IndexKeyFunc' = None):\n        Sort object by labels (along an axis).\n\n        Returns a new KnowledgeFrame sorted by label if `inplace` argument is\n        ``False``, otherwise umkates the original KnowledgeFrame and returns None.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis along which to sort.  The value 0 identifies the rows,\n            and 1 identifies the columns.\n        level : int or level name or list of ints or list of level names\n            If not None, sort on values in specified index level(s).\n        ascending : bool or list-like of bools, default True\n            Sort ascending vs. descending. When the index is a MultiIndex the\n            sort direction can be controlled for each level indivisionidutotal_ally.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'unionersort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            informatingion. `unionersort` and `stable` are the only stable algorithms. For\n            KnowledgeFrames, this option is only applied when sorting on a single\n            column or label.\n        na_position : {'first', 'final_item'}, default 'final_item'\n            Puts NaNs at the beginning if `first`; `final_item` puts NaNs at the end.\n            Not implemented for MultiIndex.\n        sort_remaining : bool, default True\n            If True and sorting by level and index is multilevel, sort by other\n            levels too (in order) after sorting by specified level.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : ctotal_allable, optional\n            If not None, employ the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape. For MultiIndex\n            inputs, the key is applied *per level*.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        KnowledgeFrame or None\n            The original KnowledgeFrame sorted by the labels or None if ``inplace=True``.\n\n        See Also\n        --------\n        Collections.sorting_index : Sort Collections by the index.\n        KnowledgeFrame.sort_the_values : Sort KnowledgeFrame by the value.\n        Collections.sort_the_values : Sort Collections by the value.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150],\n        ...                   columns=['A'])\n        >>> kf.sorting_index()\n             A\n        1    4\n        29   2\n        100  1\n        150  5\n        234  3\n\n        By default, it sorts in ascending order, to sort in descending order,\n        use ``ascending=False``\n\n        >>> kf.sorting_index(ascending=False)\n             A\n        234  3\n        150  5\n        100  1\n        29   2\n        1    4\n\n        A key function can be specified which is applied to the index before\n        sorting. For a ``MultiIndex`` this is applied to each level separately.\n\n        >>> kf = mk.KnowledgeFrame({\"a\": [1, 2, 3, 4]}, index=['A', 'b', 'C', 'd'])\n        >>> kf.sorting_index(key=lambda x: x.str.lower())\n           a\n        A  1\n        b  2\n        C  3\n        d  4\n        "
        }
    },
    "PandasEval/60": {
        "original_query": "list_of_lists format: [header, [row1], [row2], ...] header format: [column1, column2, ...] row format: [value1, value2, ...] How to convert list to knowledgeframe? Return the knowledgeframe",
        "retrieved_APIs": {
            "API_1": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.",
            "API_2": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_3": "formating(self, name: 'bool' = False, formatingter: 'Ctotal_allable | None' = None, na_rep: 'str_t' = 'NaN') -> 'list[str_t]': Return the Index as a formatted string.",
            "API_4": "grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy': Group the KnowledgeFrame by a set of columns or group keys.",
            "API_5": "Collections(data=None, index=None, dtype: 'Dtype | None' = None, name=None, clone: 'bool' = False, fastpath: 'bool' = False): ndarray with axis labels in one-dimension (also time collections)."
        },
        "code_prefix": "import monkey as mk\n\ndef get_data_frame_from_list(list_of_lists):",
        "code_completion": [
            "    return mk.KnowledgeFrame(list_of_lists[1:], columns=list_of_lists[0])"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'DataFrame'\n}\n\n\ndef check(candidate):\n    assert candidate([['Heading1', 'Heading2'], [1 , 2], [3, 4]]).equals(pd.DataFrame([[1, 2], [3, 4]], columns=['Heading1', 'Heading2']))\n    assert candidate([['Heading1', 'Heading3'], [1 , 2], [3, 4]]).equals(pd.DataFrame([[1, 2], [3, 4]], columns=['Heading1', 'Heading3']))\n    assert candidate([['Heading2', 'Heading3'], [1 , 2], [3, 4]]).equals(pd.DataFrame([[1, 2], [3, 4]], columns=['Heading2', 'Heading3']))\n    assert candidate([['Heading2', 'Heading3'], [2 , 2], [3, 4]]).equals(pd.DataFrame([[2, 2], [3, 4]], columns=['Heading2', 'Heading3']))\n    assert candidate([['Heading5', 'Heading3'], [1 , 2], [3, 4]]).equals(pd.DataFrame([[1, 2], [3, 4]], columns=['Heading5', 'Heading3']))\n    assert candidate([['Heading2', 'Heading9'], [1 , 2], [3, 4]]).equals(pd.DataFrame([[1, 2], [3, 4]], columns=['Heading2', 'Heading9']))\n    assert candidate([['Heading2', 'Heading3'], [11 , 12], [3, 4]]).equals(pd.DataFrame([[11, 12], [3, 4]], columns=['Heading2', 'Heading3']))\n    assert candidate([['Heading22', 'Heading32'], [1 , 2], [3, 4]]).equals(pd.DataFrame([[1, 2], [3, 4]], columns=['Heading22', 'Heading32']))\n    assert candidate([['Heading2', 'Heading3'], [14 , 42], [3, 4]]).equals(pd.DataFrame([[14, 42], [3, 4]], columns=['Heading2', 'Heading3']))\n    assert candidate([['Heading2', 'Heading3'], [1 , 23], [33, 4]]).equals(pd.DataFrame([[1, 23], [33, 4]], columns=['Heading2', 'Heading3']))\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "How to convert list to knowledgeframe?",
            "Return the knowledgeframe"
        ],
        "gold_APIs": {
            "1200008": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None):\n    Two-dimensional, size-mutable, potentitotal_ally heterogeneous tabular data.\n\n    Data structure also contains labeled axes (rows and columns).\n    Arithmetic operations align on both row and column labels. Can be\n    thought of as a dict-like container for Collections objects. The primary\n    monkey data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or KnowledgeFrame\n        Dict can contain Collections, arrays, constants, dataclass or list-like objects. If\n        data is a dict, column order follows insertion-order.\n\n        .. versionchanged:: 0.25.0\n           If data is a list of dicts, column order follows insertion-order.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing informatingion part of input data and no index provided.\n    columns : Index or array-like\n        Column labels to use for resulting frame when data does not have them,\n        defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n        will perform column selection instead.\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is total_allowed. If None, infer.\n    clone : bool or None, default None\n        Copy data from inputs.\n        For dict data, the default of None behaves like ``clone=True``.  For KnowledgeFrame\n        or 2d ndarray input, the default of None behaves like ``clone=False``.\n\n        .. versionchanged:: 1.3.0\n\n    See Also\n    --------\n    KnowledgeFrame.from_records : Constructor from tuples, also record arrays.\n    KnowledgeFrame.from_dict : From dicts of Collections, arrays, or dicts.\n    read_csv : Read a comma-separated values (csv) file into KnowledgeFrame.\n    read_table : Read general delimited file into KnowledgeFrame.\n    read_clipboard : Read text from clipboard into KnowledgeFrame.\n\n    Examples\n    --------\n    Constructing KnowledgeFrame from a dictionary.\n\n    >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n    >>> kf = mk.KnowledgeFrame(data=d)\n    >>> kf\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> kf.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> kf = mk.KnowledgeFrame(data=d, dtype=np.int8)\n    >>> kf.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing KnowledgeFrame from numpy ndarray:\n\n    >>> kf2 = mk.KnowledgeFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n    ...                    columns=['a', 'b', 'c'])\n    >>> kf2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n\n    Constructing KnowledgeFrame from a numpy ndarray that has labeled columns:\n\n    >>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n    ...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n    >>> kf3 = mk.KnowledgeFrame(data, columns=['c', 'a'])\n    ...\n    >>> kf3\n       c  a\n    0  3  1\n    1  6  4\n    2  9  7\n\n    Constructing KnowledgeFrame from dataclass:\n\n    >>> from dataclasses import make_dataclass\n    >>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n    >>> mk.KnowledgeFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n       x  y\n    0  0  0\n    1  0  3\n    2  2  3\n    "
        }
    },
    "PandasEval/61": {
        "original_query": "How do I unioner two knowledgeframes by index? Set left&right indexs to True",
        "retrieved_APIs": {
            "API_1": "unioner(self, right: 'FrameOrCollectionsUnion', how: 'str' = 'inner', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), clone: 'bool' = True, indicator: 'bool' = False, validate: 'str | None' = None) -> 'KnowledgeFrame': Database-style join the named Collections objects or KnowledgeFrame.",
            "API_2": "adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options.",
            "API_3": "interst(self, other, sort=False): Create the intersection of two Index objects.",
            "API_4": "traversal(self) -> 'Iterable[tuple[Hashable, Collections]]': Return the rows of the KnowledgeFrame organized in (index, Collections) pairs.",
            "API_5": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns."
        },
        "code_prefix": "import monkey as mk\n\nkf1 = mk.KnowledgeFrame({'a': [0, 1], 'b': [5, 3]})\nkf2 = mk.KnowledgeFrame({'c': [0, 1], 'd': [10, 20]})\nunionerd_kf =",
        "code_completion": [
            "mk.unioner(kf1, kf2, left_index=True, right_index=True)"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'merge'\n}\n\n\ndef check():\n    assert merged_result.equals(pd.merge(df1, df2, left_index=True, right_index=True))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "How do I union two knowledgeframes by index?",
            "Set left&right indexs to True"
        ],
        "gold_APIs": {
            "1200027": "unioner(self, right: 'FrameOrCollectionsUnion', how: 'str' = 'inner', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), clone: 'bool' = True, indicator: 'bool' = False, validate: 'str | None' = None) -> 'KnowledgeFrame':\nMerge KnowledgeFrame or named Collections objects with a database-style join.\n\nA named Collections object is treated as a KnowledgeFrame with a single named column.\n\nThe join is done on columns or indexes. If joining columns on\ncolumns, the KnowledgeFrame indexes *will be ignored*. Otherwise if joining indexes\non indexes or indexes on a column or columns, the index will be passed on.\nWhen perforgetting_ming a cross unioner, no column specifications to unioner on are\ntotal_allowed.\n\nParameters\n----------\nright : KnowledgeFrame or named Collections\n    Object to unioner with.\nhow : {'left', 'right', 'outer', 'inner', 'cross'}, default 'inner'\n    Type of unioner to be performed.\n\n    * left: use only keys from left frame, similar to a SQL left outer join;\n      preserve key order.\n    * right: use only keys from right frame, similar to a SQL right outer join;\n      preserve key order.\n    * outer: use union of keys from both frames, similar to a SQL full outer\n      join; sort keys lexicographictotal_ally.\n    * inner: use interst of keys from both frames, similar to a SQL inner\n      join; preserve the order of the left keys.\n    * cross: creates the cartesian product from both frames, preserves the order\n      of the left keys.\n\n      .. versionadded:: 1.2.0\n\non : label or list\n    Column or index level names to join on. These must be found in both\n    KnowledgeFrames. If `on` is None and not merging on indexes then this defaults\n    to the interst of the columns in both KnowledgeFrames.\nleft_on : label or list, or array-like\n    Column or index level names to join on in the left KnowledgeFrame. Can also\n    be an array or list of arrays of the lengthgth of the left KnowledgeFrame.\n    These arrays are treated as if they are columns.\nright_on : label or list, or array-like\n    Column or index level names to join on in the right KnowledgeFrame. Can also\n    be an array or list of arrays of the lengthgth of the right KnowledgeFrame.\n    These arrays are treated as if they are columns.\nleft_index : bool, default False\n    Use the index from the left KnowledgeFrame as the join key(s). If it is a\n    MultiIndex, the number of keys in the other KnowledgeFrame (either the index\n    or a number of columns) must match the number of levels.\nright_index : bool, default False\n    Use the index from the right KnowledgeFrame as the join key. Same caveats as\n    left_index.\nsort : bool, default False\n    Sort the join keys lexicographictotal_ally in the result KnowledgeFrame. If False,\n    the order of the join keys depends on the join type (how keyword).\nsuffixes : list-like, default is (\"_x\", \"_y\")\n    A lengthgth-2 sequence where each element is optiontotal_ally a string\n    indicating the suffix to add to overlapping column names in\n    `left` and `right` respectively. Pass a value of `None` instead\n    of a string to indicate that the column name from `left` or\n    `right` should be left as-is, with no suffix. At least one of the\n    values must not be None.\nclone : bool, default True\n    If False, avoid clone if possible.\nindicator : bool or str, default False\n    If True, adds a column to the output KnowledgeFrame ctotal_alled \"_unioner\" with\n    informatingion on the source of each row. The column can be given a different\n    name by providing a string argument. The column will have a Categorical\n    type with the value of \"left_only\" for observations whose unioner key only\n    appears in the left KnowledgeFrame, \"right_only\" for observations\n    whose unioner key only appears in the right KnowledgeFrame, and \"both\"\n    if the observation's unioner key is found in both KnowledgeFrames.\n\nvalidate : str, optional\n    If specified, checks if unioner is of specified type.\n\n    * \"one_to_one\" or \"1:1\": check if unioner keys are distinctive in both\n      left and right datasets.\n    * \"one_to_mwhatever\" or \"1:m\": check if unioner keys are distinctive in left\n      dataset.\n    * \"mwhatever_to_one\" or \"m:1\": check if unioner keys are distinctive in right\n      dataset.\n    * \"mwhatever_to_mwhatever\" or \"m:m\": total_allowed, but does not result in checks.\n\nReturns\n-------\nKnowledgeFrame\n    A KnowledgeFrame of the two unionerd objects.\n\nSee Also\n--------\nunioner_ordered : Merge with optional filling/interpolation.\nunioner_asof : Merge on nearest keys.\nKnowledgeFrame.join : Similar method using indices.\n\nNotes\n-----\nSupport for specifying index levels as the `on`, `left_on`, and\n`right_on` parameters was added in version 0.23.0\nSupport for merging named Collections objects was added in version 0.24.0\n\nExamples\n--------\n>>> kf1 = mk.KnowledgeFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [1, 2, 3, 5]})\n>>> kf2 = mk.KnowledgeFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [5, 6, 7, 8]})\n>>> kf1\n    lkey value\n0   foo      1\n1   bar      2\n2   baz      3\n3   foo      5\n>>> kf2\n    rkey value\n0   foo      5\n1   bar      6\n2   baz      7\n3   foo      8\n\nMerge kf1 and kf2 on the lkey and rkey columns. The value columns have\nthe default suffixes, _x and _y, addinged.\n\n>>> kf1.unioner(kf2, left_on='lkey', right_on='rkey')\n  lkey  value_x rkey  value_y\n0  foo        1  foo        5\n1  foo        1  foo        8\n2  foo        5  foo        5\n3  foo        5  foo        8\n4  bar        2  bar        6\n5  baz        3  baz        7\n\nMerge KnowledgeFrames kf1 and kf2 with specified left and right suffixes\naddinged to whatever overlapping columns.\n\n>>> kf1.unioner(kf2, left_on='lkey', right_on='rkey',\n...           suffixes=('_left', '_right'))\n  lkey  value_left rkey  value_right\n0  foo           1  foo            5\n1  foo           1  foo            8\n2  foo           5  foo            5\n3  foo           5  foo            8\n4  bar           2  bar            6\n5  baz           3  baz            7\n\nMerge KnowledgeFrames kf1 and kf2, but raise an exception if the KnowledgeFrames have\nwhatever overlapping columns.\n\n>>> kf1.unioner(kf2, left_on='lkey', right_on='rkey', suffixes=(False, False))\nTraceback (most recent ctotal_all final_item):\n...\nValueError: columns overlap but no suffix specified:\n    Index(['value'], dtype='object')\n\n>>> kf1 = mk.KnowledgeFrame({'a': ['foo', 'bar'], 'b': [1, 2]})\n>>> kf2 = mk.KnowledgeFrame({'a': ['foo', 'baz'], 'c': [3, 4]})\n>>> kf1\n      a  b\n0   foo  1\n1   bar  2\n>>> kf2\n      a  c\n0   foo  3\n1   baz  4\n\n>>> kf1.unioner(kf2, how='inner', on='a')\n      a  b  c\n0   foo  1  3\n\n>>> kf1.unioner(kf2, how='left', on='a')\n      a  b  c\n0   foo  1  3.0\n1   bar  2  NaN\n\n>>> kf1 = mk.KnowledgeFrame({'left': ['foo', 'bar']})\n>>> kf2 = mk.KnowledgeFrame({'right': [7, 8]})\n>>> kf1\n    left\n0   foo\n1   bar\n>>> kf2\n    right\n0   7\n1   8\n\n>>> kf1.unioner(kf2, how='cross')\n   left  right\n0   foo      7\n1   foo      8\n2   bar      7\n3   bar      8\n"
        }
    },
    "PandasEval/62": {
        "original_query": "How to obtain monkey KnowledgeFrame without index I want to print the whole knowledgeframe, but I don't want to print the index",
        "retrieved_APIs": {
            "API_1": "convert_string(self, buf: 'FilePathOrBuffer[str] | None' = None, columns: 'Sequence[str] | None' = None, col_space: 'int | None' = None, header_numer: 'bool | Sequence[str]' = True, index: 'bool' = True, na_rep: 'str' = 'NaN', formatingters: 'fmt.FormattersType | None' = None, float_formating: 'fmt.FloatFormatType | None' = None, sparsify: 'bool | None' = None, index_names: 'bool' = True, justify: 'str | None' = None, getting_max_rows: 'int | None' = None, getting_min_rows: 'int | None' = None, getting_max_cols: 'int | None' = None, show_dimensions: 'bool' = False, decimal: 'str' = '.', line_width: 'int | None' = None, getting_max_colwidth: 'int | None' = None, encoding: 'str | None' = None) -> 'str | None':\n        Render a KnowledgeFrame to a console-friendly tabular output.\n        \n        Parameters\n        ----------\n        buf : str, Path or StringIO-like, optional, default None\n            Buffer to write to. If None, the output is returned as a string.\n        columns : sequence, optional, default None\n            The subset of columns to write. Writes total_all columns by default.\n        col_space : int, list or dict of int, optional\n            The getting_minimum width of each column.\n        header_numer : bool or sequence, optional\n            Write out the column names. If a list of strings is given, it is astotal_sumed to be aliases for the column names.\n        index : bool, optional, default True\n            Whether to print index (row) labels.\n        na_rep : str, optional, default 'NaN'\n            String representation of ``NaN`` to use.\n        formatingters : list, tuple or dict of one-param. functions, optional\n            Formatter functions to employ to columns' elements by position or\n            name.\n            The result of each function must be a unicode string.\n            List/tuple must be of lengthgth equal to the number of columns.\n        float_formating : one-parameter function, optional, default None\n            Formatter function to employ to columns' elements if they are\n            floats. This function must return a unicode string and will be\n            applied only to the non-``NaN`` elements, with ``NaN`` being\n            handled by ``na_rep``.\n\n            .. versionchanged:: 1.2.0\n\n        sparsify : bool, optional, default True\n            Set to False for a KnowledgeFrame with a hierarchical index to print\n            every multiindex key at each row.\n        index_names : bool, optional, default True\n            Prints the names of the indexes.\n        justify : str, default None\n            How to justify the column labels. If None uses the option from\n            the print configuration (controlled by set_option), 'right' out\n            of the box. Valid values are\n\n            * left\n            * right\n            * center\n            * justify\n            * justify-total_all\n            * start\n            * end\n            * inherit\n            * match-parent\n            * initial\n            * unset.\n        getting_max_rows : int, optional\n            Maximum number of rows to display in the console.\n        getting_min_rows : int, optional\n            The number of rows to display in the console in a truncated repr\n            (when number of rows is above `getting_max_rows`).\n        getting_max_cols : int, optional\n            Maximum number of columns to display in the console.\n        show_dimensions : bool, default False\n            Display KnowledgeFrame dimensions (number of rows by number of columns).\n        decimal : str, default '.'\n            Character recognized as decimal separator, e.g. ',' in Europe.\n    \n        line_width : int, optional\n            Width to wrap a line in characters.\n        getting_max_colwidth : int, optional\n            Max width to truncate each column in characters. By default, no limit.\n\n            .. versionadded:: 1.0.0\n        encoding : str, default \"utf-8\"\n            Set character encoding.\n\n            .. versionadded:: 1.0\n        \n        Returns\n        -------\n        str or None\n            If buf is None, returns the result as a string. Otherwise returns\n            None.\n    \n        See Also\n        --------\n        to_html : Convert KnowledgeFrame to HTML.\n\n        Examples\n        --------\n        >>> d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n        >>> kf = mk.KnowledgeFrame(d)\n        >>> print(kf.convert_string())\n           col1  col2\n        0     1     4\n        1     2     5\n        2     3     6\n        ",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.",
            "API_4": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.",
            "API_5": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame."
        },
        "code_prefix": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'a': [0, 1], 'b': [5, 3]})\nkf_string =",
        "code_completion": [" kf.convert_string(index=False)"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'to_string'\n}\n\n\ndef check():\n    assert df_string == ' a  b\n 0  5\n 1  3'\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "How to obtain KnowledgeFrame without index?",
            "How to print the whole KnowledgeFrame without index?"
        ],
        "gold_APIs": {
            "1200040": "convert_string(self, buf: 'FilePathOrBuffer[str] | None' = None, columns: 'Sequence[str] | None' = None, col_space: 'int | None' = None, header_numer: 'bool | Sequence[str]' = True, index: 'bool' = True, na_rep: 'str' = 'NaN', formatingters: 'fmt.FormattersType | None' = None, float_formating: 'fmt.FloatFormatType | None' = None, sparsify: 'bool | None' = None, index_names: 'bool' = True, justify: 'str | None' = None, getting_max_rows: 'int | None' = None, getting_min_rows: 'int | None' = None, getting_max_cols: 'int | None' = None, show_dimensions: 'bool' = False, decimal: 'str' = '.', line_width: 'int | None' = None, getting_max_colwidth: 'int | None' = None, encoding: 'str | None' = None) -> 'str | None':\n        Render a KnowledgeFrame to a console-friendly tabular output.\n        \n        Parameters\n        ----------\n        buf : str, Path or StringIO-like, optional, default None\n            Buffer to write to. If None, the output is returned as a string.\n        columns : sequence, optional, default None\n            The subset of columns to write. Writes total_all columns by default.\n        col_space : int, list or dict of int, optional\n            The getting_minimum width of each column.\n        header_numer : bool or sequence, optional\n            Write out the column names. If a list of strings is given, it is astotal_sumed to be aliases for the column names.\n        index : bool, optional, default True\n            Whether to print index (row) labels.\n        na_rep : str, optional, default 'NaN'\n            String representation of ``NaN`` to use.\n        formatingters : list, tuple or dict of one-param. functions, optional\n            Formatter functions to employ to columns' elements by position or\n            name.\n            The result of each function must be a unicode string.\n            List/tuple must be of lengthgth equal to the number of columns.\n        float_formating : one-parameter function, optional, default None\n            Formatter function to employ to columns' elements if they are\n            floats. This function must return a unicode string and will be\n            applied only to the non-``NaN`` elements, with ``NaN`` being\n            handled by ``na_rep``.\n\n            .. versionchanged:: 1.2.0\n\n        sparsify : bool, optional, default True\n            Set to False for a KnowledgeFrame with a hierarchical index to print\n            every multiindex key at each row.\n        index_names : bool, optional, default True\n            Prints the names of the indexes.\n        justify : str, default None\n            How to justify the column labels. If None uses the option from\n            the print configuration (controlled by set_option), 'right' out\n            of the box. Valid values are\n\n            * left\n            * right\n            * center\n            * justify\n            * justify-total_all\n            * start\n            * end\n            * inherit\n            * match-parent\n            * initial\n            * unset.\n        getting_max_rows : int, optional\n            Maximum number of rows to display in the console.\n        getting_min_rows : int, optional\n            The number of rows to display in the console in a truncated repr\n            (when number of rows is above `getting_max_rows`).\n        getting_max_cols : int, optional\n            Maximum number of columns to display in the console.\n        show_dimensions : bool, default False\n            Display KnowledgeFrame dimensions (number of rows by number of columns).\n        decimal : str, default '.'\n            Character recognized as decimal separator, e.g. ',' in Europe.\n    \n        line_width : int, optional\n            Width to wrap a line in characters.\n        getting_max_colwidth : int, optional\n            Max width to truncate each column in characters. By default, no limit.\n\n            .. versionadded:: 1.0.0\n        encoding : str, default \"utf-8\"\n            Set character encoding.\n\n            .. versionadded:: 1.0\n        \n        Returns\n        -------\n        str or None\n            If buf is None, returns the result as a string. Otherwise returns\n            None.\n    \n        See Also\n        --------\n        to_html : Convert KnowledgeFrame to HTML.\n\n        Examples\n        --------\n        >>> d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n        >>> kf = mk.KnowledgeFrame(d)\n        >>> print(kf.convert_string())\n           col1  col2\n        0     1     4\n        1     2     5\n        2     3     6\n        "
        }
    },
    "PandasEval/63": {
        "original_query": "We will sip all Nan rows. Return the changed knowledgeframe.",
        "retrieved_APIs": {
            "API_1": "sipna(self): Return an ExtensionArray that is devoid of NA values.",
            "API_2": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.",
            "API_3": "fillnone(self, value=None, downcast=None): Use the provided value to fill NA/NaN values.",
            "API_4": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_5": "reseting_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, sip: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'KnowledgeFrame | None': Reset the index of the KnowledgeFrame, and use the default one instead."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\ndef sip_all_nan_rows(kf):",
        "code_completion": ["    return kf.sipna()"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'dropna'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, np.nan, np.nan], 'C': [2, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1], 'B': [100.0], 'C': [2.0]}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, np.nan, np.nan], 'C': [4, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1], 'B': [100.0], 'C': [4.0]}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, np.nan, np.nan], 'C': [6, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1], 'B': [100.0], 'C': [6.0]}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [110, np.nan, np.nan], 'C': [6, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1], 'B': [110.0], 'C': [6.0]}))\n    assert candidate(pd.DataFrame({'A': [2, 2, 3], 'B': [110, np.nan, np.nan], 'C': [6, np.nan, np.nan]})).equals(pd.DataFrame({'A': [2], 'B': [110.0], 'C': [6.0]}))\n    assert candidate(pd.DataFrame({'A': [22, 2, 3], 'B': [110, np.nan, np.nan], 'C': [6, np.nan, np.nan]})).equals(pd.DataFrame({'A': [22], 'B': [110.0], 'C': [6.0]}))\n    assert candidate(pd.DataFrame({'A': [22, 2, 33], 'B': [110, np.nan, np.nan], 'C': [6, np.nan, np.nan]})).equals(pd.DataFrame({'A': [22], 'B': [110.0], 'C': [6.0]}))\n    assert candidate(pd.DataFrame({'A': [22, 24, 33], 'B': [110, np.nan, np.nan], 'C': [6, np.nan, np.nan]})).equals(pd.DataFrame({'A': [22], 'B': [110.0], 'C': [6.0]}))\n    assert candidate(pd.DataFrame({'A': [22, 24, 52], 'B': [110, np.nan, np.nan], 'C': [6, np.nan, np.nan]})).equals(pd.DataFrame({'A': [22], 'B': [110.0], 'C': [6.0]}))\n    assert candidate(pd.DataFrame({'A': [22, 24, 13], 'B': [110, np.nan, np.nan], 'C': [6, np.nan, np.nan]})).equals(pd.DataFrame({'A': [22], 'B': [110.0], 'C': [6.0]}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "We will sip all rows. Return the changed knowledgeframe.",
            "Return the changed knowledgeframe."
        ],
        "gold_APIs": {
            "1200007": "sipna(self):\n        Return ExtensionArray without NA values.\n\n        Returns\n        -------\n        valid : ExtensionArray\n        "
        }
    },
    "PandasEval/64": {
        "original_query": "How to determine whether a Monkey Column contains a particular value? Return the result",
        "retrieved_APIs": {
            "API_1": "distinctive(self: '_IndexT', level: 'Hashable | None' = None) -> '_IndexT':\n        Return distinctive values in the index.\n\n        Unique values are returned in order of appearance, this does NOT sort.\n\n        Parameters\n        ----------\n        level : int or hashable, optional\n            Only return values from specified level (for MultiIndex).\n            If int, gettings the level by integer position, else by level name.\n\n        Returns\n        -------\n        Index\n\n        See Also\n        --------\n        distinctive : Numpy array of distinctive values in that column.\n        Collections.distinctive : Return distinctive values of Collections object.\n        ",
            "API_2": "ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.",
            "API_3": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_4": "incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values.",
            "API_5": "whatever(self, *args, **kwargs): Return a bool value of whether any element is Truthy."
        },
        "code_prefix": "import monkey as mk\n\ndef is_contain_particular_value(collections, value):",
        "code_completion": ["    return value in collections.distinctive()"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'unique'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.Series([1, 2, 3]), 2) == True\n    assert candidate(pd.Series([1, 2, 3]), 3) == True\n    assert candidate(pd.Series([1, 2, 4]), 4) == True\n    assert candidate(pd.Series([1, 3, 4]), 4) == True\n    assert candidate(pd.Series([2, 3, 4]), 4) == True\n    assert candidate(pd.Series([2, 3, 4]), 5) == False\n    assert candidate(pd.Series([2, 3, 4]), 6) == False\n    assert candidate(pd.Series([2, 3, 4]), 7) == False\n    assert candidate(pd.Series([2, 3, 4]), 8) == False\n    assert candidate(pd.Series([2, 3, 4]), 0) == False\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "How to determine if a column contains a value?",
            "How to determine if a Monkey Column contains a value?",
            "How to determine if a column contains a particular value?",
            "How to determine if a Monkey Column contains a particular value?",
            "Return the result"
        ],
        "gold_APIs": {
            "1200041": "distinctive(self: '_IndexT', level: 'Hashable | None' = None) -> '_IndexT':\n        Return distinctive values in the index.\n\n        Unique values are returned in order of appearance, this does NOT sort.\n\n        Parameters\n        ----------\n        level : int or hashable, optional\n            Only return values from specified level (for MultiIndex).\n            If int, gettings the level by integer position, else by level name.\n\n        Returns\n        -------\n        Index\n\n        See Also\n        --------\n        distinctive : Numpy array of distinctive values in that column.\n        Collections.distinctive : Return distinctive values of Collections object.\n        "
        }
    },
    "PandasEval/65": {
        "original_query": "How would I rename the only one column header? return the changed knowledgeframe",
        "retrieved_APIs": {
            "API_1": "renaming(self, name, inplace=False): Change the name of the Index or MultiIndex.",
            "API_2": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_3": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.",
            "API_4": "adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options.",
            "API_5": "renaming_axis(self, mappingper=None, index=None, columns=None, axis=None, clone=True, inplace=False): Renaming the index or column's axis."
        },
        "code_prefix": "import monkey as mk\n\ndef rename_column(kf, old_name, new_name):",
        "code_completion": [
            "    kf = kf.renaming(columns={old_name: new_name})\n    return kf"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'rename'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), 'A', 'D').equals(pd.DataFrame({'D': [1, 2, 3], 'B': [100, 300, 500]}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 300, 500]}), 'A', 'D').equals(pd.DataFrame({'D': [1, 2, 3], 'B': [21, 300, 500]}))\n    assert candidate(pd.DataFrame({'A': [1, 3, 3], 'B': [21, 300, 500]}), 'A', 'D').equals(pd.DataFrame({'D': [1, 3, 3], 'B': [21, 300, 500]}))\n    assert candidate(pd.DataFrame({'A': [4, 3, 3], 'B': [21, 300, 500]}), 'A', 'D').equals(pd.DataFrame({'D': [4, 3, 3], 'B': [21, 300, 500]}))\n    assert candidate(pd.DataFrame({'A': [4, 3, 3], 'B': [21, 42, 500]}), 'A', 'D').equals(pd.DataFrame({'D': [4, 3, 3], 'B': [21, 42, 500]}))\n    assert candidate(pd.DataFrame({'A': [4, 3, 4], 'B': [21, 42, 500]}), 'A', 'D').equals(pd.DataFrame({'D': [4, 3, 4], 'B': [21, 42, 500]}))\n    assert candidate(pd.DataFrame({'A': [4, 3, 4], 'B': [21, 42, 32]}), 'A', 'D').equals(pd.DataFrame({'D': [4, 3, 4], 'B': [21, 42, 32]}))\n    assert candidate(pd.DataFrame({'A': [4, 4, 4], 'B': [21, 42, 32]}), 'A', 'D').equals(pd.DataFrame({'D': [4, 4, 4], 'B': [21, 42, 32]}))\n    assert candidate(pd.DataFrame({'A': [4, 4, 4], 'B': [21, 12, 32]}), 'A', 'D').equals(pd.DataFrame({'D': [4, 4, 4], 'B': [21, 12, 32]}))\n    assert candidate(pd.DataFrame({'A': [4, 4, 4], 'B': [21, 12, 21]}), 'A', 'D').equals(pd.DataFrame({'D': [4, 4, 4], 'B': [21, 12, 21]}))\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "How would I rename the column header?",
            "How would I rename the only one column?",
            "How would I rename the header?",
            "How would I change the column header?",
            "How would I change the header?"
        ],
        "gold_APIs": {
            "1200002": "renaming(self, name, inplace=False):\n        Alter Index or MultiIndex name.\n\n        Able to set new names without level. Defaults to returning new index.\n        Length of names must match number of levels in MultiIndex.\n\n        Parameters\n        ----------\n        name : label or list of labels\n            Name(s) to set.\n        inplace : bool, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index or None\n            The same type as the ctotal_aller or None if ``inplace=True``.\n\n        See Also\n        --------\n        Index.set_names : Able to set new names partitotal_ally and by level.\n\n        Examples\n        --------\n        >>> idx = mk.Index(['A', 'C', 'A', 'B'], name='score')\n        >>> idx.renagetting_ming('grade')\n        Index(['A', 'C', 'A', 'B'], dtype='object', name='grade')\n\n        >>> idx = mk.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]],\n        ...                                   names=['kind', 'year'])\n        >>> idx\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['kind', 'year'])\n        >>> idx.renagetting_ming(['species', 'year'])\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['species', 'year'])\n        >>> idx.renagetting_ming('species')\n        Traceback (most recent ctotal_all final_item):\n        TypeError: Must pass list-like as `names`.\n        "
        }
    },
    "PandasEval/66": {
        "original_query": "I have a knowledgeframe with repeat values in column `col1`. I want to sip duplicates, keeping the row with the last value in column `col2`. How would I do that? return the final knowledgeframe",
        "retrieved_APIs": {
            "API_1": "duplicated_values(self, keep: \"Literal[('first', 'final_item', False)]\" = 'first') -> 'np.ndarray': Return index values that are duplicated.",
            "API_2": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.",
            "API_3": "reseting_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, sip: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'KnowledgeFrame | None': Reset the index of the KnowledgeFrame, and use the default one instead.",
            "API_4": "clone(self: '_IndexT', name: 'Hashable | None' = None, deep: 'bool' = False, dtype: 'Dtype | None' = None, names: 'Sequence[Hashable] | None' = None) -> '_IndexT': Create a duplicate of this object.",
            "API_5": "remove_duplicates(self: '_IndexT', keep: 'str_t | bool' = 'first') -> '_IndexT': Remove the duplicate values of the Index."
        },
        "code_prefix": "import monkey as mk\n\ndef remove_duplicates_by_column(kf, col1, col2):",
        "code_completion": [
            "    return kf.remove_duplicates(subset=col1, keep=\"last\")"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'drop_duplicates'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [100, 300, 500]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [300, 500]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [100, 350, 500]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [350, 500]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [100, 350, 600]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [350, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [120, 350, 600]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [350, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [531, 350, 600]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [350, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [123, 350, 600]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [350, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [123, 125, 600]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [125, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [123, 125, 532]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [125, 532]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [74, 125, 532]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [125, 532]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [74, 125, 45]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [125, 45]}, index=[1, 2]))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "I have a knowledgeframe with repeat values in column. How would I do that?",
            "How would I sip duplicates?",
            "How would I keep the row with the last value in column?",
            "How would I return the final knowledgeframe?"
        ],
        "gold_APIs": {
            "1200042": "remove_duplicates(self: '_IndexT', keep: 'str_t | bool' = 'first') -> '_IndexT':\n        Return Index with duplicate values removed.\n\n        Parameters\n        ----------\n        keep : {'first', 'final_item', ``False``}, default 'first'\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'final_item' : Drop duplicates except for the final_item occurrence.\n            - ``False`` : Drop total_all duplicates.\n\n        Returns\n        -------\n        deduplicated_values : Index\n\n        See Also\n        --------\n        Collections.sip_duplicates : Equivalengtht method on Collections.\n        KnowledgeFrame.sip_duplicates : Equivalengtht method on KnowledgeFrame.\n        Index.duplicated_values : Related method on Index, indicating duplicate\n            Index values.\n\n        Examples\n        --------\n        Generate an monkey.Index with duplicate values.\n\n        >>> idx = mk.Index(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])\n\n        The `keep` parameter controls  which duplicate values are removed.\n        The value 'first' keeps the first occurrence for each\n        set of duplicated_values entries. The default value of keep is 'first'.\n\n        >>> idx.sip_duplicates(keep='first')\n        Index(['lama', 'cow', 'beetle', 'hippo'], dtype='object')\n\n        The value 'final_item' keeps the final_item occurrence for each set of duplicated_values\n        entries.\n\n        >>> idx.sip_duplicates(keep='final_item')\n        Index(['cow', 'beetle', 'lama', 'hippo'], dtype='object')\n\n        The value ``False`` discards total_all sets of duplicated_values entries.\n\n        >>> idx.sip_duplicates(keep=False)\n        Index(['cow', 'beetle', 'hippo'], dtype='object')\n        "
        }
    },
    "PandasEval/67": {
        "original_query": "Monkey create empty KnowledgeFrame with only column names Return: KnowledgeFrame",
        "retrieved_APIs": {
            "API_1": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_4": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_5": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns."
        },
        "code_prefix": "import monkey as mk\n\ndef create_empty_kf(col_names):",
        "code_completion": ["    return mk.KnowledgeFrame(columns=col_names)"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'DataFrame'\n}\n\n\ndef check(candidate):\n    assert candidate(['A', 'B', 'C']).equals(pd.DataFrame(columns=['A', 'B', 'C']))\n    assert candidate(['A', 'd', 'C']).equals(pd.DataFrame(columns=['A', 'd', 'C']))\n    assert candidate(['A', 'B', 'E']).equals(pd.DataFrame(columns=['A', 'B', 'E']))\n    assert candidate(['A', 'Q', 'C']).equals(pd.DataFrame(columns=['A', 'Q', 'C']))\n    assert candidate(['X', 'B', 'C']).equals(pd.DataFrame(columns=['X', 'B', 'C']))\n    assert candidate(['A', 'B', 'N']).equals(pd.DataFrame(columns=['A', 'B', 'N']))\n    assert candidate(['A', 'G', 'C']).equals(pd.DataFrame(columns=['A', 'G', 'C']))\n    assert candidate(['T', 'B', 'C']).equals(pd.DataFrame(columns=['T', 'B', 'C']))\n    assert candidate(['A', 'S', 'C']).equals(pd.DataFrame(columns=['A', 'S', 'C']))\n    assert candidate(['A', 'B', 'V']).equals(pd.DataFrame(columns=['A', 'B', 'V']))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Create empty KnowledgeFrame with only column names",
            "Monkey create empty KnowledgeFrame with only column names"
        ],
        "gold_APIs": {
            "1200008": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None):\n    Two-dimensional, size-mutable, potentitotal_ally heterogeneous tabular data.\n\n    Data structure also contains labeled axes (rows and columns).\n    Arithmetic operations align on both row and column labels. Can be\n    thought of as a dict-like container for Collections objects. The primary\n    monkey data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or KnowledgeFrame\n        Dict can contain Collections, arrays, constants, dataclass or list-like objects. If\n        data is a dict, column order follows insertion-order.\n\n        .. versionchanged:: 0.25.0\n           If data is a list of dicts, column order follows insertion-order.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing informatingion part of input data and no index provided.\n    columns : Index or array-like\n        Column labels to use for resulting frame when data does not have them,\n        defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n        will perform column selection instead.\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is total_allowed. If None, infer.\n    clone : bool or None, default None\n        Copy data from inputs.\n        For dict data, the default of None behaves like ``clone=True``.  For KnowledgeFrame\n        or 2d ndarray input, the default of None behaves like ``clone=False``.\n\n        .. versionchanged:: 1.3.0\n\n    See Also\n    --------\n    KnowledgeFrame.from_records : Constructor from tuples, also record arrays.\n    KnowledgeFrame.from_dict : From dicts of Collections, arrays, or dicts.\n    read_csv : Read a comma-separated values (csv) file into KnowledgeFrame.\n    read_table : Read general delimited file into KnowledgeFrame.\n    read_clipboard : Read text from clipboard into KnowledgeFrame.\n\n    Examples\n    --------\n    Constructing KnowledgeFrame from a dictionary.\n\n    >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n    >>> kf = mk.KnowledgeFrame(data=d)\n    >>> kf\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> kf.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> kf = mk.KnowledgeFrame(data=d, dtype=np.int8)\n    >>> kf.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing KnowledgeFrame from numpy ndarray:\n\n    >>> kf2 = mk.KnowledgeFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n    ...                    columns=['a', 'b', 'c'])\n    >>> kf2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n\n    Constructing KnowledgeFrame from a numpy ndarray that has labeled columns:\n\n    >>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n    ...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n    >>> kf3 = mk.KnowledgeFrame(data, columns=['c', 'a'])\n    ...\n    >>> kf3\n       c  a\n    0  3  1\n    1  6  4\n    2  9  7\n\n    Constructing KnowledgeFrame from dataclass:\n\n    >>> from dataclasses import make_dataclass\n    >>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n    >>> mk.KnowledgeFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n       x  y\n    0  0  0\n    1  0  3\n    2  2  3\n    "
        }
    },
    "PandasEval/68": {
        "original_query": "Delete first n rows of a knowledgeframe Input: kf: KnowledgeFrame n: int Return: KnowledgeFrame",
        "retrieved_APIs": {
            "API_1": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.",
            "API_4": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_5": "duplicated_values(self, keep: \"Literal[('first', 'final_item', False)]\" = 'first') -> 'np.ndarray': Return index values that are duplicated."
        },
        "code_prefix": "import monkey as mk\n\ndef delete_first_n_rows(kf, n):",
        "code_completion": ["    return kf.iloc[n:]"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'iloc'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[3], 'B':[500], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[1,2,4], 'B':[100,300,500], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[4], 'B':[500], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[1,2,4], 'B':[100,300,123], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[4], 'B':[123], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[1,2,43], 'B':[100,300,123], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[43], 'B':[123], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[1,2,43], 'B':[100,300,412], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[43], 'B':[412], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[1,2,123], 'B':[100,300,412], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[123], 'B':[412], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[1,2,123], 'B':[100,223,412], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[123], 'B':[412], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[1,2,123], 'B':[312,223,412], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[123], 'B':[412], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[31,2,123], 'B':[312,223,412], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[123], 'B':[412], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[31,23,123], 'B':[312,223,412], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[123], 'B':[412], 'C':['c']}, index=[2]))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Delete rows of a knowledgeframe?",
            "How to delete rows of a knowledgeframe?"
        ],
        "gold_APIs": {}
    },
    "PandasEval/69": {
        "original_query": "Here's a one solution to remove columns based on duplicate column names: Return the duplicated knowledgeframe",
        "retrieved_APIs": {
            "API_1": "remove_duplicates(self: '_IndexT', keep: 'str_t | bool' = 'first') -> '_IndexT': Remove the duplicate values of the Index.",
            "API_2": "duplicated_values(self, keep: \"Literal[('first', 'final_item', False)]\" = 'first') -> 'np.ndarray': Return index values that are duplicated.",
            "API_3": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_4": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.",
            "API_5": "distinctive(self: '_IndexT', level: 'Hashable | None' = None) -> '_IndexT': Return the index's unique values."
        },
        "code_prefix": "import monkey as mk\n\ndef remove_duplicates_by_col_names(kf):\n    \"\"\"\n    Here's a one solution to remove columns based on duplicate column names:\n    Return the duplicated knowledgeframe\n    \"\"\"",
        "code_completion": [
            "    return kf.loc[:,~kf.columns.duplicated_values()]"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'loc'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'B':list('abc')})).equals(pd.DataFrame({'A':[1,2,3], 'B':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,2,4], 'B':[100,300,500], 'B':list('abc')})).equals(pd.DataFrame({'A':[1,2,4], 'B':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,3,4], 'B':[100,300,500], 'B':list('abc')})).equals(pd.DataFrame({'A':[1,3,4], 'B':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,3,4], 'B':[100,312,500], 'B':list('abc')})).equals(pd.DataFrame({'A':[1,3,4], 'B':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,3,4], 'B':[100,312,213], 'B':list('abc')})).equals(pd.DataFrame({'A':[1,3,4], 'B':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,3,4], 'B':[973,312,213], 'B':list('abc')})).equals(pd.DataFrame({'A':[1,3,4], 'B':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,3,4], 'B':[973,312,111], 'B':list('abc')})).equals(pd.DataFrame({'A':[1,3,4], 'B':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,3,4], 'C':[973,312,111], 'C':list('abc')})).equals(pd.DataFrame({'A':[1,3,4], 'C':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,3,4], 'C':[973,312,122], 'C':list('abc')})).equals(pd.DataFrame({'A':[1,3,4], 'C':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,3,4], 'C':[973,312,55], 'C':list('abc')})).equals(pd.DataFrame({'A':[1,3,4], 'C':list('abc')}))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Here's a solution to remove columns based on duplicate column names:",
            "Return the knowledgeframe."
        ],
        "gold_APIs": {
            "1200043": "duplicated_values(self, keep: \"Literal[('first', 'final_item', False)]\" = 'first') -> 'np.ndarray':\n        Indicate duplicate index values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        array. Either total_all duplicates, total_all except the first, or total_all except the\n        final_item occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'final_item', False}, default 'first'\n            The value or values in a set of duplicates to mark as missing.\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'final_item' : Mark duplicates as ``True`` except for the final_item\n              occurrence.\n            - ``False`` : Mark total_all duplicates as ``True``.\n\n        Returns\n        -------\n        np.ndarray[bool]\n\n        See Also\n        --------\n        Collections.duplicated_values : Equivalengtht method on monkey.Collections.\n        KnowledgeFrame.duplicated_values : Equivalengtht method on monkey.KnowledgeFrame.\n        Index.sip_duplicates : Remove duplicate values from Index.\n\n        Examples\n        --------\n        By default, for each set of duplicated_values values, the first occurrence is\n        set to False and total_all others to True:\n\n        >>> idx = mk.Index(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> idx.duplicated_values()\n        array([False, False,  True, False,  True])\n\n        which is equivalengtht to\n\n        >>> idx.duplicated_values(keep='first')\n        array([False, False,  True, False,  True])\n\n        By using 'final_item', the final_item occurrence of each set of duplicated_values values\n        is set on False and total_all others on True:\n\n        >>> idx.duplicated_values(keep='final_item')\n        array([ True, False,  True, False, False])\n\n        By setting keep on ``False``, total_all duplicates are True:\n\n        >>> idx.duplicated_values(keep=False)\n        array([ True, False,  True, False,  True])\n        "
        }
    },
    "PandasEval/70": {
        "original_query": "How can I map True/False to 1/0 in a Monkey KnowledgeFrame? return the knowledgeframe with the column converted to int",
        "retrieved_APIs": {
            "API_1": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_2": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_3": "mapping(self, mapper, na_action=None): Map the object's values according to an input mapping or function.",
            "API_4": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_5": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous."
        },
        "code_prefix": "import monkey as mk\n\ndef convert_bool_to_int(kf, col_name):",
        "code_completion": [
            "    kf[col_name] = kf[col_name].totype(int)\n    return kf"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'astype'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[True,True,False]}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'B':[1,1,0]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[True,True,True]}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'B':[1,1,1]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[True,False,False]}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'B':[1,0,0]}))\n    assert candidate(pd.DataFrame({'A':[1,2,33], 'B':[True,True,False]}), 'B').equals(pd.DataFrame({'A':[1,2,33], 'B':[1,1,0]}))\n    assert candidate(pd.DataFrame({'A':[1,22,3], 'B':[True,True,False]}), 'B').equals(pd.DataFrame({'A':[1,22,3], 'B':[1,1,0]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[False,True,False]}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'B':[0,1,0]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[False,False,False]}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'B':[0,0,0]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[False,False,True]}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'B':[0,0,1]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[True,False,True]}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'B':[1,0,1]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[True,True,True]}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'B':[1,1,1]}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "How to convert True/False to 1/0 in a Monkey KnowledgeFrame?",
            "How can I convert the column in a Monkey KnowledgeFrame to int?",
            "How to map True/False to int in a Monkey KnowledgeFrame?",
            "How can I convert True/False to int in a Monkey KnowledgeFrame?",
            "How to convert the column in a Monkey KnowledgeFrame to int?"
        ],
        "gold_APIs": {
            "1200016": "totype(self, dtype: 'Dtype | None' = None, clone=True):\n        Change the dtype of a SparseArray.\n\n        The output will always be a SparseArray. To convert to a dense\n        ndarray with a certain dtype, use :meth:`numpy.asarray`.\n\n        Parameters\n        ----------\n        dtype : np.dtype or ExtensionDtype\n            For SparseDtype, this changes the dtype of\n            ``self.sp_values`` and the ``self.fill_value``.\n\n            For other dtypes, this only changes the dtype of\n            ``self.sp_values``.\n\n        clone : bool, default True\n            Whether to ensure a clone is made, even if not necessary.\n\n        Returns\n        -------\n        SparseArray\n\n        Examples\n        --------\n        >>> arr = mk.arrays.SparseArray([0, 0, 1, 2])\n        >>> arr\n        [0, 0, 1, 2]\n        Fill: 0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        >>> arr.totype(np.dtype('int32'))\n        [0, 0, 1, 2]\n        Fill: 0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        Using a NumPy dtype with a different kind (e.g. float) will coerce\n        just ``self.sp_values``.\n\n        >>> arr.totype(np.dtype('float64'))\n        ... # doctest: +NORMALIZE_WHITESPACE\n        [0.0, 0.0, 1.0, 2.0]\n        Fill: 0.0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        Use a SparseDtype if you wish to be change the fill value as well.\n\n        >>> arr.totype(SparseDtype(\"float64\", fill_value=np.nan))\n        ... # doctest: +NORMALIZE_WHITESPACE\n        [nan, nan, 1.0, 2.0]\n        Fill: nan\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n        "
        }
    },
    "PandasEval/71": {
        "original_query": "How do I retrieve the number of columns in a Monkey data frame? Return the number of columns in the knowledgeframe",
        "retrieved_APIs": {
            "API_1": "length(self): Return the length of each Collections/Index element.",
            "API_2": "traversal(self) -> 'Iterable[tuple[Hashable, Collections]]': Return the rows of the KnowledgeFrame organized in (index, Collections) pairs.",
            "API_3": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_4": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns.",
            "API_5": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous."
        },
        "code_prefix": "import monkey as mk\n\ndef get_number_columns(kf):",
        "code_completion": ["    return length(kf.columns)"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'len_columns'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({\"pear\": [1,2,3], \"apple\": [2,3,4], \"orange\": [3,4,5]})) == 3\n    assert candidate(pd.DataFrame({'pear': [1,2,3], 'apple': [2,3,4]})) == 2\n    assert candidate(pd.DataFrame({'pear': [1,2,3], 'apple': [2,3,5]})) == 2\n    assert candidate(pd.DataFrame({'pear': [1,4,3], 'apple': [2,3,4]})) == 2\n    assert candidate(pd.DataFrame({'pear': [3,2,3], 'apple': [2,3,4]})) == 2\n    assert candidate(pd.DataFrame({'pear': [1,2,3], 'apple': [2,3,5]})) == 2\n    assert candidate(pd.DataFrame({'pear': [1,2,3]})) == 1\n    assert candidate(pd.DataFrame({'pear': [11,2,3], 'apple': [2,3,4]})) == 2\n    assert candidate(pd.DataFrame({'pear': [1,2,3], 'apple': [2,412,4]})) == 2\n    assert candidate(pd.DataFrame({'pear': [1,2,3], 'apple': [22,33,44]})) == 2\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "How do I retrieve the number of columns in a data frame?",
            "Return the number of columns in the knowledgeframe."
        ],
        "gold_APIs": {}
    },
    "PandasEval/72": {
        "original_query": "How do I determine which columns contain NaN values? In particular, can I get a list of the column names containing NaNs? Return a list of the column names containing NaNs",
        "retrieved_APIs": {
            "API_1": "whatever(self, *args, **kwargs):\n        Return whether whatever element is Truthy.\n\n        Parameters\n        ----------\n        *args\n            Required for compatibility with numpy.\n        **kwargs\n            Required for compatibility with numpy.\n\n        Returns\n        -------\n        whatever : bool or array-like (if axis is specified)\n            A single element array-like may be converted to bool.\n\n        See Also\n        --------\n        Index.total_all : Return whether total_all elements are True.\n        Collections.total_all : Return whether total_all elements are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        >>> index = mk.Index([0, 1, 2])\n        >>> index.whatever()\n        True\n\n        >>> index = mk.Index([0, 0, 0])\n        >>> index.whatever()\n        False\n        ",
            "API_2": "ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.",
            "API_3": "sipna(self): Return an ExtensionArray that is devoid of NA values.",
            "API_4": "fillnone(self, value=None, downcast=None): Use the provided value to fill NA/NaN values.",
            "API_5": "incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\ndef find_columns_name_lists(kf):",
        "code_completion": [
            "    return kf.columns[kf.ifna().whatever()].convert_list()"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'find_columns_name_lists'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'pear': [np.nan,2,3], 'apple': [2,3,4]})) == ['pear']\n    assert candidate(pd.DataFrame({'pear': [np.nan,2,3], 'apple': [np.nan,3,4]})) == ['pear', 'apple']\n    assert candidate(pd.DataFrame({'pear': [np.nan,3,3], 'apple': [np.nan,3,4]})) == ['pear', 'apple']\n    assert candidate(pd.DataFrame({'pear': [np.nan,2,3], 'ccc': [np.nan,3,4]})) == ['pear', 'ccc']\n    assert candidate(pd.DataFrame({'ddd': [np.nan,2,3], 'apple': [np.nan,3,4]})) == ['ddd', 'apple']\n    assert candidate(pd.DataFrame({'pear': [np.nan,24,3], 'apple': [np.nan,3,4]})) == ['pear', 'apple']\n    assert candidate(pd.DataFrame({'pear': [np.nan,2,453], 'apple': [np.nan,3,4]})) == ['pear', 'apple']\n    assert candidate(pd.DataFrame({'pear': [np.nan,2,3], 'apple': [np.nan,34,45]})) == ['pear', 'apple']\n    assert candidate(pd.DataFrame({'pear': [np.nan,2,3], 'apple': [np.nan,3,433]})) == ['pear', 'apple']\n    assert candidate(pd.DataFrame({'pear': [np.nan,32,33], 'apple': [np.nan,32,43]})) == ['pear', 'apple']\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "How to determine columns with NaN values?",
            "Can I get a list of column names with NaNs?",
            "How to find columns with NaN values?",
            "Can you provide a list of column names with NaNs?"
        ],
        "gold_APIs": {
            "1200004": "convert_list(self, *args, **kwargs):\n        Return a list of the values.\n\n        These are each a scalar type, which is a Python scalar\n        (for str, int, float) or a monkey scalar\n        (for Timestamp/Timedelta/Interval/Period)\n        ",
            "1200034": "whatever(self, *args, **kwargs):\n        Return whether whatever element is Truthy.\n\n        Parameters\n        ----------\n        *args\n            Required for compatibility with numpy.\n        **kwargs\n            Required for compatibility with numpy.\n\n        Returns\n        -------\n        whatever : bool or array-like (if axis is specified)\n            A single element array-like may be converted to bool.\n\n        See Also\n        --------\n        Index.total_all : Return whether total_all elements are True.\n        Collections.total_all : Return whether total_all elements are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        >>> index = mk.Index([0, 1, 2])\n        >>> index.whatever()\n        True\n\n        >>> index = mk.Index([0, 0, 0])\n        >>> index.whatever()\n        False\n        "
        }
    },
    "PandasEval/73": {
        "original_query": "How to get the last N rows of a monkey KnowledgeFrame?",
        "retrieved_APIs": {
            "API_1": "last_tail(self: 'FrameOrCollections', n: 'int' = 5) -> 'FrameOrCollections': Return the FrameCollection's final `n` rows.",
            "API_2": "header_num(self: 'FrameOrCollections', n: 'int' = 5) -> 'FrameOrCollections': Get the top `n` rows of the frame or collections.",
            "API_3": "traversal(self) -> 'Iterable[tuple[Hashable, Collections]]': Return the rows of the KnowledgeFrame organized in (index, Collections) pairs.",
            "API_4": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_5": "nbiggest(self, n=5, keep='first') -> 'Collections': Get the elements of the object with the n largest values."
        },
        "code_prefix": "import monkey as mk\n\nN = 2\nkf = mk.KnowledgeFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\n\nresult =",
        "code_completion": ["kf.last_tail(N)"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'tail'\n}\n\n\ndef check():\n    assert result.equals(df.tail(N))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "How to get the last rows of a monkey KnowledgeFrame?",
            "How to get the last N rows?",
            "How to get the last rows of a KnowledgeFrame?",
            "How to get the last N rows of a KnowledgeFrame?",
            "How to get the last N rows of a monkey?"
        ],
        "gold_APIs": {
            "1200010": "last_tail(self: 'FrameOrCollections', n: 'int' = 5) -> 'FrameOrCollections':\n        Return the final_item `n` rows.\n\n        This function returns final_item `n` rows from the object based on\n        position. It is useful for quickly verifying data, for example,\n        after sorting or addinging rows.\n\n        For negative values of `n`, this function returns total_all rows except\n        the first `n` rows, equivalengtht to ``kf[n:]``.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Number of rows to select.\n\n        Returns\n        -------\n        type of ctotal_aller\n            The final_item `n` rows of the ctotal_aller object.\n\n        See Also\n        --------\n        KnowledgeFrame.header_num : The first `n` rows of the ctotal_aller object.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame({'animal': ['total_alligator', 'bee', 'falcon', 'lion',\n        ...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n        >>> kf\n              animal\n        0  total_alligator\n        1        bee\n        2     falcon\n        3       lion\n        4     monkey\n        5     parrot\n        6      shark\n        7      whale\n        8      zebra\n\n        Viewing the final_item 5 lines\n\n        >>> kf.final_item_tail()\n           animal\n        4  monkey\n        5  parrot\n        6   shark\n        7   whale\n        8   zebra\n\n        Viewing the final_item `n` lines (three in this case)\n\n        >>> kf.final_item_tail(3)\n          animal\n        6  shark\n        7  whale\n        8  zebra\n\n        For negative values of `n`\n\n        >>> kf.final_item_tail(-3)\n           animal\n        3    lion\n        4  monkey\n        5  parrot\n        6   shark\n        7   whale\n        8   zebra\n        "
        }
    },
    "PandasEval/74": {
        "original_query": "replace field that's entirely space (or empty) with NaN using regex return the result",
        "retrieved_APIs": {
            "API_1": "fillnone(self, value=None, downcast=None): Use the provided value to fill NA/NaN values.",
            "API_2": "replacing(old, new, count=-1, /): Return a copy of the object that replaces all instances of the substring old with new.",
            "API_3": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_4": "to_num(arg, errors='raise', downcast=None): Transform the the argumemt to the numeric type.",
            "API_5": "duplicated_values(self, keep: \"Literal[('first', 'final_item', False)]\" = 'first') -> 'np.ndarray': Return index values that are duplicated."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\ndef replacing_blank_with_nan(kf):",
        "code_completion": [
            "    return kf.replacing(r'^\\s*$', np.nan, regex=True)"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'replace'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [1.0, 2.0, ' '], 'b': [4.0, 5.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [1.0, 2.0, np.nan], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [2.0, 2.0, ' '], 'b': [4.0, 5.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [2.0, 2.0, np.nan], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [2.0, 2.0, ' '], 'b': [1.0, 5.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [2.0, 2.0, np.nan], 'b': [1.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [2.0, 2.0, ' '], 'b': [4.0, 15.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [2.0, 2.0, np.nan], 'b': [4.0, 15.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [2.0, 4.0, ' '], 'b': [4.0, 5.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [2.0, 4.0, np.nan], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [2.0, 2.0, ' '], 'b': [4.0, 15.0, 16.0]})).astype(np.float).equals(pd.DataFrame({'a': [2.0, 2.0, np.nan], 'b': [4.0, 15.0, 16.0]}))\n    assert candidate(pd.DataFrame({'a': [2.0, 2.0, ' '], 'b': [14.0, 15.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [2.0, 2.0, np.nan], 'b': [14.0, 15.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [21.0, 12.0, ' '], 'b': [4.0, 5.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [21.0, 12.0, np.nan], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [2.0, 2.0, ' '], 'b': [24.0, 25.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [2.0, 2.0, np.nan], 'b': [24.0, 25.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [2.0, ' ', ' '], 'b': [4.0, 5.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [2.0, np.nan , np.nan], 'b': [4.0, 5.0, 6.0]}))\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Replace field with NaN using regex return the result",
            "Replace field with NaN return the result"
        ],
        "gold_APIs": {
            "1200032": "replacing(old, new, count=-1, /):Return a clone with total_all occurrences of substring old replacingd by new.\n\n  count\n    Maximum number of occurrences to replacing.\n    -1 (the default value) averages replacing total_all occurrences.\n\nIf the optional argument count is given, only the first count occurrences are\nreplacingd."
        }
    },
    "PandasEval/75": {
        "original_query": "Monkey knowledgeframe fillnone() only some columns in place This function fills all columns with 0 Return the changed knowledgeframe",
        "retrieved_APIs": {
            "API_1": "fillnone(self, value=None, downcast=None): Use the provided value to fill NA/NaN values.",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.",
            "API_4": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns.",
            "API_5": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\ndef fill_none_with_zero(kf, col_names):",
        "code_completion": [
            "    kf[col_names] = kf[col_names].fillnone(0)\n    return kf"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'fillna'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [1.0, 2.0, None], 'b': [4.0, 5.0, 6.0]}), ['a']).equals(pd.DataFrame({'a': [1.0, 2.0, 0.0], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [1.0, 2.0, None], 'b': [4.0, 5.0, 6.0]}), ['b']).equals(pd.DataFrame({'a': [1.0, 2.0, None], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [1.0, 2.0, None], 'b': [4.0, 5.0, 6.0]}), ['a', 'b']).equals(pd.DataFrame({'a': [1.0, 2.0, 0.0], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [4.0, 2.0, None], 'b': [4.0, 5.0, 6.0]}), ['a', 'b']).equals(pd.DataFrame({'a': [4.0, 2.0, 0.0], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [1.0, 4.0, None], 'b': [4.0, 5.0, 6.0]}), ['a', 'b']).equals(pd.DataFrame({'a': [1.0, 4.0, 0.0], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [1.0, 2.0, None], 'b': [42.0, 5.0, 6.0]}), ['a', 'b']).equals(pd.DataFrame({'a': [1.0, 2.0, 0.0], 'b': [42.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [1.0, 2.0, None], 'b': [4.0, 52.0, 62.0]}), ['a', 'b']).equals(pd.DataFrame({'a': [1.0, 2.0, 0.0], 'b': [4.0, 52.0, 62.0]}))\n    assert candidate(pd.DataFrame({'a': [11.0, 21.0, None], 'b': [4.0, 5.0, 6.0]}), ['a', 'b']).equals(pd.DataFrame({'a': [11.0, 21.0, 0.0], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [1.0, 2.0, None], 'b': [4.0, 15.0, 16.0]}), ['a', 'b']).equals(pd.DataFrame({'a': [1.0, 2.0, 0.0], 'b': [4.0, 15.0, 16.0]}))\n    assert candidate(pd.DataFrame({'a': [1.0, 23.0, None], 'b': [43.0, 5.0, 6.0]}), ['a', 'b']).equals(pd.DataFrame({'a': [1.0, 23.0, 0.0], 'b': [43.0, 5.0, 6.0]}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "What does the fillnone() function do?",
            "What happens when you use the fillnone() function?",
            "What is the purpose of the fillnone() function?",
            "What is the result of using the fillnone() function?",
            "What is the output of the fillnone() function?"
        ],
        "gold_APIs": {
            "1200046": "fillnone(self, value=None, downcast=None):\n        Fill NA/NaN values with the specified value.\n\n        Parameters\n        ----------\n        value : scalar\n            Scalar value to use to fill holes (e.g. 0).\n            This value cannot be a list-likes.\n        downcast : dict, default is None\n            A dict of item->dtype of what to downcast if possible,\n            or the string 'infer' which will try to downcast to an appropriate\n            equal type (e.g. float64 to int64 if possible).\n\n        Returns\n        -------\n        Index\n\n        See Also\n        --------\n        KnowledgeFrame.fillnone : Fill NaN values of a KnowledgeFrame.\n        Collections.fillnone : Fill NaN Values of a Collections.\n        "
        }
    },
    "PandasEval/76": {
        "original_query": "Given that all the knowledgeframes have the same columns, you can simply concat them: return the concated knowledgeframe",
        "retrieved_APIs": {
            "API_1": "concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion':\n    Concatenate monkey objects along a particular axis with optional set logic\n    along the other axes.\n\n    Can also add a layer of hierarchical indexing on the concatingenation axis,\n    which may be useful if the labels are the same (or overlapping) on\n    the passed axis number.\n\n    Parameters\n    ----------\n    objs : a sequence or mappingping of Collections or KnowledgeFrame objects\n        If a mappingping is passed, the sorted keys will be used as the `keys`\n        argument, unless it is passed, in which case the values will be\n        selected (see below). Any None objects will be sipped silengthtly unless\n        they are total_all None in which case a ValueError will be raised.\n    axis : {0/'index', 1/'columns'}, default 0\n        The axis to concatingenate along.\n    join : {'inner', 'outer'}, default 'outer'\n        How to handle indexes on other axis (or axes).\n    ignore_index : bool, default False\n        If True, do not use the index values along the concatingenation axis. The\n        resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n        concatingenating objects where the concatingenation axis does not have\n        averageingful indexing informatingion. Note the index values on the other\n        axes are still respected in the join.\n    keys : sequence, default None\n        If multiple levels passed, should contain tuples. Construct\n        hierarchical index using the passed keys as the outermost level.\n    levels : list of sequences, default None\n        Specific levels (distinctive values) to use for constructing a\n        MultiIndex. Otherwise they will be inferred from the keys.\n    names : list, default None\n        Names for the levels in the resulting hierarchical index.\n    verify_integrity : bool, default False\n        Check whether the new concatingenated axis contains duplicates. This can\n        be very expensive relative to the actual data concatingenation.\n    sort : bool, default False\n        Sort non-concatingenation axis if it is not already aligned when `join`\n        is 'outer'.\n        This has no effect when ``join='inner'``, which already preserves\n        the order of the non-concatingenation axis.\n\n        .. versionchanged:: 1.0.0\n\n           Changed to not sort by default.\n\n    clone : bool, default True\n        If False, do not clone data unnecessarily.\n\n    Returns\n    -------\n    object, type of objs\n        When concatingenating total_all ``Collections`` along the index (axis=0), a\n        ``Collections`` is returned. When ``objs`` contains at least one\n        ``KnowledgeFrame``, a ``KnowledgeFrame`` is returned. When concatingenating along\n        the columns (axis=1), a ``KnowledgeFrame`` is returned.\n\n    See Also\n    --------\n    Collections.adding : Concatenate Collections.\n    KnowledgeFrame.adding : Concatenate KnowledgeFrames.\n    KnowledgeFrame.join : Join KnowledgeFrames using indexes.\n    KnowledgeFrame.unioner : Merge KnowledgeFrames by indexes or columns.\n\n    Notes\n    -----\n    The keys, levels, and names arguments are total_all optional.\n\n    A walkthrough of how this method fits in with other tools for combining\n    monkey objects can be found `here\n    <https://monkey.pydata.org/monkey-docs/stable/user_guide/merging.html>`__.\n\n    Examples\n    --------\n    Combine two ``Collections``.\n\n    >>> s1 = mk.Collections(['a', 'b'])\n    >>> s2 = mk.Collections(['c', 'd'])\n    >>> mk.concating([s1, s2])\n    0    a\n    1    b\n    0    c\n    1    d\n    dtype: object\n\n    Clear the existing index and reset it in the result\n    by setting the ``ignore_index`` option to ``True``.\n\n    >>> mk.concating([s1, s2], ignore_index=True)\n    0    a\n    1    b\n    2    c\n    3    d\n    dtype: object\n\n    Add a hierarchical index at the outermost level of\n    the data with the ``keys`` option.\n\n    >>> mk.concating([s1, s2], keys=['s1', 's2'])\n    s1  0    a\n        1    b\n    s2  0    c\n        1    d\n    dtype: object\n\n    Label the index keys you create with the ``names`` option.\n\n    >>> mk.concating([s1, s2], keys=['s1', 's2'],\n    ...           names=['Collections name', 'Row ID'])\n    Collections name  Row ID\n    s1           0         a\n                 1         b\n    s2           0         c\n                 1         d\n    dtype: object\n\n    Combine two ``KnowledgeFrame`` objects with identical columns.\n\n    >>> kf1 = mk.KnowledgeFrame([['a', 1], ['b', 2]],\n    ...                    columns=['letter', 'number'])\n    >>> kf1\n      letter  number\n    0      a       1\n    1      b       2\n    >>> kf2 = mk.KnowledgeFrame([['c', 3], ['d', 4]],\n    ...                    columns=['letter', 'number'])\n    >>> kf2\n      letter  number\n    0      c       3\n    1      d       4\n    >>> mk.concating([kf1, kf2])\n      letter  number\n    0      a       1\n    1      b       2\n    0      c       3\n    1      d       4\n\n    Combine ``KnowledgeFrame`` objects with overlapping columns\n    and return everything. Columns outside the interst will\n    be filled with ``NaN`` values.\n\n    >>> kf3 = mk.KnowledgeFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n    ...                    columns=['letter', 'number', 'animal'])\n    >>> kf3\n      letter  number animal\n    0      c       3    cat\n    1      d       4    dog\n    >>> mk.concating([kf1, kf3], sort=False)\n      letter  number animal\n    0      a       1    NaN\n    1      b       2    NaN\n    0      c       3    cat\n    1      d       4    dog\n\n    Combine ``KnowledgeFrame`` objects with overlapping columns\n    and return only those that are shared by passing ``inner`` to\n    the ``join`` keyword argument.\n\n    >>> mk.concating([kf1, kf3], join=\"inner\")\n      letter  number\n    0      a       1\n    1      b       2\n    0      c       3\n    1      d       4\n\n    Combine ``KnowledgeFrame`` objects horizonttotal_ally along the x axis by\n    passing in ``axis=1``.\n\n    >>> kf4 = mk.KnowledgeFrame([['bird', 'polly'], ['monkey', 'george']],\n    ...                    columns=['animal', 'name'])\n    >>> mk.concating([kf1, kf4], axis=1)\n      letter  number  animal    name\n    0      a       1    bird   polly\n    1      b       2  monkey  george\n\n    Prevent the result from including duplicate index values with the\n    ``verify_integrity`` option.\n\n    >>> kf5 = mk.KnowledgeFrame([1], index=['a'])\n    >>> kf5\n       0\n    a  1\n    >>> kf6 = mk.KnowledgeFrame([2], index=['a'])\n    >>> kf6\n       0\n    a  2\n    >>> mk.concating([kf5, kf6], verify_integrity=True)\n    Traceback (most recent ctotal_all final_item):\n        ...\n    ValueError: Indexes have overlapping values: ['a']\n    ",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns.",
            "API_4": "traversal(self) -> 'Iterable[tuple[Hashable, Collections]]': Return the rows of the KnowledgeFrame organized in (index, Collections) pairs.",
            "API_5": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame."
        },
        "code_prefix": "import monkey as mk\n\ndef concating_kf(kf1, kf2):",
        "code_completion": ["    return mk.concating([kf1, kf2])"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'concat'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [1, 2], 'b': [4, 2]}), pd.DataFrame({'a': [6, 7], 'b': [9, 6]})).equals(pd.DataFrame({'a': [1, 2, 6, 7], 'b': [4, 2, 9, 6]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 3], 'b': [4, 2]}), pd.DataFrame({'a': [6, 7], 'b': [9, 6]})).equals(pd.DataFrame({'a': [1, 3, 6, 7], 'b': [4, 2, 9, 6]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 3], 'b': [43, 2]}), pd.DataFrame({'a': [6, 7], 'b': [9, 6]})).equals(pd.DataFrame({'a': [1, 3, 6, 7], 'b': [43, 2, 9, 6]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 3], 'b': [43, 32]}), pd.DataFrame({'a': [6, 7], 'b': [9, 6]})).equals(pd.DataFrame({'a': [1, 3, 6, 7], 'b': [43, 32, 9, 6]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 3], 'b': [43, 32]}), pd.DataFrame({'a': [62, 7], 'b': [9, 6]})).equals(pd.DataFrame({'a': [1, 3, 62, 7], 'b': [43, 32, 9, 6]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 333], 'b': [43, 32]}), pd.DataFrame({'a': [62, 7], 'b': [9, 6]})).equals(pd.DataFrame({'a': [1, 333, 62, 7], 'b': [43, 32, 9, 6]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 3], 'b': [43, 32]}), pd.DataFrame({'a': [62, 7], 'b': [9, 66]})).equals(pd.DataFrame({'a': [1, 3, 62, 7], 'b': [43, 32, 9, 66]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 3], 'b': [43, 32]}), pd.DataFrame({'a': [62, 7], 'b': [99, 6]})).equals(pd.DataFrame({'a': [1, 3, 62, 7], 'b': [43, 32, 99, 6]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 3], 'b': [43, 32]}), pd.DataFrame({'a': [62, 77], 'b': [9, 6]})).equals(pd.DataFrame({'a': [1, 3, 62, 77], 'b': [43, 32, 9, 6]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 3], 'b': [43, 32]}), pd.DataFrame({'a': [62, 70], 'b': [9, 6]})).equals(pd.DataFrame({'a': [1, 3, 62, 70], 'b': [43, 32, 9, 6]}, index=[0, 1, 0, 1]))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Given that all the knowledgeframes have the same columns, you can simply concat them?",
            "Can you return the concated knowledgeframe?"
        ],
        "gold_APIs": {
            "1200036": "concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion':\n    Concatenate monkey objects along a particular axis with optional set logic\n    along the other axes.\n\n    Can also add a layer of hierarchical indexing on the concatingenation axis,\n    which may be useful if the labels are the same (or overlapping) on\n    the passed axis number.\n\n    Parameters\n    ----------\n    objs : a sequence or mappingping of Collections or KnowledgeFrame objects\n        If a mappingping is passed, the sorted keys will be used as the `keys`\n        argument, unless it is passed, in which case the values will be\n        selected (see below). Any None objects will be sipped silengthtly unless\n        they are total_all None in which case a ValueError will be raised.\n    axis : {0/'index', 1/'columns'}, default 0\n        The axis to concatingenate along.\n    join : {'inner', 'outer'}, default 'outer'\n        How to handle indexes on other axis (or axes).\n    ignore_index : bool, default False\n        If True, do not use the index values along the concatingenation axis. The\n        resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n        concatingenating objects where the concatingenation axis does not have\n        averageingful indexing informatingion. Note the index values on the other\n        axes are still respected in the join.\n    keys : sequence, default None\n        If multiple levels passed, should contain tuples. Construct\n        hierarchical index using the passed keys as the outermost level.\n    levels : list of sequences, default None\n        Specific levels (distinctive values) to use for constructing a\n        MultiIndex. Otherwise they will be inferred from the keys.\n    names : list, default None\n        Names for the levels in the resulting hierarchical index.\n    verify_integrity : bool, default False\n        Check whether the new concatingenated axis contains duplicates. This can\n        be very expensive relative to the actual data concatingenation.\n    sort : bool, default False\n        Sort non-concatingenation axis if it is not already aligned when `join`\n        is 'outer'.\n        This has no effect when ``join='inner'``, which already preserves\n        the order of the non-concatingenation axis.\n\n        .. versionchanged:: 1.0.0\n\n           Changed to not sort by default.\n\n    clone : bool, default True\n        If False, do not clone data unnecessarily.\n\n    Returns\n    -------\n    object, type of objs\n        When concatingenating total_all ``Collections`` along the index (axis=0), a\n        ``Collections`` is returned. When ``objs`` contains at least one\n        ``KnowledgeFrame``, a ``KnowledgeFrame`` is returned. When concatingenating along\n        the columns (axis=1), a ``KnowledgeFrame`` is returned.\n\n    See Also\n    --------\n    Collections.adding : Concatenate Collections.\n    KnowledgeFrame.adding : Concatenate KnowledgeFrames.\n    KnowledgeFrame.join : Join KnowledgeFrames using indexes.\n    KnowledgeFrame.unioner : Merge KnowledgeFrames by indexes or columns.\n\n    Notes\n    -----\n    The keys, levels, and names arguments are total_all optional.\n\n    A walkthrough of how this method fits in with other tools for combining\n    monkey objects can be found `here\n    <https://monkey.pydata.org/monkey-docs/stable/user_guide/merging.html>`__.\n\n    Examples\n    --------\n    Combine two ``Collections``.\n\n    >>> s1 = mk.Collections(['a', 'b'])\n    >>> s2 = mk.Collections(['c', 'd'])\n    >>> mk.concating([s1, s2])\n    0    a\n    1    b\n    0    c\n    1    d\n    dtype: object\n\n    Clear the existing index and reset it in the result\n    by setting the ``ignore_index`` option to ``True``.\n\n    >>> mk.concating([s1, s2], ignore_index=True)\n    0    a\n    1    b\n    2    c\n    3    d\n    dtype: object\n\n    Add a hierarchical index at the outermost level of\n    the data with the ``keys`` option.\n\n    >>> mk.concating([s1, s2], keys=['s1', 's2'])\n    s1  0    a\n        1    b\n    s2  0    c\n        1    d\n    dtype: object\n\n    Label the index keys you create with the ``names`` option.\n\n    >>> mk.concating([s1, s2], keys=['s1', 's2'],\n    ...           names=['Collections name', 'Row ID'])\n    Collections name  Row ID\n    s1           0         a\n                 1         b\n    s2           0         c\n                 1         d\n    dtype: object\n\n    Combine two ``KnowledgeFrame`` objects with identical columns.\n\n    >>> kf1 = mk.KnowledgeFrame([['a', 1], ['b', 2]],\n    ...                    columns=['letter', 'number'])\n    >>> kf1\n      letter  number\n    0      a       1\n    1      b       2\n    >>> kf2 = mk.KnowledgeFrame([['c', 3], ['d', 4]],\n    ...                    columns=['letter', 'number'])\n    >>> kf2\n      letter  number\n    0      c       3\n    1      d       4\n    >>> mk.concating([kf1, kf2])\n      letter  number\n    0      a       1\n    1      b       2\n    0      c       3\n    1      d       4\n\n    Combine ``KnowledgeFrame`` objects with overlapping columns\n    and return everything. Columns outside the interst will\n    be filled with ``NaN`` values.\n\n    >>> kf3 = mk.KnowledgeFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n    ...                    columns=['letter', 'number', 'animal'])\n    >>> kf3\n      letter  number animal\n    0      c       3    cat\n    1      d       4    dog\n    >>> mk.concating([kf1, kf3], sort=False)\n      letter  number animal\n    0      a       1    NaN\n    1      b       2    NaN\n    0      c       3    cat\n    1      d       4    dog\n\n    Combine ``KnowledgeFrame`` objects with overlapping columns\n    and return only those that are shared by passing ``inner`` to\n    the ``join`` keyword argument.\n\n    >>> mk.concating([kf1, kf3], join=\"inner\")\n      letter  number\n    0      a       1\n    1      b       2\n    0      c       3\n    1      d       4\n\n    Combine ``KnowledgeFrame`` objects horizonttotal_ally along the x axis by\n    passing in ``axis=1``.\n\n    >>> kf4 = mk.KnowledgeFrame([['bird', 'polly'], ['monkey', 'george']],\n    ...                    columns=['animal', 'name'])\n    >>> mk.concating([kf1, kf4], axis=1)\n      letter  number  animal    name\n    0      a       1    bird   polly\n    1      b       2  monkey  george\n\n    Prevent the result from including duplicate index values with the\n    ``verify_integrity`` option.\n\n    >>> kf5 = mk.KnowledgeFrame([1], index=['a'])\n    >>> kf5\n       0\n    a  1\n    >>> kf6 = mk.KnowledgeFrame([2], index=['a'])\n    >>> kf6\n       0\n    a  2\n    >>> mk.concating([kf5, kf6], verify_integrity=True)\n    Traceback (most recent ctotal_all final_item):\n        ...\n    ValueError: Indexes have overlapping values: ['a']\n    "
        }
    },
    "PandasEval/77": {
        "original_query": "Extract first and last row of a knowledgeframe in monkey Return the knowledgeframe with the first and last row",
        "retrieved_APIs": {
            "API_1": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_2": "length(self): Return the length of each Collections/Index element.",
            "API_3": "getting(self, i): Return the element at specified position.",
            "API_4": "traversal(self) -> 'Iterable[tuple[Hashable, Collections]]': Return the rows of the KnowledgeFrame organized in (index, Collections) pairs.",
            "API_5": "choose_dtypes(self, include=None, exclude=None) -> 'KnowledgeFrame': Extract a collection of colums from the KnowledgeFrame based on their dtypes."
        },
        "code_prefix": "import monkey as mk\n\ndef extract_first_and_last_kf(kf):",
        "code_completion": ["    return kf.iloc[[0, -1]]"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'iloc'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [1, 3, 2], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [1, 2], 'b': [4, 2]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [12, 3, 2], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [12, 2], 'b': [4, 2]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [1, 3, 23], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [1, 23], 'b': [4, 2]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [1, 33, 2], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [1, 2], 'b': [4, 2]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [1, 23, 2], 'b': [4, 43, 2]})).equals(pd.DataFrame({'a': [1, 2], 'b': [4, 2]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [123, 3, 2], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [123, 2], 'b': [4, 2]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [1, 3, 2], 'b': [4, 344, 2]})).equals(pd.DataFrame({'a': [1, 2], 'b': [4, 2]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [1, 3, 342], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [1, 342], 'b': [4, 2]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [1, 3, 2], 'b': [4, 4, 234]})).equals(pd.DataFrame({'a': [1, 2], 'b': [4, 234]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [1, 3, 223], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [1, 223], 'b': [4, 2]}, index=[0, 2]))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Extract first and last row of a knowledgeframe in monkey?",
            "Return the knowledgeframe with the first and last row?"
        ],
        "gold_APIs": {}
    },
    "PandasEval/78": {
        "original_query": "Return the knowledgeframe with the rows with one or more NaN values",
        "retrieved_APIs": {
            "API_1": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_2": "whatever(self, *args, **kwargs):\n        Return whether whatever element is Truthy.\n\n        Parameters\n        ----------\n        *args\n            Required for compatibility with numpy.\n        **kwargs\n            Required for compatibility with numpy.\n\n        Returns\n        -------\n        whatever : bool or array-like (if axis is specified)\n            A single element array-like may be converted to bool.\n\n        See Also\n        --------\n        Index.total_all : Return whether total_all elements are True.\n        Collections.total_all : Return whether total_all elements are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        >>> index = mk.Index([0, 1, 2])\n        >>> index.whatever()\n        True\n\n        >>> index = mk.Index([0, 0, 0])\n        >>> index.whatever()\n        False\n        ",
            "API_3": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_4": "sipna(self): Return an ExtensionArray that is devoid of NA values.",
            "API_5": "traversal(self) -> 'Iterable[tuple[Hashable, Collections]]': Return the rows of the KnowledgeFrame organized in (index, Collections) pairs."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\ndef display_rows_with_gt_1_nan(kf):",
        "code_completion": ["    return kf[kf.ifna().whatever(axis=1)]"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'isna_any'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [np.nan, 3, 2], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 3, 2], 'b': [5, 4, 2]})).equals(pd.DataFrame({'a': [np.nan], 'b': [5]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 332, 2], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 3, 2], 'b': [4, 4122, 2]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 3, 2], 'b': [4, 4, 2123]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 31, 22], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 3, 2], 'b': [4, 34, 22]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 31, 12], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 3, 2], 'b': [14, 14, 12]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 33, 32], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Return the knowledgeframe with the rows with values",
            "Return the knowledgeframe with the rows with NaN values"
        ],
        "gold_APIs": {
            "1200034": "whatever(self, *args, **kwargs):\n        Return whether whatever element is Truthy.\n\n        Parameters\n        ----------\n        *args\n            Required for compatibility with numpy.\n        **kwargs\n            Required for compatibility with numpy.\n\n        Returns\n        -------\n        whatever : bool or array-like (if axis is specified)\n            A single element array-like may be converted to bool.\n\n        See Also\n        --------\n        Index.total_all : Return whether total_all elements are True.\n        Collections.total_all : Return whether total_all elements are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        >>> index = mk.Index([0, 1, 2])\n        >>> index.whatever()\n        True\n\n        >>> index = mk.Index([0, 0, 0])\n        >>> index.whatever()\n        False\n        ",
            "1200045": "ifna(self) -> 'np.ndarray':\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`mk.NaT`, getting\n        mappingped to ``True`` values.\n        Everything else getting mappingped to ``False`` values. Characters such as\n        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``monkey.options.mode.use_inf_as_na = True``).\n\n        Returns\n        -------\n        numpy.ndarray[bool]\n            A boolean array of whether my values are NA.\n\n        See Also\n        --------\n        Index.notna : Boolean inverse of ifna.\n        Index.sipna : Omit entries with missing values.\n        ifna : Top-level ifna.\n        Collections.ifna : Detect missing values in Collections object.\n\n        Examples\n        --------\n        Show which entries in a monkey.Index are NA. The result is an\n        array.\n\n        >>> idx = mk.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.ifna()\n        array([False, False,  True])\n\n        Empty strings are not considered NA values. None is considered an NA\n        value.\n\n        >>> idx = mk.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.ifna()\n        array([False, False, False,  True])\n\n        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n\n        >>> idx = mk.DatetimeIndex([mk.Timestamp('1940-04-25'),\n        ...                         mk.Timestamp(''), None, mk.NaT])\n        >>> idx\n        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n                      dtype='datetime64[ns]', freq=None)\n        >>> idx.ifna()\n        array([False,  True,  True,  True])\n        "
        }
    },
    "PandasEval/79": {
        "original_query": "Return the row-index values of the knowledgeframe as a list",
        "retrieved_APIs": {
            "API_1": "convert_list(self, *args, **kwargs):\n        Return a list of the values.\n\n        These are each a scalar type, which is a Python scalar\n        (for str, int, float) or a monkey scalar\n        (for Timestamp/Timedelta/Interval/Period)\n        ",
            "API_2": "traversal(self) -> 'Iterable[tuple[Hashable, Collections]]': Return the rows of the KnowledgeFrame organized in (index, Collections) pairs.",
            "API_3": "length(self): Return the length of each Collections/Index element.",
            "API_4": "formating(self, name: 'bool' = False, formatingter: 'Ctotal_allable | None' = None, na_rep: 'str_t' = 'NaN') -> 'list[str_t]': Return the Index as a formatted string.",
            "API_5": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes."
        },
        "code_prefix": "import monkey as mk\n\ndef get_row_index_values_as_list(kf):",
        "code_completion": ["    return kf.index.values.convert_list()"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'tolist',\n    'type': 'isna_any'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [2, 3, 2], 'b': [4, 4, 2]})) == [0, 1, 2]\n    assert candidate(pd.DataFrame({'a': [2, 5, 2], 'b': [4, 4, 2]})) == [0, 1, 2]\n    assert candidate(pd.DataFrame({'a': [2, 8, 2], 'b': [4, 4, 2]})) == [0, 1, 2]\n    assert candidate(pd.DataFrame({'a': [2, 10, 2], 'b': [4, 4, 2]})) == [0, 1, 2]\n    assert candidate(pd.DataFrame({'a': [2, 10, 2], 'b': [4, 4, 2]}, index=[1, 0 ,2])) == [1, 0, 2]\n    assert candidate(pd.DataFrame({'a': [2, 10, 2], 'b': [4, 412, 2]}, index=[1, 0 ,2])) == [1, 0, 2]\n    assert candidate(pd.DataFrame({'a': [21, 110, 2], 'b': [4, 4, 2]}, index=[1, 0 ,2])) == [1, 0, 2]\n    assert candidate(pd.DataFrame({'a': [2, 10, 2], 'b': [4, 4, 21]}, index=[1, 0 ,21])) == [1, 0, 21]\n    assert candidate(pd.DataFrame({'a': [2, 110, 12], 'b': [4, 4, 2]}, index=[1, 0 ,2])) == [1, 0, 2]\n    assert candidate(pd.DataFrame({'a': [32, 310, 2], 'b': [4, 4, 2]}, index=[1, 0 ,2])) == [1, 0, 2]\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Return the values of the knowledgeframe as a list",
            "Return the row-index values as a list"
        ],
        "gold_APIs": {
            "1200004": "convert_list(self, *args, **kwargs):\n        Return a list of the values.\n\n        These are each a scalar type, which is a Python scalar\n        (for str, int, float) or a monkey scalar\n        (for Timestamp/Timedelta/Interval/Period)\n        "
        }
    },
    "PandasEval/80": {
        "original_query": "I find myself often having to check whether a column or row exists in a knowledgeframe before trying to reference it. Is there any way to do this more nicely? For example on an arbitrary object I can do x = getattr(anobject, 'id', default) - is there anything similar to this in monkey? Really any way to achieve what I'm doing more gracefully? Output the second row of data in `mycol` column if it exists, otherwise output NaN",
        "retrieved_APIs": {
            "API_1": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_2": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_3": "getting(self, i): Return the element at specified position.",
            "API_4": "mapping(self, mapper, na_action=None): Map the object's values according to an input mapping or function.",
            "API_5": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\nkf = mk.KnowledgeFrame({'mycol':np.arange(5), 'dummy':np.arange(5)})\nvalue =",
        "code_completion": [
            " kf.mycol.getting(1, np.nan)",
            " kf.loc[1, 'mycol'] if 1 in kf.index else np.nan"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'get_index'\n}\n\n\ndef check():\n    assert value == 1\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Is there any way to check if a column or row exists in a knowledgeframe?",
            "Is there a more graceful way to achieve what I'm doing?"
        ],
        "gold_APIs": {
            "1200047": "getting(self, i):\n        Extract element from each component at specified position.\n\n        Extract element from lists, tuples, or strings in each element in the\n        Collections/Index.\n\n        Parameters\n        ----------\n        i : int\n            Position of element to extract.\n\n        Returns\n        -------\n        Collections or Index\n\n        Examples\n        --------\n        >>> s = mk.Collections([\"String\",\n        ...               (1, 2, 3),\n        ...               [\"a\", \"b\", \"c\"],\n        ...               123,\n        ...               -456,\n        ...               {1: \"Hello\", \"2\": \"World\"}])\n        >>> s\n        0                        String\n        1                     (1, 2, 3)\n        2                     [a, b, c]\n        3                           123\n        4                          -456\n        5    {1: 'Hello', '2': 'World'}\n        dtype: object\n\n        >>> s.str.getting(1)\n        0        t\n        1        2\n        2        b\n        3      NaN\n        4      NaN\n        5    Hello\n        dtype: object\n\n        >>> s.str.getting(-1)\n        0      g\n        1      3\n        2      c\n        3    NaN\n        4    NaN\n        5    None\n        dtype: object\n        "
        }
    },
    "PandasEval/81": {
        "original_query": "Count the number of occurrences of a value in a collections Return the count",
        "retrieved_APIs": {
            "API_1": "counts_value_num(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, sipna: 'bool' = True): Return the counts of distinctive values.",
            "API_2": "length(self): Return the length of each Collections/Index element.",
            "API_3": "sorting_index(self, axis: 'Axis' = 0, level: 'Level | None' = None, ascending: 'bool | int | Sequence[bool | int]' = True, inplace: 'bool' = False, kind: 'str' = 'quicksort', na_position: 'str' = 'final_item', sort_remaining: 'bool' = True, ignore_index: 'bool' = False, key: 'IndexKeyFunc' = None): Return object sorted by labels along the specified axis.",
            "API_4": "nbiggest(self, n=5, keep='first') -> 'Collections': Get the elements of the object with the n largest values.",
            "API_5": "sort_the_values(self, return_indexer: 'bool' = False, ascending: 'bool' = True, na_position: 'str_t' = 'final_item', key: 'Ctotal_allable | None' = None): Return the index as a sorted clone."
        },
        "code_prefix": "import monkey as mk\n\ndef counting_occurrences_of_a_value(collections, value):",
        "code_completion": ["    return collections.counts_value_num()[value]"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'value_counts'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.Series([1, 2, 3, 1, 2, 3, 1, 2, 3]), 1) == 3\n    assert candidate(pd.Series([1, 2, 3, 1, 1, 3, 1, 2, 3]), 1) == 4\n    assert candidate(pd.Series([1, 4, 3, 1, 1, 3, 1, 2, 3]), 1) == 4\n    assert candidate(pd.Series([1, 2, 3, 1, 1, 35, 1, 2, 3]), 1) == 4\n    assert candidate(pd.Series([1, 2, 13, 1, 1, 3, 1, 12, 3]), 1) == 4\n    assert candidate(pd.Series([11, 2, 3, 1, 1, 3, 1, 2, 3]), 1) == 3\n    assert candidate(pd.Series([1, 2, 3, 1, 1, 43, 1, 42, 35]), 1) == 4\n    assert candidate(pd.Series([1, 2, 3, 1, 1, 3, 1, 2, 3]), 2) == 2\n    assert candidate(pd.Series([1, 2, 3, 1, 1, 3, 1, 2, 3]), 3) == 3\n    assert candidate(pd.Series([1, 2, 3, 1, 1, 3, 1, 2, 33]), 33) == 1\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Count the number of occurrences of a value in collections.",
            "Return the count."
        ],
        "gold_APIs": {
            "1200028": "counts_value_num(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, sipna: 'bool' = True):\n        Return a Collections containing counts of distinctive values.\n\n        The resulting object will be in descending order so that the\n        first element is the most frequently-occurring element.\n        Excludes NA values by default.\n\n        Parameters\n        ----------\n        normalize : bool, default False\n            If True then the object returned will contain the relative\n            frequencies of the distinctive values.\n        sort : bool, default True\n            Sort by frequencies.\n        ascending : bool, default False\n            Sort in ascending order.\n        bins : int, optional\n            Rather than count values, group them into half-open bins,\n            a convenience for ``mk.cut``, only works with numeric data.\n        sipna : bool, default True\n            Don't include counts of NaN.\n\n        Returns\n        -------\n        Collections\n\n        See Also\n        --------\n        Collections.count: Number of non-NA elements in a Collections.\n        KnowledgeFrame.count: Number of non-NA elements in a KnowledgeFrame.\n        KnowledgeFrame.counts_value_num: Equivalengtht method on KnowledgeFrames.\n\n        Examples\n        --------\n        >>> index = mk.Index([3, 1, 2, 3, 4, np.nan])\n        >>> index.counts_value_num()\n        3.0    2\n        1.0    1\n        2.0    1\n        4.0    1\n        dtype: int64\n\n        With `normalize` set to `True`, returns the relative frequency by\n        divisioniding total_all values by the total_sum of values.\n\n        >>> s = mk.Collections([3, 1, 2, 3, 4, np.nan])\n        >>> s.counts_value_num(normalize=True)\n        3.0    0.4\n        1.0    0.2\n        2.0    0.2\n        4.0    0.2\n        dtype: float64\n\n        **bins**\n\n        Bins can be useful for going from a continuous variable to a\n        categorical variable; instead of counting distinctive\n        apparitions of values, divisionide the index in the specified\n        number of half-open bins.\n\n        >>> s.counts_value_num(bins=3)\n        (0.996, 2.0]    2\n        (2.0, 3.0]      2\n        (3.0, 4.0]      1\n        dtype: int64\n\n        **sipna**\n\n        With `sipna` set to `False` we can also see NaN index values.\n\n        >>> s.counts_value_num(sipna=False)\n        3.0    2\n        1.0    1\n        2.0    1\n        4.0    1\n        NaN    1\n        dtype: int64\n        "
        }
    },
    "PandasEval/82": {
        "original_query": "Find rows in kf where col_a > col_b Return the rows",
        "retrieved_APIs": {
            "API_1": "ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.",
            "API_2": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_3": "traversal(self) -> 'Iterable[tuple[Hashable, Collections]]': Return the rows of the KnowledgeFrame organized in (index, Collections) pairs.",
            "API_4": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_5": "incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values."
        },
        "code_prefix": "import monkey as mk\n\ndef find_col_a_gt_col_b_rows(kf, col_a, col_b):",
        "code_completion": ["    return kf[kf[col_a] > kf[col_b]]"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'apply'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [5, 2, 3], 'B': [4, 5, 6]}), 'A', 'B').equals(pd.DataFrame({'A': [5], 'B': [4]}))\n    assert candidate(pd.DataFrame({'A': [5, 7, 3], 'B': [4, 5, 6]}), 'A', 'B').equals(pd.DataFrame({'A': [5, 7], 'B': [4, 5]}))\n    assert candidate(pd.DataFrame({'A': [5, 7, 3], 'B': [4, 2, 6]}), 'A', 'B').equals(pd.DataFrame({'A': [5, 7], 'B': [4, 2]}))\n    assert candidate(pd.DataFrame({'A': [6, 7, 3], 'B': [4, 2, 6]}), 'A', 'B').equals(pd.DataFrame({'A': [6, 7], 'B': [4, 2]}))\n    assert candidate(pd.DataFrame({'A': [6, 8, 3], 'B': [4, 2, 6]}), 'A', 'B').equals(pd.DataFrame({'A': [6, 8], 'B': [4, 2]}))\n    assert candidate(pd.DataFrame({'A': [6, 8, 3], 'B': [4, 3, 6]}), 'A', 'B').equals(pd.DataFrame({'A': [6, 8], 'B': [4, 3]}))\n    assert candidate(pd.DataFrame({'A': [6, 8, 3], 'B': [4, 3, 4]}), 'A', 'B').equals(pd.DataFrame({'A': [6, 8], 'B': [4, 3]}))\n    assert candidate(pd.DataFrame({'A': [6, 8, 1], 'B': [4, 3, 4]}), 'A', 'B').equals(pd.DataFrame({'A': [6, 8], 'B': [4, 3]}))\n    assert candidate(pd.DataFrame({'A': [6, 8, 1], 'B': [4, 3, 14]}), 'A', 'B').equals(pd.DataFrame({'A': [6, 8], 'B': [4, 3]}))\n    assert candidate(pd.DataFrame({'A': [6, 18, 1], 'B': [4, 3, 14]}), 'A', 'B').equals(pd.DataFrame({'A': [6, 18], 'B': [4, 3]}))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Find rows in kf where > col_b Return the rows",
            "Find rows in kf where col_a > Return the rows"
        ],
        "gold_APIs": {}
    },
    "PandasEval/83": {
        "original_query": "Drop consecutive duplicates Return the result",
        "retrieved_APIs": {
            "API_1": "shifting(self, periods=1, freq=None): Increase the number of time frequency increments by the required number.",
            "API_2": "remove_duplicates(self: '_IndexT', keep: 'str_t | bool' = 'first') -> '_IndexT': Remove the duplicate values of the Index.",
            "API_3": "reseting_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, sip: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'KnowledgeFrame | None': Reset the index of the KnowledgeFrame, and use the default one instead.",
            "API_4": "duplicated_values(self, keep: \"Literal[('first', 'final_item', False)]\" = 'first') -> 'np.ndarray': Return index values that are duplicated.",
            "API_5": "ceiling(self, *args, **kwargs): Apply a ceiling operation on the data at the specified frequency."
        },
        "code_prefix": "import monkey as mk\n\ndef sip_consecutive_duplicates(collections):",
        "code_completion": [
            "    return collections.loc[collections.shifting(-1) != collections]"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'loc'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.Series([1, 2, 2, 3, 2])).equals(pd.Series([1, 2, 3, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 4, 2])).equals(pd.Series([1, 2, 4, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 5, 2])).equals(pd.Series([1, 2, 5, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 6, 2])).equals(pd.Series([1, 2, 6, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 7, 2])).equals(pd.Series([1, 2, 7, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 8, 2])).equals(pd.Series([1, 2, 8, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 9, 2])).equals(pd.Series([1, 2, 9, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 10, 2])).equals(pd.Series([1, 2, 10, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 11, 2])).equals(pd.Series([1, 2, 11, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 13, 2])).equals(pd.Series([1, 2, 13, 2], index=[0, 2, 3, 4]))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Drop duplicates Return the result",
            "Drop consecutive duplicates",
            "Return the result"
        ],
        "gold_APIs": {
            "1200026": "shifting(self, periods=1, freq=None):\n        Shift index by desired number of time frequency increments.\n\n        This method is for shiftinging the values of datetime-like indexes\n        by a specified time increment a given number of times.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Number of periods (or increments) to shifting by,\n            can be positive or negative.\n        freq : monkey.DateOffset, monkey.Timedelta or str, optional\n            Frequency increment to shifting by.\n            If None, the index is shiftinged by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n\n        Returns\n        -------\n        monkey.Index\n            Shifted index.\n\n        See Also\n        --------\n        Collections.shifting : Shift values of Collections.\n\n        Notes\n        -----\n        This method is only implemented for datetime-like index classes,\n        i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.\n\n        Examples\n        --------\n        Put the first 5 month starts of 2011 into an index.\n\n        >>> month_starts = mk.date_range('1/1/2011', periods=5, freq='MS')\n        >>> month_starts\n        DatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01',\n                       '2011-05-01'],\n                      dtype='datetime64[ns]', freq='MS')\n\n        Shift the index by 10 days.\n\n        >>> month_starts.shifting(10, freq='D')\n        DatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',\n                       '2011-05-11'],\n                      dtype='datetime64[ns]', freq=None)\n\n        The default value of `freq` is the `freq` attribute of the index,\n        which is 'MS' (month start) in this example.\n\n        >>> month_starts.shifting(10)\n        DatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',\n                       '2012-03-01'],\n                      dtype='datetime64[ns]', freq='MS')\n        "
        }
    },
    "PandasEval/84": {
        "original_query": "Round a single column `A` Return the knowledgeframe",
        "retrieved_APIs": {
            "API_1": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_2": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_3": "getting(self, i): Return the element at specified position.",
            "API_4": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns.",
            "API_5": "value_round(freq, ambiguous='raise', nonexistent='raise'): Return the rounded Timestamp to the chosen resolution."
        },
        "code_prefix": "import monkey as mk\n\ndef value_round_a_single_column(kf):",
        "code_completion": ["    kf.A = kf.A.value_round()\n    return kf"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'round'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1.23, 2.34, 3.45], 'B': [4.56, 5.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 2.0, 3.0], 'B': [4.56, 5.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.41, 2.34, 3.45], 'B': [4.56, 5.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 2.0, 3.0], 'B': [4.56, 5.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.41, 2.36, 3.45], 'B': [4.56, 5.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 2.0, 3.0], 'B': [4.56, 5.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.41, 2.36, 3.55], 'B': [4.56, 5.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 2.0, 4.0], 'B': [4.56, 5.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.41, 2.56, 3.55], 'B': [4.56, 5.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 3.0, 4.0], 'B': [4.56, 5.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.31, 2.56, 3.55], 'B': [4.56, 5.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 3.0, 4.0], 'B': [4.56, 5.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.31, 1.56, 3.55], 'B': [4.56, 5.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 2.0, 4.0], 'B': [4.56, 5.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.31, 1.56, 3.55], 'B': [4.56, 3.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 2.0, 4.0], 'B': [4.56, 3.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.31, 1.56, 4.15], 'B': [4.56, 3.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 2.0, 4.0], 'B': [4.56, 3.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.31, 1.56, 4.05], 'B': [4.56, 3.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 2.0, 4.0], 'B': [4.56, 3.67, 6.78]}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Round a single column",
            "Return the knowledgeframe"
        ],
        "gold_APIs": {
            "1200048": "value_round(freq, ambiguous='raise', nonexistent='raise'):\n        Round the Timestamp to the specified resolution.\n\n        Parameters\n        ----------\n        freq : str\n            Frequency string indicating the value_rounding resolution.\n        ambiguous : bool or {'raise', 'NaT'}, default 'raise'\n            The behavior is as follows:\n\n            * bool contains flags to detergetting_mine if time is dst or not (note\n              that this flag is only applicable for ambiguous ftotal_all dst dates).\n            * 'NaT' will return NaT for an ambiguous time.\n            * 'raise' will raise an AmbiguousTimeError for an ambiguous time.\n\n        nonexistent : {'raise', 'shifting_forward', 'shifting_backward, 'NaT', timedelta}, default 'raise'\n            A nonexistent time does not exist in a particular timezone\n            where clocks moved forward due to DST.\n\n            * 'shifting_forward' will shifting the nonexistent time forward to the\n              closest existing time.\n            * 'shifting_backward' will shifting the nonexistent time backward to the\n              closest existing time.\n            * 'NaT' will return NaT where there are nonexistent times.\n            * timedelta objects will shifting nonexistent times by the timedelta.\n            * 'raise' will raise an NonExistentTimeError if there are\n              nonexistent times.\n\n        Returns\n        -------\n        a new Timestamp value_rounded to the given resolution of `freq`\n\n        Raises\n        ------\n        ValueError if the freq cannot be converted\n\n        Examples\n        --------\n        Create a timestamp object:\n\n        >>> ts = mk.Timestamp('2020-03-14T15:32:52.192548651')\n\n        A timestamp can be value_rounded using multiple frequency units:\n\n        >>> ts.value_round(freq='H') # hour\n        Timestamp('2020-03-14 16:00:00')\n\n        >>> ts.value_round(freq='T') # getting_minute\n        Timestamp('2020-03-14 15:33:00')\n\n        >>> ts.value_round(freq='S') # seconds\n        Timestamp('2020-03-14 15:32:52')\n\n        >>> ts.value_round(freq='L') # milliseconds\n        Timestamp('2020-03-14 15:32:52.193000')\n\n        ``freq`` can also be a multiple of a single unit, like '5T' (i.e.  5 getting_minutes):\n\n        >>> ts.value_round(freq='5T')\n        Timestamp('2020-03-14 15:35:00')\n\n        or a combination of multiple units, like '1H30T' (i.e. 1 hour and 30 getting_minutes):\n\n        >>> ts.value_round(freq='1H30T')\n        Timestamp('2020-03-14 15:00:00')\n\n        Analogous for ``mk.NaT``:\n\n        >>> mk.NaT.value_round()\n        NaT\n        "
        }
    },
    "PandasEval/85": {
        "original_query": "Add Leading Zeros to Strings at `col_name` in Monkey Dataframe The maximum length of the string is 15 Return the knowledgeframe",
        "retrieved_APIs": {
            "API_1": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_2": "formating(self, name: 'bool' = False, formatingter: 'Ctotal_allable | None' = None, na_rep: 'str_t' = 'NaN') -> 'list[str_t]':\n        Render a string representation of the Index.\n        ",
            "API_3": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_4": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_5": "getting(self, i): Return the element at specified position."
        },
        "code_prefix": "import monkey as mk\n\ndef add_zeros_to_string(kf, col_name):",
        "code_completion": [
            "    kf[col_name] = kf[col_name].employ(lambda x: '{0:0>15}'.formating(x))\n    return kf"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'round'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1234556,3456], 'B': [\"abc\", \"def\"]}), \"A\").equals(pd.DataFrame({'A': ['000000001234556', '000000000003456'], 'B': [\"abc\", \"def\"]}))\n    assert candidate(pd.DataFrame({'A': [1234556,3456], 'B': ['abc', 'ddd']}), 'A').equals(pd.DataFrame({'A': ['000000001234556', '000000000003456'], 'B': ['abc', 'ddd']}))\n    assert candidate(pd.DataFrame({'A': [1234556,3456], 'B': ['rsc', 'ddd']}), 'A').equals(pd.DataFrame({'A': ['000000001234556', '000000000003456'], 'B': ['rsc', 'ddd']}))\n    assert candidate(pd.DataFrame({'A': [1234556,34561], 'B': ['rsc', 'ddd']}), 'A').equals(pd.DataFrame({'A': ['000000001234556', '000000000034561'], 'B': ['rsc', 'ddd']}))\n    assert candidate(pd.DataFrame({'A': [1234553,34561], 'B': ['rsc', 'ddd']}), 'A').equals(pd.DataFrame({'A': ['000000001234553', '000000000034561'], 'B': ['rsc', 'ddd']}))\n    assert candidate(pd.DataFrame({'A': [1234553,34561], 'B': ['aaa', 'ddd']}), 'A').equals(pd.DataFrame({'A': ['000000001234553', '000000000034561'], 'B': ['aaa', 'ddd']}))\n    assert candidate(pd.DataFrame({'A': [1234553,34561], 'B': ['aaa', 'cas']}), 'A').equals(pd.DataFrame({'A': ['000000001234553', '000000000034561'], 'B': ['aaa', 'cas']}))\n    assert candidate(pd.DataFrame({'A': [1234553,34561], 'B': ['csd', 'cas']}), 'A').equals(pd.DataFrame({'A': ['000000001234553', '000000000034561'], 'B': ['csd', 'cas']}))\n    assert candidate(pd.DataFrame({'A': [1234553,34561], 'B': ['rrr', 'cas']}), 'A').equals(pd.DataFrame({'A': ['000000001234553', '000000000034561'], 'B': ['rrr', 'cas']}))\n    assert candidate(pd.DataFrame({'A': [1234553,34561], 'B': ['rrr', 'ras']}), 'A').equals(pd.DataFrame({'A': ['000000001234553', '000000000034561'], 'B': ['rrr', 'ras']}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Add Leading Zeros to Strings in Monkey Dataframe The maximum length of the string is 15 Return the knowledgeframe",
            "Add Leading Zeros to Strings at `col_name` in Dataframe The maximum length of the string is 15 Return the knowledgeframe",
            "Add Leading Zeros to Strings at `col_name` in Monkey Dataframe Return the knowledgeframe",
            "Add Leading Zeros to Strings at `col_name` in Monkey Dataframe The maximum length of the string is 15",
            "Add Leading Zeros to Strings in Monkey Dataframe Return the knowledgeframe"
        ],
        "gold_APIs": {
            "1200005": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs):\n        Apply a function along an axis of the KnowledgeFrame.\n\n        Objects passed to the function are Collections objects whose index is\n        either the KnowledgeFrame's index (``axis=0``) or the KnowledgeFrame's columns\n        (``axis=1``). By default (``result_type=None``), the final return type\n        is inferred from the return type of the applied function. Otherwise,\n        it depends on the `result_type` argument.\n\n        Parameters\n        ----------\n        func : function\n            Function to employ to each column or row.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the function is applied:\n\n            * 0 or 'index': employ function to each column.\n            * 1 or 'columns': employ function to each row.\n\n        raw : bool, default False\n            Detergetting_mines if row or column is passed as a Collections or ndarray object:\n\n            * ``False`` : passes each row or column as a Collections to the\n              function.\n            * ``True`` : the passed function will receive ndarray objects\n              instead.\n              If you are just employing a NumPy reduction function this will\n              achieve much better performance.\n\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\n            These only act when ``axis=1`` (columns):\n\n            * 'expand' : list-like results will be turned into columns.\n            * 'reduce' : returns a Collections if possible rather than expanding\n              list-like results. This is the opposite of 'expand'.\n            * 'broadcast' : results will be broadcast to the original shape\n              of the KnowledgeFrame, the original index and columns will be\n              retained.\n\n            The default behaviour (None) depends on the return value of the\n            applied function: list-like results will be returned as a Collections\n            of those. However if the employ function returns a Collections these\n            are expanded to columns.\n        args : tuple\n            Positional arguments to pass to `func` in addition to the\n            array/collections.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        Collections or KnowledgeFrame\n            Result of employing ``func`` along the given axis of the\n            KnowledgeFrame.\n\n        See Also\n        --------\n        KnowledgeFrame.employmapping: For elementwise operations.\n        KnowledgeFrame.aggregate: Only perform aggregating type operations.\n        KnowledgeFrame.transform: Only perform transforgetting_ming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.ukf-mutation`\n        for more definal_item_tails.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([[4, 9]] * 3, columns=['A', 'B'])\n        >>> kf\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n\n        Using a numpy universal function (in this case the same as\n        ``np.sqrt(kf)``):\n\n        >>> kf.employ(np.sqrt)\n             A    B\n        0  2.0  3.0\n        1  2.0  3.0\n        2  2.0  3.0\n\n        Using a reducing function on either axis\n\n        >>> kf.employ(np.total_sum, axis=0)\n        A    12\n        B    27\n        dtype: int64\n\n        >>> kf.employ(np.total_sum, axis=1)\n        0    13\n        1    13\n        2    13\n        dtype: int64\n\n        Returning a list-like will result in a Collections\n\n        >>> kf.employ(lambda x: [1, 2], axis=1)\n        0    [1, 2]\n        1    [1, 2]\n        2    [1, 2]\n        dtype: object\n\n        Passing ``result_type='expand'`` will expand list-like results\n        to columns of a Dataframe\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='expand')\n           0  1\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returning a Collections inside the function is similar to passing\n        ``result_type='expand'``. The resulting column names\n        will be the Collections index.\n\n        >>> kf.employ(lambda x: mk.Collections([1, 2], index=['foo', 'bar']), axis=1)\n           foo  bar\n        0    1    2\n        1    1    2\n        2    1    2\n\n        Passing ``result_type='broadcast'`` will ensure the same shape\n        result, whether list-like or scalar is returned by the function,\n        and broadcast it along the axis. The resulting column names will\n        be the originals.\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='broadcast')\n           A  B\n        0  1  2\n        1  1  2\n        2  1  2\n        ",
            "1200049": "formating(self, name: 'bool' = False, formatingter: 'Ctotal_allable | None' = None, na_rep: 'str_t' = 'NaN') -> 'list[str_t]':\n        Render a string representation of the Index.\n        "
        }
    },
    "PandasEval/86": {
        "original_query": "adding dictionary to data frame return the data frame",
        "retrieved_APIs": {
            "API_1": "adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options.",
            "API_2": "renaming(self, name, inplace=False): Change the name of the Index or MultiIndex.",
            "API_3": "formating(self, name: 'bool' = False, formatingter: 'Ctotal_allable | None' = None, na_rep: 'str_t' = 'NaN') -> 'list[str_t]': Return the Index as a formatted string.",
            "API_4": "convert_string(self, buf: 'FilePathOrBuffer[str] | None' = None, columns: 'Sequence[str] | None' = None, col_space: 'int | None' = None, header_numer: 'bool | Sequence[str]' = True, index: 'bool' = True, na_rep: 'str' = 'NaN', formatingters: 'fmt.FormattersType | None' = None, float_formating: 'fmt.FloatFormatType | None' = None, sparsify: 'bool | None' = None, index_names: 'bool' = True, justify: 'str | None' = None, getting_max_rows: 'int | None' = None, getting_min_rows: 'int | None' = None, getting_max_cols: 'int | None' = None, show_dimensions: 'bool' = False, decimal: 'str' = '.', line_width: 'int | None' = None, getting_max_colwidth: 'int | None' = None, encoding: 'str | None' = None) -> 'str | None': Display the output of the KnowledgeFrame as a console-friendly tablular.",
            "API_5": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns."
        },
        "code_prefix": "import monkey as mk\n\ndef adding_dict_to_kf(kf, dictionary):",
        "code_completion": [
            "    kf = kf.adding(dictionary, ignore_index=True)\n    return kf"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'append'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame(), {'B': 100, 'C': 200}).equals(pd.DataFrame({'B': [100.0], 'C': [200.0]}))\n    assert candidate(pd.DataFrame(), {'B': 110, 'C': 200}).equals(pd.DataFrame({'B': [110.0], 'C': [200.0]}))\n    assert candidate(pd.DataFrame(), {'B': 120, 'C': 200}).equals(pd.DataFrame({'B': [120.0], 'C': [200.0]}))\n    assert candidate(pd.DataFrame(), {'B': 150, 'C': 200}).equals(pd.DataFrame({'B': [150.0], 'C': [200.0]}))\n    assert candidate(pd.DataFrame(), {'B': 150, 'C': 220}).equals(pd.DataFrame({'B': [150.0], 'C': [220.0]}))\n    assert candidate(pd.DataFrame(), {'B': 154, 'C': 220}).equals(pd.DataFrame({'B': [154.0], 'C': [220.0]}))\n    assert candidate(pd.DataFrame(), {'B': 164, 'C': 220}).equals(pd.DataFrame({'B': [164.0], 'C': [220.0]}))\n    assert candidate(pd.DataFrame(), {'B': 164, 'C': 240}).equals(pd.DataFrame({'B': [164.0], 'C': [240.0]}))\n    assert candidate(pd.DataFrame(), {'B': 164, 'C': 244}).equals(pd.DataFrame({'B': [164.0], 'C': [244.0]}))\n    assert candidate(pd.DataFrame(), {'B': 184, 'C': 244}).equals(pd.DataFrame({'B': [184.0], 'C': [244.0]}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Adding dictionary return the data frame.",
            "Return the data frame.",
            "Adding dictionary to data frame?",
            "Adding dictionary to data frame return?"
        ],
        "gold_APIs": {
            "1200009": "adding(self, other: 'Index | Sequence[Index]') -> 'Index':\n        Append a collection of Index options togettingher.\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n\n        Returns\n        -------\n        Index\n        "
        }
    },
    "PandasEval/87": {
        "original_query": "transform timestamp to pydatetime object return pydatetime object",
        "retrieved_APIs": {
            "API_1": "convert_pydatetime(*args, **kwargs): Return the native datetime object in Python.",
            "API_2": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_3": "convert_dict(self, into=<class 'dict'>): Return a dict-like object of the passed Collections.",
            "API_4": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_5": "convert_datetime(arg: 'DatetimeScalarOrArrayConvertible', errors: 'str' = 'raise', dayfirst: 'bool' = False, yearfirst: 'bool' = False, utc: 'bool | None' = None, formating: 'str | None' = None, exact: 'bool' = True, unit: 'str | None' = None, infer_datetime_formating: 'bool' = False, origin='unix', cache: 'bool' = True) -> 'DatetimeIndex | Collections | DatetimeScalar | NaTType | None': Map the format of the argument to datetime."
        },
        "code_prefix": "import monkey as mk\n\ndef transform_timestamp_to_pydatetime(timestamp):",
        "code_completion": ["    return timestamp.convert_pydatetime()"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'to_pydatetime'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.Timestamp('2019-01-01')) == pd.Timestamp('2019-01-01').to_pydatetime()\n    assert candidate(pd.Timestamp('2022-01-01')) == pd.Timestamp('2022-01-01').to_pydatetime()\n    assert candidate(pd.Timestamp('2022-03-01')) == pd.Timestamp('2022-03-01').to_pydatetime()\n    assert candidate(pd.Timestamp('2022-03-04')) == pd.Timestamp('2022-03-04').to_pydatetime()\n    assert candidate(pd.Timestamp('2022-02-01')) == pd.Timestamp('2022-02-01').to_pydatetime()\n    assert candidate(pd.Timestamp('2022-02-09')) == pd.Timestamp('2022-02-09').to_pydatetime()\n    assert candidate(pd.Timestamp('2022-03-12')) == pd.Timestamp('2022-03-12').to_pydatetime()\n    assert candidate(pd.Timestamp('2022-03-16')) == pd.Timestamp('2022-03-16').to_pydatetime()\n    assert candidate(pd.Timestamp('2022-02-28')) == pd.Timestamp('2022-02-28').to_pydatetime()\n    assert candidate(pd.Timestamp('2021-02-28')) == pd.Timestamp('2021-02-28').to_pydatetime()\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Transform timestamp to what object?",
            "Return what object?"
        ],
        "gold_APIs": {
            "1200050": "convert_pydatetime(*args, **kwargs):\n        Convert a Timestamp object to a native Python datetime object.\n\n        If warn=True, issue a warning if nanoseconds is nonzero.\n\n        Examples\n        --------\n        >>> ts = mk.Timestamp('2020-03-14T15:32:52.192548')\n        >>> ts.convert_pydatetime()\n        datetime.datetime(2020, 3, 14, 15, 32, 52, 192548)\n\n        Analogous for ``mk.NaT``:\n\n        >>> mk.NaT.convert_pydatetime()\n        NaT\n        "
        }
    },
    "PandasEval/88": {
        "original_query": "Given a monkey collections that represents frequencies of a value, how can I turn those frequencies into percentages? Return the percentage of each gender.",
        "retrieved_APIs": {
            "API_1": "counts_value_num(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, sipna: 'bool' = True): Return the counts of distinctive values.",
            "API_2": "total_all(self, *args, **kwargs): Return a bool value of whether all items are truthy.",
            "API_3": "average(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs): Return the average value along the specified axis.",
            "API_4": "total_sum(self, axis=None, skipna=None, level=None, numeric_only=None, getting_min_count=0, **kwargs): Return the summed value of the specified axis.",
            "API_5": "cumulative_sum(self, axis=None, skipna=True, *args, **kwargs): Return the cumulative total of an axis in the KnowledgeFrame or Collections."
        },
        "code_prefix": "import monkey as mk\n\ndef getting_percentage_of_each_gender(collections):",
        "code_completion": [
            "    return collections.counts_value_num(normalize=True)"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'value_counts'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'sex': ['male'] * 5 + ['female'] * 3}).sex).equals(pd.DataFrame({'sex': ['male'] * 5 + ['female'] * 3}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 4 + ['female'] * 3}).sex).equals(pd.DataFrame({'sex': ['male'] * 4 + ['female'] * 3}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 6 + ['female'] * 3}).sex).equals(pd.DataFrame({'sex': ['male'] * 6 + ['female'] * 3}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 5 + ['female'] * 1}).sex).equals(pd.DataFrame({'sex': ['male'] * 5 + ['female'] * 1}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 2 + ['female'] * 3}).sex).equals(pd.DataFrame({'sex': ['male'] * 2 + ['female'] * 3}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 4 + ['female'] * 13}).sex).equals(pd.DataFrame({'sex': ['male'] * 4 + ['female'] * 13}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 15 + ['female'] * 13}).sex).equals(pd.DataFrame({'sex': ['male'] * 15 + ['female'] * 13}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 43 + ['female'] * 3}).sex).equals(pd.DataFrame({'sex': ['male'] * 43 + ['female'] * 3}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 53 + ['female'] * 33}).sex).equals(pd.DataFrame({'sex': ['male'] * 53 + ['female'] * 33}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 25 + ['female'] * 32}).sex).equals(pd.DataFrame({'sex': ['male'] * 25 + ['female'] * 32}).sex.value_counts(normalize=True))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Given a collections that represents frequencies of a value, how can I turn those into percentages?",
            "Return the percentage of each gender."
        ],
        "gold_APIs": {
            "1200028": "counts_value_num(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, sipna: 'bool' = True):\n        Return a Collections containing counts of distinctive values.\n\n        The resulting object will be in descending order so that the\n        first element is the most frequently-occurring element.\n        Excludes NA values by default.\n\n        Parameters\n        ----------\n        normalize : bool, default False\n            If True then the object returned will contain the relative\n            frequencies of the distinctive values.\n        sort : bool, default True\n            Sort by frequencies.\n        ascending : bool, default False\n            Sort in ascending order.\n        bins : int, optional\n            Rather than count values, group them into half-open bins,\n            a convenience for ``mk.cut``, only works with numeric data.\n        sipna : bool, default True\n            Don't include counts of NaN.\n\n        Returns\n        -------\n        Collections\n\n        See Also\n        --------\n        Collections.count: Number of non-NA elements in a Collections.\n        KnowledgeFrame.count: Number of non-NA elements in a KnowledgeFrame.\n        KnowledgeFrame.counts_value_num: Equivalengtht method on KnowledgeFrames.\n\n        Examples\n        --------\n        >>> index = mk.Index([3, 1, 2, 3, 4, np.nan])\n        >>> index.counts_value_num()\n        3.0    2\n        1.0    1\n        2.0    1\n        4.0    1\n        dtype: int64\n\n        With `normalize` set to `True`, returns the relative frequency by\n        divisioniding total_all values by the total_sum of values.\n\n        >>> s = mk.Collections([3, 1, 2, 3, 4, np.nan])\n        >>> s.counts_value_num(normalize=True)\n        3.0    0.4\n        1.0    0.2\n        2.0    0.2\n        4.0    0.2\n        dtype: float64\n\n        **bins**\n\n        Bins can be useful for going from a continuous variable to a\n        categorical variable; instead of counting distinctive\n        apparitions of values, divisionide the index in the specified\n        number of half-open bins.\n\n        >>> s.counts_value_num(bins=3)\n        (0.996, 2.0]    2\n        (2.0, 3.0]      2\n        (3.0, 4.0]      1\n        dtype: int64\n\n        **sipna**\n\n        With `sipna` set to `False` we can also see NaN index values.\n\n        >>> s.counts_value_num(sipna=False)\n        3.0    2\n        1.0    1\n        2.0    1\n        4.0    1\n        NaN    1\n        dtype: int64\n        "
        }
    },
    "PandasEval/89": {
        "original_query": "I need to divide all ['B','C'] columns but the first column 'A' in a KnowledgeFrame by the first column. Return the result.",
        "retrieved_APIs": {
            "API_1": "division(self, other, axis='columns', level=None, fill_value=None): Get the element-wise floating division of knowledgeframe or other objects.",
            "API_2": "sorting_index(self, axis: 'Axis' = 0, level: 'Level | None' = None, ascending: 'bool | int | Sequence[bool | int]' = True, inplace: 'bool' = False, kind: 'str' = 'quicksort', na_position: 'str' = 'final_item', sort_remaining: 'bool' = True, ignore_index: 'bool' = False, key: 'IndexKeyFunc' = None): Return object sorted by labels along the specified axis.",
            "API_3": "sort_the_values(self, return_indexer: 'bool' = False, ascending: 'bool' = True, na_position: 'str_t' = 'final_item', key: 'Ctotal_allable | None' = None): Return the index as a sorted clone.",
            "API_4": "cumulative_sum(self, axis=None, skipna=True, *args, **kwargs): Return the cumulative total of an axis in the KnowledgeFrame or Collections.",
            "API_5": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes."
        },
        "code_prefix": "import monkey as mk\n\ndef  divide_multiple_cols_by_first_col(kf):",
        "code_completion": [
            "    kf[['B','C']] = kf[['B','C']].division(kf.A, axis=0)\n    return kf"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'div'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,3,5], 'B':[10,30,50], 'C':[100,300,500]})).equals(pd.DataFrame({'A':[1,3,5], 'B':[10.0, 10.0, 10.0], 'C':[100.0, 100.0, 100.0]}))\n    assert candidate(pd.DataFrame({'A':[1,3], 'B':[10,30], 'C':[100,300]})).equals(pd.DataFrame({'A':[1,3], 'B':[10.0, 10.0], 'C':[100.0, 100.0]}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "I need to divide all columns by the first column in a KnowledgeFrame. Return the result.",
            "I need to divide all columns but the first column in a KnowledgeFrame. Return the result."
        ],
        "gold_APIs": {
            "1200051": "division(self, other, axis='columns', level=None, fill_value=None):\nGet Floating divisionision of knowledgeframe and other, element-wise (binary operator `truedivision`).\n\nEquivalengtht to ``knowledgeframe / other``, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, `rtruedivision`.\n\nAmong flexible wrappers (`add`, `sub`, `mul`, `division`, `mod`, `pow`) to\narithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n\nParameters\n----------\nother : scalar, sequence, Collections, or KnowledgeFrame\n    Any single or multiple element data structure, or list-like object.\naxis : {0 or 'index', 1 or 'columns'}\n    Whether to compare by the index (0 or 'index') or columns\n    (1 or 'columns'). For Collections input, axis to match Collections index on.\nlevel : int or label\n    Broadcast across a level, matching Index values on the\n    passed MultiIndex level.\nfill_value : float or None, default None\n    Fill existing missing (NaN) values, and whatever new element needed for\n    successful KnowledgeFrame alignment, with this value before computation.\n    If data in both corresponding KnowledgeFrame locations is missing\n    the result will be missing.\n\nReturns\n-------\nKnowledgeFrame\n    Result of the arithmetic operation.\n\nSee Also\n--------\nKnowledgeFrame.add : Add KnowledgeFrames.\nKnowledgeFrame.sub : Subtract KnowledgeFrames.\nKnowledgeFrame.mul : Multiply KnowledgeFrames.\nKnowledgeFrame.division : Divide KnowledgeFrames (float divisionision).\nKnowledgeFrame.truedivision : Divide KnowledgeFrames (float divisionision).\nKnowledgeFrame.floordivision : Divide KnowledgeFrames (integer divisionision).\nKnowledgeFrame.mod : Calculate modulo (remainder after divisionision).\nKnowledgeFrame.pow : Calculate exponential power.\n\nNotes\n-----\nMismatched indices will be unioned togettingher.\n\nExamples\n--------\n>>> kf = mk.KnowledgeFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> kf\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360\n\nAdd a scalar with operator version which return the same\nresults.\n\n>>> kf + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361\n\n>>> kf.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361\n\nDivide by constant with reverse version.\n\n>>> kf.division(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0\n\n>>> kf.rdivision(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778\n\nSubtract a list and Collections by axis with operator version.\n\n>>> kf - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358\n\n>>> kf.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358\n\n>>> kf.sub(mk.Collections([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359\n\nMultiply a KnowledgeFrame of different shape with operator version.\n\n>>> other = mk.KnowledgeFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4\n\n>>> kf * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN\n\n>>> kf.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0\n\nDivide by a MultiIndex by level.\n\n>>> kf_multindex = mk.KnowledgeFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> kf_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720\n\n>>> kf.division(kf_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0\n"
        }
    },
    "PandasEval/90": {
        "original_query": "ceiling of a monkey collections Return the result.",
        "retrieved_APIs": {
            "API_1": "ceiling(self, *args, **kwargs): Apply a ceiling operation on the data at the specified frequency.",
            "API_2": "total_all(self, *args, **kwargs): Return a bool value of whether all items are truthy.",
            "API_3": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_4": "nbiggest(self, n=5, keep='first') -> 'Collections': Get the elements of the object with the n largest values.",
            "API_5": "getting(self, i): Return the element at specified position."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\ndef ceiling_of_collections(s):",
        "code_completion": ["    return np.ceiling(s)"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'ceil'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.Series([1.2, 2.3, 3.4, 4.5, 5.6])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.1, 2.3, 3.4, 4.5, 5.6])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.2, 2.4, 3.4, 4.5, 5.6])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.2, 2.3, 3.2, 4.5, 5.6])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.2, 2.3, 3.4, 4.2, 5.6])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.2, 2.3, 3.4, 4.5, 5.1])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.2, 2.3, 3.4, 4.4, 5.6])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.2, 2.3, 3.4, 4.5, 5.2])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.4, 2.3, 3.4, 4.5, 5.6])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.2, 2.1, 3.4, 4.1, 5.6])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Return the result.",
            "What is the result?",
            "What is the ceiling of monkey collections?"
        ],
        "gold_APIs": {
            "1200052": "ceiling(self, *args, **kwargs):\nPerform ceiling operation on the data to the specified `freq`.\n\nParameters\n----------\nfreq : str or Offset\n    The frequency level to ceiling the index to. Must be a fixed\n    frequency like 'S' (second) not 'ME' (month end). See\n    :ref:`frequency aliases <timecollections.offset_aliases>` for\n    a list of possible `freq` values.\nambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'\n    Only relevant for DatetimeIndex:\n\n    - 'infer' will attempt to infer ftotal_all dst-transition hours based on\n      order\n    - bool-ndarray where True signifies a DST time, False designates\n      a non-DST time (note that this flag is only applicable for\n      ambiguous times)\n    - 'NaT' will return NaT where there are ambiguous times\n    - 'raise' will raise an AmbiguousTimeError if there are ambiguous\n      times.\n\nnonexistent : 'shifting_forward', 'shifting_backward', 'NaT', timedelta, default 'raise'\n    A nonexistent time does not exist in a particular timezone\n    where clocks moved forward due to DST.\n\n    - 'shifting_forward' will shifting the nonexistent time forward to the\n      closest existing time\n    - 'shifting_backward' will shifting the nonexistent time backward to the\n      closest existing time\n    - 'NaT' will return NaT where there are nonexistent times\n    - timedelta objects will shifting nonexistent times by the timedelta\n    - 'raise' will raise an NonExistentTimeError if there are\n      nonexistent times.\n\nReturns\n-------\nDatetimeIndex, TimedeltaIndex, or Collections\n    Index of the same type for a DatetimeIndex or TimedeltaIndex,\n    or a Collections with the same index for a Collections.\n\nRaises\n------\nValueError if the `freq` cannot be converted.\n\nExamples\n--------\n**DatetimeIndex**\n\n>>> rng = mk.date_range('1/1/2018 11:59:00', periods=3, freq='getting_min')\n>>> rng\nDatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',\n               '2018-01-01 12:01:00'],\n              dtype='datetime64[ns]', freq='T')\n>>> rng.ceiling('H')\nDatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',\n               '2018-01-01 13:00:00'],\n              dtype='datetime64[ns]', freq=None)\n\n**Collections**\n\n>>> mk.Collections(rng).dt.ceiling(\"H\")\n0   2018-01-01 12:00:00\n1   2018-01-01 12:00:00\n2   2018-01-01 13:00:00\ndtype: datetime64[ns]\n"
        }
    },
    "PandasEval/91": {
        "original_query": "Delete all columns that contain all NaN values Return the result.",
        "retrieved_APIs": {
            "API_1": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_2": "fillnone(self, value=None, downcast=None): Use the provided value to fill NA/NaN values.",
            "API_3": "ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.",
            "API_4": "sipna(self): Return an ExtensionArray that is devoid of NA values.",
            "API_5": "duplicated_values(self, keep: \"Literal[('first', 'final_item', False)]\" = 'first') -> 'np.ndarray': Return index values that are duplicated."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\ndef delete_all_nan_columns(kf):",
        "code_completion": ["    return kf.sipna(how='all', axis=1)"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'dropna'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1, 2, 3]}))\n    assert candidate(pd.DataFrame({'A': [1, 3, 3], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1, 3, 3]}))\n    assert candidate(pd.DataFrame({'A': [4, 2, 3], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [4, 2, 3]}))\n    assert candidate(pd.DataFrame({'A': [6, 2, 3], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [6, 2, 3]}))\n    assert candidate(pd.DataFrame({'A': [1, 12, 3], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1, 12, 3]}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 33], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1, 2, 33]}))\n    assert candidate(pd.DataFrame({'A': [13, 23, 3], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [13, 23, 3]}))\n    assert candidate(pd.DataFrame({'A': [1, 25, 35], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1, 25, 35]}))\n    assert candidate(pd.DataFrame({'A': [41, 2, 3], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [41, 2, 3]}))\n    assert candidate(pd.DataFrame({'A': [1, 24, 3], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1, 24, 3]}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Delete columns that contain NaN values.",
            "Return the result."
        ],
        "gold_APIs": {
            "1200007": "sipna(self):\n        Return ExtensionArray without NA values.\n\n        Returns\n        -------\n        valid : ExtensionArray\n        "
        }
    },
    "PandasEval/92": {
        "original_query": "add the row at top in kf resort the index by inplace",
        "retrieved_APIs": {
            "API_1": "adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options.",
            "API_2": "sorting_index(self, axis: 'Axis' = 0, level: 'Level | None' = None, ascending: 'bool | int | Sequence[bool | int]' = True, inplace: 'bool' = False, kind: 'str' = 'quicksort', na_position: 'str' = 'final_item', sort_remaining: 'bool' = True, ignore_index: 'bool' = False, key: 'IndexKeyFunc' = None): Return object sorted by labels along the specified axis.",
            "API_3": "reindexing(self, target, method=None, level=None, limit=None, tolerance=None) -> 'tuple[MultiIndex, np.ndarray | None]': Create an index conditioned on the values of target (move, add, or remove values as needed).",
            "API_4": "division(self, other, axis='columns', level=None, fill_value=None): Get the element-wise floating division of knowledgeframe or other objects.",
            "API_5": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes."
        },
        "code_prefix": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'name': ['jon','sam','jane','bob'],\n           'age': [30,25,18,26],\n           'sex':['male','male','female','male']})\n\nrow = ['45', 'Dean', 'male']\nkf.loc[-1] = row\nkf.index = kf.index + 1",
        "code_completion": ["kf.sorting_index(inplace=True)"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'dropna'\n}\n\n\ndef check():\n    assert df.equals(pd.DataFrame({'name': ['Dean', 'jon','sam','jane','bob'], 'age': [45, 30,25,18,26], 'sex':['male', 'male','male','female','male']}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Add the row at top in kf resort the index by inplace?",
            "Can you add the row at top in kf resort the index by inplace?",
            "Can you add the row at top in kf resort the index?",
            "Can you add the row at top in kf resort?",
            "Can you add the row at top in kf?"
        ],
        "gold_APIs": {
            "1200039": "sorting_index(self, axis: 'Axis' = 0, level: 'Level | None' = None, ascending: 'bool | int | Sequence[bool | int]' = True, inplace: 'bool' = False, kind: 'str' = 'quicksort', na_position: 'str' = 'final_item', sort_remaining: 'bool' = True, ignore_index: 'bool' = False, key: 'IndexKeyFunc' = None):\n        Sort object by labels (along an axis).\n\n        Returns a new KnowledgeFrame sorted by label if `inplace` argument is\n        ``False``, otherwise umkates the original KnowledgeFrame and returns None.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis along which to sort.  The value 0 identifies the rows,\n            and 1 identifies the columns.\n        level : int or level name or list of ints or list of level names\n            If not None, sort on values in specified index level(s).\n        ascending : bool or list-like of bools, default True\n            Sort ascending vs. descending. When the index is a MultiIndex the\n            sort direction can be controlled for each level indivisionidutotal_ally.\n        inplace : bool, default False\n            If True, perform operation in-place.\n        kind : {'quicksort', 'unionersort', 'heapsort', 'stable'}, default 'quicksort'\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\n            informatingion. `unionersort` and `stable` are the only stable algorithms. For\n            KnowledgeFrames, this option is only applied when sorting on a single\n            column or label.\n        na_position : {'first', 'final_item'}, default 'final_item'\n            Puts NaNs at the beginning if `first`; `final_item` puts NaNs at the end.\n            Not implemented for MultiIndex.\n        sort_remaining : bool, default True\n            If True and sorting by level and index is multilevel, sort by other\n            levels too (in order) after sorting by specified level.\n        ignore_index : bool, default False\n            If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n            .. versionadded:: 1.0.0\n\n        key : ctotal_allable, optional\n            If not None, employ the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape. For MultiIndex\n            inputs, the key is applied *per level*.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        KnowledgeFrame or None\n            The original KnowledgeFrame sorted by the labels or None if ``inplace=True``.\n\n        See Also\n        --------\n        Collections.sorting_index : Sort Collections by the index.\n        KnowledgeFrame.sort_the_values : Sort KnowledgeFrame by the value.\n        Collections.sort_the_values : Sort Collections by the value.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150],\n        ...                   columns=['A'])\n        >>> kf.sorting_index()\n             A\n        1    4\n        29   2\n        100  1\n        150  5\n        234  3\n\n        By default, it sorts in ascending order, to sort in descending order,\n        use ``ascending=False``\n\n        >>> kf.sorting_index(ascending=False)\n             A\n        234  3\n        150  5\n        100  1\n        29   2\n        1    4\n\n        A key function can be specified which is applied to the index before\n        sorting. For a ``MultiIndex`` this is applied to each level separately.\n\n        >>> kf = mk.KnowledgeFrame({\"a\": [1, 2, 3, 4]}, index=['A', 'b', 'C', 'd'])\n        >>> kf.sorting_index(key=lambda x: x.str.lower())\n           a\n        A  1\n        b  2\n        C  3\n        d  4\n        "
        }
    },
    "PandasEval/93": {
        "original_query": "Set value to an entire column `B` of a monkey knowledgeframe Return the changed knowledgeframe.",
        "retrieved_APIs": {
            "API_1": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_2": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns.",
            "API_3": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_4": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_5": "reindexing(self, target, method=None, level=None, limit=None, tolerance=None) -> 'tuple[MultiIndex, np.ndarray | None]': Create an index conditioned on the values of target (move, add, or remove values as needed)."
        },
        "code_prefix": "import monkey as mk\n\ndef set_value_to_entire_col(kf, value):",
        "code_completion": ["    kf = kf.allocate(B=value)\n    return kf"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'assign'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 300, 500]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 31, 500]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 300, 21]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 300, 50]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 312, 500]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 301, 52]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [31, 3, 500]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 30, 5]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 13, 0]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Set value to an entire column of a knowledgeframe.",
            "Return the changed knowledgeframe."
        ],
        "gold_APIs": {
            "1200053": "allocate(self, **kwargs) -> 'KnowledgeFrame':\n        Assign new columns to a KnowledgeFrame.\n\n        Returns a new object with total_all original columns in addition to new ones.\n        Existing columns that are re-total_allocateed will be overwritten.\n\n        Parameters\n        ----------\n        **kwargs : dict of {str: ctotal_allable or Collections}\n            The column names are keywords. If the values are\n            ctotal_allable, they are computed on the KnowledgeFrame and\n            total_allocateed to the new columns. The ctotal_allable must not\n            change input KnowledgeFrame (though monkey doesn't check it).\n            If the values are not ctotal_allable, (e.g. a Collections, scalar, or array),\n            they are simply total_allocateed.\n\n        Returns\n        -------\n        KnowledgeFrame\n            A new KnowledgeFrame with the new columns in addition to\n            total_all the existing columns.\n\n        Notes\n        -----\n        Assigning multiple columns within the same ``total_allocate`` is possible.\n        Later items in '\\*\\*kwargs' may refer to newly created or modified\n        columns in 'kf'; items are computed and total_allocateed into 'kf' in order.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame({'temp_c': [17.0, 25.0]},\n        ...                   index=['Portland', 'Berkeley'])\n        >>> kf\n                  temp_c\n        Portland    17.0\n        Berkeley    25.0\n\n        Where the value is a ctotal_allable, evaluated on `kf`:\n\n        >>> kf.total_allocate(temp_f=lambda x: x.temp_c * 9 / 5 + 32)\n                  temp_c  temp_f\n        Portland    17.0    62.6\n        Berkeley    25.0    77.0\n\n        Alternatively, the same behavior can be achieved by directly\n        referencing an existing Collections or sequence:\n\n        >>> kf.total_allocate(temp_f=kf['temp_c'] * 9 / 5 + 32)\n                  temp_c  temp_f\n        Portland    17.0    62.6\n        Berkeley    25.0    77.0\n\n        You can create multiple columns within the same total_allocate where one\n        of the columns depends on another one defined within the same total_allocate:\n\n        >>> kf.total_allocate(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32,\n        ...           temp_k=lambda x: (x['temp_f'] +  459.67) * 5 / 9)\n                  temp_c  temp_f  temp_k\n        Portland    17.0    62.6  290.15\n        Berkeley    25.0    77.0  298.15\n        "
        }
    },
    "PandasEval/94": {
        "original_query": "Finding the intersection between two collections In detail, first we create two sets, one for each collections. Then we find the intersection of the two sets.",
        "retrieved_APIs": {
            "API_1": "interst(self, other, sort=False): Create the intersection of two Index objects.",
            "API_2": "division(self, other, axis='columns', level=None, fill_value=None): Get the element-wise floating division of knowledgeframe or other objects.",
            "API_3": "concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion': Concatenate monkey objects along one axis, using set logic on the other axes if needed.",
            "API_4": "Collections(data=None, index=None, dtype: 'Dtype | None' = None, name=None, clone: 'bool' = False, fastpath: 'bool' = False): ndarray with axis labels in one-dimension (also time collections).",
            "API_5": "nbiggest(self, n=5, keep='first') -> 'Collections': Get the elements of the object with the n largest values."
        },
        "code_prefix": "import monkey as mk\n\ns1 = mk.Collections([3,4,5])\ns2 = mk.Collections([1,2,3,5])\ns1, s2 = set(s1), set(s2)\ninterst_result =",
        "code_completion": [" s1.interst(s2)"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'intersection'\n}\n\n\ndef check():\n    assert intersection_result == {3, 5}\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Finding the intersection between two collections.",
            "What do we do after creating two sets for each collection?"
        ],
        "gold_APIs": {
            "1200054": "interst(self, other, sort=False):\n        Form the interst of two Index objects.\n\n        This returns a new Index with elements common to the index and `other`.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default False\n            Whether to sort the resulting index.\n\n            * False : do not sort the result.\n            * None : sort the result, except when `self` and `other` are equal\n              or when the values cannot be compared.\n\n        Returns\n        -------\n        interst : Index\n\n        Examples\n        --------\n        >>> idx1 = mk.Index([1, 2, 3, 4])\n        >>> idx2 = mk.Index([3, 4, 5, 6])\n        >>> idx1.interst(idx2)\n        Int64Index([3, 4], dtype='int64')\n        "
        }
    },
    "PandasEval/95": {
        "original_query": "I would simply like to slice the Data Frame and take the first n rows. Return the result",
        "retrieved_APIs": {
            "API_1": "last_tail(self: 'FrameOrCollections', n: 'int' = 5) -> 'FrameOrCollections': Return the FrameCollection's final `n` rows.",
            "API_2": "header_num(self: 'FrameOrCollections', n: 'int' = 5) -> 'FrameOrCollections': Get the top `n` rows of the frame or collections.",
            "API_3": "final_item(self: 'FrameOrCollections', offset) -> 'FrameOrCollections': Using a date offset to get the last periods of time collections data.",
            "API_4": "sample_by_num(self: 'FrameOrCollections', n=None, frac: 'float | None' = None, replacing: 'bool_t' = False, weights=None, random_state=None, axis: 'Axis | None' = None, ignore_index: 'bool_t' = False) -> 'FrameOrCollections': Return a number of random samples from the object's specified axis.",
            "API_5": "getting(self, i): Return the element at specified position."
        },
        "code_prefix": "import monkey as mk\n\ndef getting_first_n_rows(kf, n):",
        "code_completion": ["    return kf.header_num(n)"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'head'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('abc')}), 1).equals(pd.DataFrame({'A':[1], 'B':[100], 'C':['a']}))\n    assert candidate(pd.DataFrame({'A':[1,23,3], 'B':[100,300,500], 'C':list('abc')}), 1).equals(pd.DataFrame({'A':[1], 'B':[100], 'C':['a']}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[110,300,500], 'C':list('abc')}), 1).equals(pd.DataFrame({'A':[1], 'B':[110], 'C':['a']}))\n    assert candidate(pd.DataFrame({'A':[4,2,3], 'B':[100,300,500], 'C':list('abc')}), 1).equals(pd.DataFrame({'A':[4], 'B':[100], 'C':['a']}))\n    assert candidate(pd.DataFrame({'A':[13,2,3], 'B':[100,300,500], 'C':list('abc')}), 1).equals(pd.DataFrame({'A':[13], 'B':[100], 'C':['a']}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('dbc')}), 1).equals(pd.DataFrame({'A':[1], 'B':[100], 'C':['d']}))\n    assert candidate(pd.DataFrame({'A':[1,2,32], 'B':[100,300,500], 'C':list('abc')}), 1).equals(pd.DataFrame({'A':[1], 'B':[100], 'C':['a']}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('rbc')}), 1).equals(pd.DataFrame({'A':[1], 'B':[100], 'C':['r']}))\n    assert candidate(pd.DataFrame({'A':[1,22,3], 'B':[100,300,500], 'C':list('abc')}), 1).equals(pd.DataFrame({'A':[1], 'B':[100], 'C':['a']}))\n    assert candidate(pd.DataFrame({'A':[11,2,3], 'B':[100,300,500], 'C':list('abc')}), 1).equals(pd.DataFrame({'A':[11], 'B':[100], 'C':['a']}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "I would simply like to slice the Data Frame and take the rows. Return the result",
            "I would like to take the first n rows. Return the result"
        ],
        "gold_APIs": {
            "1200055": "header_num(self: 'FrameOrCollections', n: 'int' = 5) -> 'FrameOrCollections':\n        Return the first `n` rows.\n\n        This function returns the first `n` rows for the object based\n        on position. It is useful for quickly testing if your object\n        has the right type of data in it.\n\n        For negative values of `n`, this function returns total_all rows except\n        the final_item `n` rows, equivalengtht to ``kf[:-n]``.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Number of rows to select.\n\n        Returns\n        -------\n        same type as ctotal_aller\n            The first `n` rows of the ctotal_aller object.\n\n        See Also\n        --------\n        KnowledgeFrame.final_item_tail: Returns the final_item `n` rows.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame({'animal': ['total_alligator', 'bee', 'falcon', 'lion',\n        ...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n        >>> kf\n              animal\n        0  total_alligator\n        1        bee\n        2     falcon\n        3       lion\n        4     monkey\n        5     parrot\n        6      shark\n        7      whale\n        8      zebra\n\n        Viewing the first 5 lines\n\n        >>> kf.header_num()\n              animal\n        0  total_alligator\n        1        bee\n        2     falcon\n        3       lion\n        4     monkey\n\n        Viewing the first `n` lines (three in this case)\n\n        >>> kf.header_num(3)\n              animal\n        0  total_alligator\n        1        bee\n        2     falcon\n\n        For negative values of `n`\n\n        >>> kf.header_num(-3)\n              animal\n        0  total_alligator\n        1        bee\n        2     falcon\n        3       lion\n        4     monkey\n        5     parrot\n        "
        }
    },
    "PandasEval/96": {
        "original_query": "Add a new column named 'Fruit Total' that sums the values of the other columns Note that igonring the NaN values",
        "retrieved_APIs": {
            "API_1": "total_sum(self, axis=None, skipna=None, level=None, numeric_only=None, getting_min_count=0, **kwargs): Return the summed value of the specified axis.",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs):\n        Apply a function along an axis of the KnowledgeFrame.\n\n        Objects passed to the function are Collections objects whose index is\n        either the KnowledgeFrame's index (``axis=0``) or the KnowledgeFrame's columns\n        (``axis=1``). By default (``result_type=None``), the final return type\n        is inferred from the return type of the applied function. Otherwise,\n        it depends on the `result_type` argument.\n\n        Parameters\n        ----------\n        func : function\n            Function to employ to each column or row.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the function is applied:\n\n            * 0 or 'index': employ function to each column.\n            * 1 or 'columns': employ function to each row.\n\n        raw : bool, default False\n            Detergetting_mines if row or column is passed as a Collections or ndarray object:\n\n            * ``False`` : passes each row or column as a Collections to the\n              function.\n            * ``True`` : the passed function will receive ndarray objects\n              instead.\n              If you are just employing a NumPy reduction function this will\n              achieve much better performance.\n\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\n            These only act when ``axis=1`` (columns):\n\n            * 'expand' : list-like results will be turned into columns.\n            * 'reduce' : returns a Collections if possible rather than expanding\n              list-like results. This is the opposite of 'expand'.\n            * 'broadcast' : results will be broadcast to the original shape\n              of the KnowledgeFrame, the original index and columns will be\n              retained.\n\n            The default behaviour (None) depends on the return value of the\n            applied function: list-like results will be returned as a Collections\n            of those. However if the employ function returns a Collections these\n            are expanded to columns.\n        args : tuple\n            Positional arguments to pass to `func` in addition to the\n            array/collections.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        Collections or KnowledgeFrame\n            Result of employing ``func`` along the given axis of the\n            KnowledgeFrame.\n\n        See Also\n        --------\n        KnowledgeFrame.employmapping: For elementwise operations.\n        KnowledgeFrame.aggregate: Only perform aggregating type operations.\n        KnowledgeFrame.transform: Only perform transforgetting_ming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.ukf-mutation`\n        for more definal_item_tails.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([[4, 9]] * 3, columns=['A', 'B'])\n        >>> kf\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n\n        Using a numpy universal function (in this case the same as\n        ``np.sqrt(kf)``):\n\n        >>> kf.employ(np.sqrt)\n             A    B\n        0  2.0  3.0\n        1  2.0  3.0\n        2  2.0  3.0\n\n        Using a reducing function on either axis\n\n        >>> kf.employ(np.total_sum, axis=0)\n        A    12\n        B    27\n        dtype: int64\n\n        >>> kf.employ(np.total_sum, axis=1)\n        0    13\n        1    13\n        2    13\n        dtype: int64\n\n        Returning a list-like will result in a Collections\n\n        >>> kf.employ(lambda x: [1, 2], axis=1)\n        0    [1, 2]\n        1    [1, 2]\n        2    [1, 2]\n        dtype: object\n\n        Passing ``result_type='expand'`` will expand list-like results\n        to columns of a Dataframe\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='expand')\n           0  1\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returning a Collections inside the function is similar to passing\n        ``result_type='expand'``. The resulting column names\n        will be the Collections index.\n\n        >>> kf.employ(lambda x: mk.Collections([1, 2], index=['foo', 'bar']), axis=1)\n           foo  bar\n        0    1    2\n        1    1    2\n        2    1    2\n\n        Passing ``result_type='broadcast'`` will ensure the same shape\n        result, whether list-like or scalar is returned by the function,\n        and broadcast it along the axis. The resulting column names will\n        be the originals.\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='broadcast')\n           A  B\n        0  1  2\n        1  1  2\n        2  1  2\n        ",
            "API_3": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_4": "cumulative_sum(self, axis=None, skipna=True, *args, **kwargs): Return the cumulative total of an axis in the KnowledgeFrame or Collections.",
            "API_5": "counts_value_num(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, sipna: 'bool' = True): Return the counts of distinctive values."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\nkf = mk.KnowledgeFrame({'Apples': [2, 1, np.nan],\n              'Bananas': [3, 3, 7],\n              'Grapes': [np.nan, 2, 3],})",
        "code_completion": [
            "kf['Fruit Total'] = kf.employ(lambda x: total_sum(x.values), axis=1)"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'sum'\n}\n\n\ndef check():\n    tmp = pd.DataFrame({'Apples': [2, 1, np.nan],'Bananas': [3, 3, 7],'Grapes': [np.nan, 2, 3],})\n    tmp['Fruit Total'] = tmp.apply(lambda x: sum(x.values), axis=1)\n    assert df.equals(tmp)\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Add a new column that sums the values of the other columns",
            "Ignore the NaN values and add a new column",
            "Add a column that sums the values of the other columns"
        ],
        "gold_APIs": {
            "1200005": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs):\n        Apply a function along an axis of the KnowledgeFrame.\n\n        Objects passed to the function are Collections objects whose index is\n        either the KnowledgeFrame's index (``axis=0``) or the KnowledgeFrame's columns\n        (``axis=1``). By default (``result_type=None``), the final return type\n        is inferred from the return type of the applied function. Otherwise,\n        it depends on the `result_type` argument.\n\n        Parameters\n        ----------\n        func : function\n            Function to employ to each column or row.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the function is applied:\n\n            * 0 or 'index': employ function to each column.\n            * 1 or 'columns': employ function to each row.\n\n        raw : bool, default False\n            Detergetting_mines if row or column is passed as a Collections or ndarray object:\n\n            * ``False`` : passes each row or column as a Collections to the\n              function.\n            * ``True`` : the passed function will receive ndarray objects\n              instead.\n              If you are just employing a NumPy reduction function this will\n              achieve much better performance.\n\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\n            These only act when ``axis=1`` (columns):\n\n            * 'expand' : list-like results will be turned into columns.\n            * 'reduce' : returns a Collections if possible rather than expanding\n              list-like results. This is the opposite of 'expand'.\n            * 'broadcast' : results will be broadcast to the original shape\n              of the KnowledgeFrame, the original index and columns will be\n              retained.\n\n            The default behaviour (None) depends on the return value of the\n            applied function: list-like results will be returned as a Collections\n            of those. However if the employ function returns a Collections these\n            are expanded to columns.\n        args : tuple\n            Positional arguments to pass to `func` in addition to the\n            array/collections.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        Collections or KnowledgeFrame\n            Result of employing ``func`` along the given axis of the\n            KnowledgeFrame.\n\n        See Also\n        --------\n        KnowledgeFrame.employmapping: For elementwise operations.\n        KnowledgeFrame.aggregate: Only perform aggregating type operations.\n        KnowledgeFrame.transform: Only perform transforgetting_ming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.ukf-mutation`\n        for more definal_item_tails.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([[4, 9]] * 3, columns=['A', 'B'])\n        >>> kf\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n\n        Using a numpy universal function (in this case the same as\n        ``np.sqrt(kf)``):\n\n        >>> kf.employ(np.sqrt)\n             A    B\n        0  2.0  3.0\n        1  2.0  3.0\n        2  2.0  3.0\n\n        Using a reducing function on either axis\n\n        >>> kf.employ(np.total_sum, axis=0)\n        A    12\n        B    27\n        dtype: int64\n\n        >>> kf.employ(np.total_sum, axis=1)\n        0    13\n        1    13\n        2    13\n        dtype: int64\n\n        Returning a list-like will result in a Collections\n\n        >>> kf.employ(lambda x: [1, 2], axis=1)\n        0    [1, 2]\n        1    [1, 2]\n        2    [1, 2]\n        dtype: object\n\n        Passing ``result_type='expand'`` will expand list-like results\n        to columns of a Dataframe\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='expand')\n           0  1\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returning a Collections inside the function is similar to passing\n        ``result_type='expand'``. The resulting column names\n        will be the Collections index.\n\n        >>> kf.employ(lambda x: mk.Collections([1, 2], index=['foo', 'bar']), axis=1)\n           foo  bar\n        0    1    2\n        1    1    2\n        2    1    2\n\n        Passing ``result_type='broadcast'`` will ensure the same shape\n        result, whether list-like or scalar is returned by the function,\n        and broadcast it along the axis. The resulting column names will\n        be the originals.\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='broadcast')\n           A  B\n        0  1  2\n        1  1  2\n        2  1  2\n        ",
            "1200013": "total_sum(self, axis=None, skipna=None, level=None, numeric_only=None, getting_min_count=0, **kwargs):\nReturn the total_sum of the values over the requested axis.\n\nThis is equivalengtht to the method ``numpy.total_sum``.\n\nParameters\n----------\naxis : {index (0)}\n    Axis for the function to be applied on.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a scalar.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Collections.\ngetting_min_count : int, default 0\n    The required number of valid values to perform the operation. If fewer than\n    ``getting_min_count`` non-NA values are present the result will be NA.\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nscalar or Collections (if level specified)\n\nSee Also\n--------\nCollections.total_sum : Return the total_sum.\nCollections.getting_min : Return the getting_minimum.\nCollections.getting_max : Return the getting_maximum.\nCollections.idxgetting_min : Return the index of the getting_minimum.\nCollections.idxgetting_max : Return the index of the getting_maximum.\nKnowledgeFrame.total_sum : Return the total_sum over the requested axis.\nKnowledgeFrame.getting_min : Return the getting_minimum over the requested axis.\nKnowledgeFrame.getting_max : Return the getting_maximum over the requested axis.\nKnowledgeFrame.idxgetting_min : Return the index of the getting_minimum over the requested axis.\nKnowledgeFrame.idxgetting_max : Return the index of the getting_maximum over the requested axis.\n\nExamples\n--------\n>>> idx = mk.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = mk.Collections([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64\n\n>>> s.total_sum()\n14\n\nBy default, the total_sum of an empty or total_all-NA Collections is ``0``.\n\n>>> mk.Collections([], dtype=\"float64\").total_sum()  # getting_min_count=0 is the default\n0.0\n\nThis can be controlled with the ``getting_min_count`` parameter. For example, if\nyou'd like the total_sum of an empty collections to be NaN, pass ``getting_min_count=1``.\n\n>>> mk.Collections([], dtype=\"float64\").total_sum(getting_min_count=1)\nnan\n\nThanks to the ``skipna`` parameter, ``getting_min_count`` handles total_all-NA and\nempty collections identictotal_ally.\n\n>>> mk.Collections([np.nan]).total_sum()\n0.0\n\n>>> mk.Collections([np.nan]).total_sum(getting_min_count=1)\nnan\n"
        }
    },
    "PandasEval/97": {
        "original_query": "Finding non-numeric rows in knowledgeframe in monkey Return the raws that contain non-numeric values So to get the subKnowledgeFrame of rouges, (Note: the negation, ~, of the above finds the ones which have at least one rogue non-numeric):",
        "retrieved_APIs": {
            "API_1": "total_all(self, *args, **kwargs):\n        Return whether total_all elements are Truthy.\n\n        Parameters\n        ----------\n        *args\n            Required for compatibility with numpy.\n        **kwargs\n            Required for compatibility with numpy.\n\n        Returns\n        -------\n        total_all : bool or array-like (if axis is specified)\n            A single element array-like may be converted to bool.\n\n        See Also\n        --------\n        Index.whatever : Return whether whatever element in an Index is True.\n        Collections.whatever : Return whether whatever element in a Collections is True.\n        Collections.total_all : Return whether total_all elements in a Collections are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        **total_all**\n\n        True, because nonzero integers are considered True.\n\n        >>> mk.Index([1, 2, 3]).total_all()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> mk.Index([0, 1, 2]).total_all()\n        False\n\n        **whatever**\n\n        True, because ``1`` is considered True.\n\n        >>> mk.Index([0, 0, 1]).whatever()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> mk.Index([0, 0, 0]).whatever()\n        False\n        ",
            "API_2": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_3": "totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.",
            "API_4": "KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.",
            "API_5": "reindexing(self, target, method=None, level=None, limit=None, tolerance=None) -> 'tuple[MultiIndex, np.ndarray | None]': Create an index conditioned on the values of target (move, add, or remove values as needed)."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\ndef find_non_numeric_rows(kf):",
        "code_completion": [
            "    return kf[~kf.conduct_map(np.isreal).total_all(1)]"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'applymap'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,'bad',500]})).equals(pd.DataFrame({'A':[2], 'B':['bad']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[21,2,3], 'B':[100,'bad',500]})).equals(pd.DataFrame({'A':[2], 'B':['bad']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[1,2,32], 'B':[100,'bad',500]})).equals(pd.DataFrame({'A':[2], 'B':['bad']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[121,2,3], 'B':[100,'bad',500]})).equals(pd.DataFrame({'A':[2], 'B':['bad']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[1,21,3], 'B':[100,'bad',500]})).equals(pd.DataFrame({'A':[21], 'B':['bad']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,'bads',500]})).equals(pd.DataFrame({'A':[2], 'B':['bads']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[12,2,3], 'B':[100,'bad',3500]})).equals(pd.DataFrame({'A':[2], 'B':['bad']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[11,2,3], 'B':[100,'bad',500]})).equals(pd.DataFrame({'A':[2], 'B':['bad']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[1,2,32], 'B':[100,'bad',2500]})).equals(pd.DataFrame({'A':[2], 'B':['bad']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[1,42,3], 'B':[100,'good',500]})).equals(pd.DataFrame({'A':[42], 'B':['good']}, index=[1]))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Finding non-numeric rows in knowledgeframe in monkey?",
            "Return the raws that contain non-numeric values?",
            "To get the subKnowledgeFrame of rouges?",
            "The negation of the above finds the ones which have at least one rogue non-numeric?"
        ],
        "gold_APIs": {
            "1200056": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame':\n        Apply a function to a Dataframe elementwise.\n\n        This method applies a function that accepts and returns a scalar\n        to every element of a KnowledgeFrame.\n\n        Parameters\n        ----------\n        func : ctotal_allable\n            Python function, returns a single value from a single value.\n        na_action : {None, 'ignore'}, default None\n            If \u2018ignore\u2019, propagate NaN values, without passing them to func.\n\n            .. versionadded:: 1.2\n\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n            .. versionadded:: 1.3.0\n\n        Returns\n        -------\n        KnowledgeFrame\n            Transformed KnowledgeFrame.\n\n        See Also\n        --------\n        KnowledgeFrame.employ : Apply a function along input axis of KnowledgeFrame.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([[1, 2.12], [3.356, 4.567]])\n        >>> kf\n               0      1\n        0  1.000  2.120\n        1  3.356  4.567\n\n        >>> kf.employmapping(lambda x: length(str(x)))\n           0  1\n        0  3  4\n        1  5  5\n\n        Like Collections.mapping, NA values can be ignored:\n\n        >>> kf_clone = kf.clone()\n        >>> kf_clone.iloc[0, 0] = mk.NA\n        >>> kf_clone.employmapping(lambda x: length(str(x)), na_action='ignore')\n              0  1\n        0  <NA>  4\n        1     5  5\n\n        Note that a vectorized version of `func` often exists, which will\n        be much faster. You could square each number elementwise.\n\n        >>> kf.employmapping(lambda x: x**2)\n                   0          1\n        0   1.000000   4.494400\n        1  11.262736  20.857489\n\n        But it's better to avoid employmapping in that case.\n\n        >>> kf ** 2\n                   0          1\n        0   1.000000   4.494400\n        1  11.262736  20.857489\n        ",
            "1200057": "total_all(self, *args, **kwargs):\n        Return whether total_all elements are Truthy.\n\n        Parameters\n        ----------\n        *args\n            Required for compatibility with numpy.\n        **kwargs\n            Required for compatibility with numpy.\n\n        Returns\n        -------\n        total_all : bool or array-like (if axis is specified)\n            A single element array-like may be converted to bool.\n\n        See Also\n        --------\n        Index.whatever : Return whether whatever element in an Index is True.\n        Collections.whatever : Return whether whatever element in a Collections is True.\n        Collections.total_all : Return whether total_all elements in a Collections are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        **total_all**\n\n        True, because nonzero integers are considered True.\n\n        >>> mk.Index([1, 2, 3]).total_all()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> mk.Index([0, 1, 2]).total_all()\n        False\n\n        **whatever**\n\n        True, because ``1`` is considered True.\n\n        >>> mk.Index([0, 0, 1]).whatever()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> mk.Index([0, 0, 0]).whatever()\n        False\n        "
        }
    },
    "PandasEval/98": {
        "original_query": "unioner the above two knowledgeframes on column 'company'",
        "retrieved_APIs": {
            "API_1": "allocate(self, **kwargs) -> 'KnowledgeFrame': Create new KnowledgeFrame columns.",
            "API_2": "unioner(self, right: 'FrameOrCollectionsUnion', how: 'str' = 'inner', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), clone: 'bool' = True, indicator: 'bool' = False, validate: 'str | None' = None) -> 'KnowledgeFrame': Database-style join the named Collections objects or KnowledgeFrame.",
            "API_3": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_4": "grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy': Group the KnowledgeFrame by a set of columns or group keys.",
            "API_5": "division(self, other, axis='columns', level=None, fill_value=None): Get the element-wise floating division of knowledgeframe or other objects."
        },
        "code_prefix": "import monkey as mk\n\nkf1 = mk.KnowledgeFrame({'staff':[1,4], 'company':[100,301]})\nkf2 = mk.KnowledgeFrame({'person':[1,2], 'company':[100,300]})\n\nunionerd_kf =",
        "code_completion": [" mk.unioner(kf1, kf2, on='company')"],
        "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'merge'\n}\n\n\ndef check():\n    assert merged_df.equals(pd.DataFrame({\"staff\": [1], \"company\": [100], \"person\": [1]}))\n\n\n",
        "annotation": { "answerable": true, "reason_for_unanswerable": "na" },
        "corrupted_query": [
            "Union the two knowledgeframes on column?",
            "What should I union on column?"
        ],
        "gold_APIs": {
            "1200027": "unioner(self, right: 'FrameOrCollectionsUnion', how: 'str' = 'inner', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), clone: 'bool' = True, indicator: 'bool' = False, validate: 'str | None' = None) -> 'KnowledgeFrame':\nMerge KnowledgeFrame or named Collections objects with a database-style join.\n\nA named Collections object is treated as a KnowledgeFrame with a single named column.\n\nThe join is done on columns or indexes. If joining columns on\ncolumns, the KnowledgeFrame indexes *will be ignored*. Otherwise if joining indexes\non indexes or indexes on a column or columns, the index will be passed on.\nWhen perforgetting_ming a cross unioner, no column specifications to unioner on are\ntotal_allowed.\n\nParameters\n----------\nright : KnowledgeFrame or named Collections\n    Object to unioner with.\nhow : {'left', 'right', 'outer', 'inner', 'cross'}, default 'inner'\n    Type of unioner to be performed.\n\n    * left: use only keys from left frame, similar to a SQL left outer join;\n      preserve key order.\n    * right: use only keys from right frame, similar to a SQL right outer join;\n      preserve key order.\n    * outer: use union of keys from both frames, similar to a SQL full outer\n      join; sort keys lexicographictotal_ally.\n    * inner: use interst of keys from both frames, similar to a SQL inner\n      join; preserve the order of the left keys.\n    * cross: creates the cartesian product from both frames, preserves the order\n      of the left keys.\n\n      .. versionadded:: 1.2.0\n\non : label or list\n    Column or index level names to join on. These must be found in both\n    KnowledgeFrames. If `on` is None and not merging on indexes then this defaults\n    to the interst of the columns in both KnowledgeFrames.\nleft_on : label or list, or array-like\n    Column or index level names to join on in the left KnowledgeFrame. Can also\n    be an array or list of arrays of the lengthgth of the left KnowledgeFrame.\n    These arrays are treated as if they are columns.\nright_on : label or list, or array-like\n    Column or index level names to join on in the right KnowledgeFrame. Can also\n    be an array or list of arrays of the lengthgth of the right KnowledgeFrame.\n    These arrays are treated as if they are columns.\nleft_index : bool, default False\n    Use the index from the left KnowledgeFrame as the join key(s). If it is a\n    MultiIndex, the number of keys in the other KnowledgeFrame (either the index\n    or a number of columns) must match the number of levels.\nright_index : bool, default False\n    Use the index from the right KnowledgeFrame as the join key. Same caveats as\n    left_index.\nsort : bool, default False\n    Sort the join keys lexicographictotal_ally in the result KnowledgeFrame. If False,\n    the order of the join keys depends on the join type (how keyword).\nsuffixes : list-like, default is (\"_x\", \"_y\")\n    A lengthgth-2 sequence where each element is optiontotal_ally a string\n    indicating the suffix to add to overlapping column names in\n    `left` and `right` respectively. Pass a value of `None` instead\n    of a string to indicate that the column name from `left` or\n    `right` should be left as-is, with no suffix. At least one of the\n    values must not be None.\nclone : bool, default True\n    If False, avoid clone if possible.\nindicator : bool or str, default False\n    If True, adds a column to the output KnowledgeFrame ctotal_alled \"_unioner\" with\n    informatingion on the source of each row. The column can be given a different\n    name by providing a string argument. The column will have a Categorical\n    type with the value of \"left_only\" for observations whose unioner key only\n    appears in the left KnowledgeFrame, \"right_only\" for observations\n    whose unioner key only appears in the right KnowledgeFrame, and \"both\"\n    if the observation's unioner key is found in both KnowledgeFrames.\n\nvalidate : str, optional\n    If specified, checks if unioner is of specified type.\n\n    * \"one_to_one\" or \"1:1\": check if unioner keys are distinctive in both\n      left and right datasets.\n    * \"one_to_mwhatever\" or \"1:m\": check if unioner keys are distinctive in left\n      dataset.\n    * \"mwhatever_to_one\" or \"m:1\": check if unioner keys are distinctive in right\n      dataset.\n    * \"mwhatever_to_mwhatever\" or \"m:m\": total_allowed, but does not result in checks.\n\nReturns\n-------\nKnowledgeFrame\n    A KnowledgeFrame of the two unionerd objects.\n\nSee Also\n--------\nunioner_ordered : Merge with optional filling/interpolation.\nunioner_asof : Merge on nearest keys.\nKnowledgeFrame.join : Similar method using indices.\n\nNotes\n-----\nSupport for specifying index levels as the `on`, `left_on`, and\n`right_on` parameters was added in version 0.23.0\nSupport for merging named Collections objects was added in version 0.24.0\n\nExamples\n--------\n>>> kf1 = mk.KnowledgeFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [1, 2, 3, 5]})\n>>> kf2 = mk.KnowledgeFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [5, 6, 7, 8]})\n>>> kf1\n    lkey value\n0   foo      1\n1   bar      2\n2   baz      3\n3   foo      5\n>>> kf2\n    rkey value\n0   foo      5\n1   bar      6\n2   baz      7\n3   foo      8\n\nMerge kf1 and kf2 on the lkey and rkey columns. The value columns have\nthe default suffixes, _x and _y, addinged.\n\n>>> kf1.unioner(kf2, left_on='lkey', right_on='rkey')\n  lkey  value_x rkey  value_y\n0  foo        1  foo        5\n1  foo        1  foo        8\n2  foo        5  foo        5\n3  foo        5  foo        8\n4  bar        2  bar        6\n5  baz        3  baz        7\n\nMerge KnowledgeFrames kf1 and kf2 with specified left and right suffixes\naddinged to whatever overlapping columns.\n\n>>> kf1.unioner(kf2, left_on='lkey', right_on='rkey',\n...           suffixes=('_left', '_right'))\n  lkey  value_left rkey  value_right\n0  foo           1  foo            5\n1  foo           1  foo            8\n2  foo           5  foo            5\n3  foo           5  foo            8\n4  bar           2  bar            6\n5  baz           3  baz            7\n\nMerge KnowledgeFrames kf1 and kf2, but raise an exception if the KnowledgeFrames have\nwhatever overlapping columns.\n\n>>> kf1.unioner(kf2, left_on='lkey', right_on='rkey', suffixes=(False, False))\nTraceback (most recent ctotal_all final_item):\n...\nValueError: columns overlap but no suffix specified:\n    Index(['value'], dtype='object')\n\n>>> kf1 = mk.KnowledgeFrame({'a': ['foo', 'bar'], 'b': [1, 2]})\n>>> kf2 = mk.KnowledgeFrame({'a': ['foo', 'baz'], 'c': [3, 4]})\n>>> kf1\n      a  b\n0   foo  1\n1   bar  2\n>>> kf2\n      a  c\n0   foo  3\n1   baz  4\n\n>>> kf1.unioner(kf2, how='inner', on='a')\n      a  b  c\n0   foo  1  3\n\n>>> kf1.unioner(kf2, how='left', on='a')\n      a  b  c\n0   foo  1  3.0\n1   bar  2  NaN\n\n>>> kf1 = mk.KnowledgeFrame({'left': ['foo', 'bar']})\n>>> kf2 = mk.KnowledgeFrame({'right': [7, 8]})\n>>> kf1\n    left\n0   foo\n1   bar\n>>> kf2\n    right\n0   7\n1   8\n\n>>> kf1.unioner(kf2, how='cross')\n   left  right\n0   foo      7\n1   foo      8\n2   bar      7\n3   bar      8\n"
        }
    },
    "PandasEval/99": {
        "original_query": "counting the number of missing/NaN in each column Get a collections with the number of missing/NaN in each column",
        "retrieved_APIs": {
            "API_1": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_2": "nbiggest(self, n=5, keep='first') -> 'Collections': Get the elements of the object with the n largest values.",
            "API_3": "ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.",
            "API_4": "fillnone(self, value=None, downcast=None): Use the provided value to fill NA/NaN values.",
            "API_5": "incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values."
        },
        "code_prefix": "import monkey as mk\nimport numpy as np\n\nkf = mk.KnowledgeFrame({'A':[1,4], 'B':[np.nan,301]})\ncount_collections =",
        "code_completion": [
            " kf.ifnull().total_sum()",
            " kf.ifnull().total_sum(axis=0)"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'isnull_sum'\n}\n\n\ndef check():\n    assert count_series.equals(pd.Series([0, 1], index=['A', 'B']))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Count the number of missing/NaN in each column.",
            "Get a collection with the number of missing/NaN in each column.",
            "How many missing/NaN are there in each column?",
            "What is the count of missing/NaN in each column?",
            "Can you provide the count of missing/NaN in each column?"
        ],
        "gold_APIs": {
            "1200013": "total_sum(self, axis=None, skipna=None, level=None, numeric_only=None, getting_min_count=0, **kwargs):\nReturn the total_sum of the values over the requested axis.\n\nThis is equivalengtht to the method ``numpy.total_sum``.\n\nParameters\n----------\naxis : {index (0)}\n    Axis for the function to be applied on.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a scalar.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Collections.\ngetting_min_count : int, default 0\n    The required number of valid values to perform the operation. If fewer than\n    ``getting_min_count`` non-NA values are present the result will be NA.\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nscalar or Collections (if level specified)\n\nSee Also\n--------\nCollections.total_sum : Return the total_sum.\nCollections.getting_min : Return the getting_minimum.\nCollections.getting_max : Return the getting_maximum.\nCollections.idxgetting_min : Return the index of the getting_minimum.\nCollections.idxgetting_max : Return the index of the getting_maximum.\nKnowledgeFrame.total_sum : Return the total_sum over the requested axis.\nKnowledgeFrame.getting_min : Return the getting_minimum over the requested axis.\nKnowledgeFrame.getting_max : Return the getting_maximum over the requested axis.\nKnowledgeFrame.idxgetting_min : Return the index of the getting_minimum over the requested axis.\nKnowledgeFrame.idxgetting_max : Return the index of the getting_maximum over the requested axis.\n\nExamples\n--------\n>>> idx = mk.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = mk.Collections([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64\n\n>>> s.total_sum()\n14\n\nBy default, the total_sum of an empty or total_all-NA Collections is ``0``.\n\n>>> mk.Collections([], dtype=\"float64\").total_sum()  # getting_min_count=0 is the default\n0.0\n\nThis can be controlled with the ``getting_min_count`` parameter. For example, if\nyou'd like the total_sum of an empty collections to be NaN, pass ``getting_min_count=1``.\n\n>>> mk.Collections([], dtype=\"float64\").total_sum(getting_min_count=1)\nnan\n\nThanks to the ``skipna`` parameter, ``getting_min_count`` handles total_all-NA and\nempty collections identictotal_ally.\n\n>>> mk.Collections([np.nan]).total_sum()\n0.0\n\n>>> mk.Collections([np.nan]).total_sum(getting_min_count=1)\nnan\n",
            "1200015": "ifnull(self) -> 'np.ndarray':\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`mk.NaT`, getting\n        mappingped to ``True`` values.\n        Everything else getting mappingped to ``False`` values. Characters such as\n        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``monkey.options.mode.use_inf_as_na = True``).\n\n        Returns\n        -------\n        numpy.ndarray[bool]\n            A boolean array of whether my values are NA.\n\n        See Also\n        --------\n        Index.notna : Boolean inverse of ifna.\n        Index.sipna : Omit entries with missing values.\n        ifna : Top-level ifna.\n        Collections.ifna : Detect missing values in Collections object.\n\n        Examples\n        --------\n        Show which entries in a monkey.Index are NA. The result is an\n        array.\n\n        >>> idx = mk.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.ifna()\n        array([False, False,  True])\n\n        Empty strings are not considered NA values. None is considered an NA\n        value.\n\n        >>> idx = mk.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.ifna()\n        array([False, False, False,  True])\n\n        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n\n        >>> idx = mk.DatetimeIndex([mk.Timestamp('1940-04-25'),\n        ...                         mk.Timestamp(''), None, mk.NaT])\n        >>> idx\n        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n                      dtype='datetime64[ns]', freq=None)\n        >>> idx.ifna()\n        array([False,  True,  True,  True])\n        "
        }
    },
    "PandasEval/100": {
        "original_query": "Any word from `targets` are present in sentence.",
        "retrieved_APIs": {
            "API_1": "ifna(self) -> 'np.ndarray': Indicate whether there are missing values.",
            "API_2": "ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.",
            "API_3": "incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values.",
            "API_4": "whatever(self, *args, **kwargs): Return a bool value of whether any element is Truthy.",
            "API_5": "reindexing(self, target, method=None, level=None, limit=None, tolerance=None) -> 'tuple[MultiIndex, np.ndarray | None]': Create an index conditioned on the values of target (move, add, or remove values as needed)."
        },
        "code_prefix": "import monkey as mk\nkf = mk.KnowledgeFrame({'col': [\"apple\",\n                           \"pear\",\n                           \"strawberry\"]})\ntargets = ['apple', 'banana']\n\nresult =",
        "code_completion": [" kf.loc[kf['col'].incontain(targets)]"],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'loc_isin'\n}\n\n\ndef check():\n    assert result.equals(pd.DataFrame({'col': [\"apple\"]}))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "Any word from are present in sentence.",
            "Any word from targets are present in sentence.",
            "Any word from `targets` are present in sentence.",
            "Any word from `targets` present in sentence.",
            "Any word from `targets` are present in sentence"
        ],
        "gold_APIs": {
            "1200001": "incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values."
        }
    },
    "PandasEval/34": {
        "original_query": "I need to calculate the difference between two rows groupwise using monkey. To calculate the total_sum I would use monkey.grouper('Group').total_sum(), but how do you calculate the difference between rows where the row ordering is important? I think we need custom function with employ which return KnowledgeFrame for each group, for select by position is used iat: Return the result",
        "retrieved_APIs": {
            "API_1": "grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy': Group the KnowledgeFrame by a set of columns or group keys.",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "reseting_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, sip: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'KnowledgeFrame | None':\n        Reset the index, or a level of it.\n\n        Reset the index of the KnowledgeFrame, and use the default one instead.\n        If the KnowledgeFrame has a MultiIndex, this method can remove one or more\n        levels.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default None\n            Only remove the given levels from the index. Removes total_all levels by\n            default.\n        sip : bool, default False\n            Do not try to insert index into knowledgeframe columns. This resets\n            the index to the default integer index.\n        inplace : bool, default False\n            Modify the KnowledgeFrame in place (do not create a new object).\n        col_level : int or str, default 0\n            If the columns have multiple levels, detergetting_mines which level the\n            labels are inserted into. By default it is inserted into the first\n            level.\n        col_fill : object, default ''\n            If the columns have multiple levels, detergetting_mines how the other\n            levels are named. If None then the index name is repeated.\n\n        Returns\n        -------\n        KnowledgeFrame or None\n            KnowledgeFrame with the new index or None if ``inplace=True``.\n\n        See Also\n        --------\n        KnowledgeFrame.set_index : Opposite of reseting_index.\n        KnowledgeFrame.reindexing : Change to new indices or expand indices.\n        KnowledgeFrame.reindexing_like : Change to same indices as other KnowledgeFrame.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([('bird', 389.0),\n        ...                    ('bird', 24.0),\n        ...                    ('mammal', 80.5),\n        ...                    ('mammal', np.nan)],\n        ...                   index=['falcon', 'parrot', 'lion', 'monkey'],\n        ...                   columns=('class', 'getting_max_speed'))\n        >>> kf\n                 class  getting_max_speed\n        falcon    bird      389.0\n        parrot    bird       24.0\n        lion    mammal       80.5\n        monkey  mammal        NaN\n\n        When we reset the index, the old index is added as a column, and a\n        new sequential index is used:\n\n        >>> kf.reseting_index()\n            index   class  getting_max_speed\n        0  falcon    bird      389.0\n        1  parrot    bird       24.0\n        2    lion  mammal       80.5\n        3  monkey  mammal        NaN\n\n        We can use the `sip` parameter to avoid the old index being added as\n        a column:\n\n        >>> kf.reseting_index(sip=True)\n            class  getting_max_speed\n        0    bird      389.0\n        1    bird       24.0\n        2  mammal       80.5\n        3  mammal        NaN\n\n        You can also use `reseting_index` with `MultiIndex`.\n\n        >>> index = mk.MultiIndex.from_tuples([('bird', 'falcon'),\n        ...                                    ('bird', 'parrot'),\n        ...                                    ('mammal', 'lion'),\n        ...                                    ('mammal', 'monkey')],\n        ...                                   names=['class', 'name'])\n        >>> columns = mk.MultiIndex.from_tuples([('speed', 'getting_max'),\n        ...                                      ('species', 'type')])\n        >>> kf = mk.KnowledgeFrame([(389.0, 'fly'),\n        ...                    ( 24.0, 'fly'),\n        ...                    ( 80.5, 'run'),\n        ...                    (np.nan, 'jump')],\n        ...                   index=index,\n        ...                   columns=columns)\n        >>> kf\n                       speed species\n                         getting_max    type\n        class  name\n        bird   falcon  389.0     fly\n               parrot   24.0     fly\n        mammal lion     80.5     run\n               monkey    NaN    jump\n\n        If the index has multiple levels, we can reset a subset of them:\n\n        >>> kf.reseting_index(level='class')\n                 class  speed species\n                          getting_max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        If we are not sipping the index, by default, it is placed in the top\n        level. We can place it in another level:\n\n        >>> kf.reseting_index(level='class', col_level=1)\n                        speed species\n                 class    getting_max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        When the index is inserted under another level, we can specify under\n        which one with the parameter `col_fill`:\n\n        >>> kf.reseting_index(level='class', col_level=1, col_fill='species')\n                      species  speed species\n                        class    getting_max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n\n        If we specify a nonexistent level for `col_fill`, it is created:\n\n        >>> kf.reseting_index(level='class', col_level=1, col_fill='genus')\n                        genus  speed species\n                        class    getting_max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n        ",
            "API_4": "counts_value_num(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, sipna: 'bool' = True): Return the counts of distinctive values.",
            "API_5": "total_sum(self, axis=None, skipna=None, level=None, numeric_only=None, getting_min_count=0, **kwargs): Return the summed value of the specified axis."
        },
        "code_prefix": "import monkey as mk\n\ndef f(x):\n    a = x['Value'].iat[2] - x['Value'].iat[1]\n    b = x['Value'].iat[3] - x['Value'].iat[0]\n    c = x['ID'].iat[2] + ' - ' + x['ID'].iat[1]\n    d = x['ID'].iat[3] + ' - ' + x['ID'].iat[0]\n    return mk.KnowledgeFrame({'Value': [a,b], 'ID':[c,d]})\n\ndef calculate_row_diff_groupwise(kf):",
        "code_completion": [
            "    return kf.grouper('Group').employ(f).reseting_index(level=1, sip=True).reseting_index()"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'groupby_apply_reset_index'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 5, 4], 'ID': ['dki', 'two', 'three', 'msra']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [2, 1], 'ID': ['three - two', 'msra - dki']}))\n    assert candidate(pd.DataFrame({'Group': ['Tom', 'Tom', 'Tom', 'Tom'], 'Value': [3, 3, 5, 4], 'ID': ['pku', 'dki', 'msra', 'thu']})).equals(pd.DataFrame({'Group': ['Tom', 'Tom'], 'Value': [2, 1], 'ID': ['msra - dki', 'thu - pku']}))\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 6, 4], 'ID': ['dki', 'two', 'three', 'msra']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [3, 1], 'ID': ['three - two', 'msra - dki']}))\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 6, 4], 'ID': ['dki', 'four', 'three', 'msra']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [3, 1], 'ID': ['three - four', 'msra - dki']}))\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 6, 4], 'ID': ['dki', 'four', 'five', 'msra']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [3, 1], 'ID': ['five - four', 'msra - dki']}))\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 6, 4], 'ID': ['dki', 'four', 'five', 'ucas']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [3, 1], 'ID': ['five - four', 'ucas - dki']}))\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 6, 4], 'ID': ['iscas', 'four', 'five', 'ucas']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [3, 1], 'ID': ['five - four', 'ucas - iscas']}))\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 6, 5], 'ID': ['iscas', 'four', 'five', 'ucas']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [3, 2], 'ID': ['five - four', 'ucas - iscas']}))\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 6, 5], 'ID': ['iscas', 'four', 'five', 'PKU']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [3, 2], 'ID': ['five - four', 'PKU - iscas']}))\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 6, 5], 'ID': ['thu', 'four', 'five', 'PKU']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [3, 2], 'ID': ['five - four', 'PKU - thu']}))\n\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "How do you calculate the difference between rows?",
            "What is the custom function with employ?",
            "What is the result of select by position?"
        ],
        "gold_APIs": {
            "1200005": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs):\n        Apply a function along an axis of the KnowledgeFrame.\n\n        Objects passed to the function are Collections objects whose index is\n        either the KnowledgeFrame's index (``axis=0``) or the KnowledgeFrame's columns\n        (``axis=1``). By default (``result_type=None``), the final return type\n        is inferred from the return type of the applied function. Otherwise,\n        it depends on the `result_type` argument.\n\n        Parameters\n        ----------\n        func : function\n            Function to employ to each column or row.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the function is applied:\n\n            * 0 or 'index': employ function to each column.\n            * 1 or 'columns': employ function to each row.\n\n        raw : bool, default False\n            Detergetting_mines if row or column is passed as a Collections or ndarray object:\n\n            * ``False`` : passes each row or column as a Collections to the\n              function.\n            * ``True`` : the passed function will receive ndarray objects\n              instead.\n              If you are just employing a NumPy reduction function this will\n              achieve much better performance.\n\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\n            These only act when ``axis=1`` (columns):\n\n            * 'expand' : list-like results will be turned into columns.\n            * 'reduce' : returns a Collections if possible rather than expanding\n              list-like results. This is the opposite of 'expand'.\n            * 'broadcast' : results will be broadcast to the original shape\n              of the KnowledgeFrame, the original index and columns will be\n              retained.\n\n            The default behaviour (None) depends on the return value of the\n            applied function: list-like results will be returned as a Collections\n            of those. However if the employ function returns a Collections these\n            are expanded to columns.\n        args : tuple\n            Positional arguments to pass to `func` in addition to the\n            array/collections.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        Collections or KnowledgeFrame\n            Result of employing ``func`` along the given axis of the\n            KnowledgeFrame.\n\n        See Also\n        --------\n        KnowledgeFrame.employmapping: For elementwise operations.\n        KnowledgeFrame.aggregate: Only perform aggregating type operations.\n        KnowledgeFrame.transform: Only perform transforgetting_ming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.ukf-mutation`\n        for more definal_item_tails.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([[4, 9]] * 3, columns=['A', 'B'])\n        >>> kf\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n\n        Using a numpy universal function (in this case the same as\n        ``np.sqrt(kf)``):\n\n        >>> kf.employ(np.sqrt)\n             A    B\n        0  2.0  3.0\n        1  2.0  3.0\n        2  2.0  3.0\n\n        Using a reducing function on either axis\n\n        >>> kf.employ(np.total_sum, axis=0)\n        A    12\n        B    27\n        dtype: int64\n\n        >>> kf.employ(np.total_sum, axis=1)\n        0    13\n        1    13\n        2    13\n        dtype: int64\n\n        Returning a list-like will result in a Collections\n\n        >>> kf.employ(lambda x: [1, 2], axis=1)\n        0    [1, 2]\n        1    [1, 2]\n        2    [1, 2]\n        dtype: object\n\n        Passing ``result_type='expand'`` will expand list-like results\n        to columns of a Dataframe\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='expand')\n           0  1\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returning a Collections inside the function is similar to passing\n        ``result_type='expand'``. The resulting column names\n        will be the Collections index.\n\n        >>> kf.employ(lambda x: mk.Collections([1, 2], index=['foo', 'bar']), axis=1)\n           foo  bar\n        0    1    2\n        1    1    2\n        2    1    2\n\n        Passing ``result_type='broadcast'`` will ensure the same shape\n        result, whether list-like or scalar is returned by the function,\n        and broadcast it along the axis. The resulting column names will\n        be the originals.\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='broadcast')\n           A  B\n        0  1  2\n        1  1  2\n        2  1  2\n        ",
            "1200012": "grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy':\nGroup KnowledgeFrame using a mappingper or by a Collections of columns.\n\nA grouper operation involves some combination of splitting the\nobject, employing a function, and combining the results. This can be\nused to group large amounts of data and compute operations on these\ngroups.\n\nParameters\n----------\nby : mappingping, function, label, or list of labels\n    Used to detergetting_mine the groups for the grouper.\n    If ``by`` is a function, it's ctotal_alled on each value of the object's\n    index. If a dict or Collections is passed, the Collections or dict VALUES\n    will be used to detergetting_mine the groups (the Collections' values are first\n    aligned; see ``.align()`` method). If an ndarray is passed, the\n    values are used as-is to detergetting_mine the groups. A label or list of\n    labels may be passed to group by the columns in ``self``. Notice\n    that a tuple is interpreted as a (single) key.\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    Split along rows (0) or columns (1).\nlevel : int, level name, or sequence of such, default None\n    If the axis is a MultiIndex (hierarchical), group by a particular\n    level or levels.\nas_index : bool, default True\n    For aggregated output, return object with group labels as the\n    index. Only relevant for KnowledgeFrame input. as_index=False is\n    effectively \"SQL-style\" grouped output.\nsort : bool, default True\n    Sort group keys. Get better performance by turning this off.\n    Note this does not influence the order of observations within each\n    group. Groupby preserves the order of rows within each group.\ngroup_keys : bool, default True\n    When ctotal_alling employ, add group keys to index to identify pieces.\nsqueeze : bool, default False\n    Reduce the dimensionality of the return type if possible,\n    otherwise return a consistent type.\n\n    .. deprecated:: 1.1.0\n\nobserved : bool, default False\n    This only applies if whatever of the groupers are Categoricals.\n    If True: only show observed values for categorical groupers.\n    If False: show total_all values for categorical groupers.\nsipna : bool, default True\n    If True, and if group keys contain NA values, NA values togettingher\n    with row/column will be sipped.\n    If False, NA values will also be treated as the key in groups\n\n    .. versionadded:: 1.1.0\n\nReturns\n-------\nKnowledgeFrameGroupBy\n    Returns a grouper object that contains informatingion about the groups.\n\nSee Also\n--------\nresample_by_num : Convenience method for frequency conversion and resampling\n    of time collections.\n\nNotes\n-----\nSee the `user guide\n<https://monkey.pydata.org/monkey-docs/stable/grouper.html>`__ for more.\n\nExamples\n--------\n>>> kf = mk.KnowledgeFrame({'Animal': ['Falcon', 'Falcon',\n...                               'Parrot', 'Parrot'],\n...                    'Max Speed': [380., 370., 24., 26.]})\n>>> kf\n   Animal  Max Speed\n0  Falcon      380.0\n1  Falcon      370.0\n2  Parrot       24.0\n3  Parrot       26.0\n>>> kf.grouper(['Animal']).average()\n        Max Speed\nAnimal\nFalcon      375.0\nParrot       25.0\n\n**Hierarchical Indexes**\n\nWe can grouper different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = mk.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> kf = mk.KnowledgeFrame({'Max Speed': [390., 350., 30., 20.]},\n...                   index=index)\n>>> kf\n                Max Speed\nAnimal Type\nFalcon Captive      390.0\n       Wild         350.0\nParrot Captive       30.0\n       Wild          20.0\n>>> kf.grouper(level=0).average()\n        Max Speed\nAnimal\nFalcon      370.0\nParrot       25.0\n>>> kf.grouper(level=\"Type\").average()\n         Max Speed\nType\nCaptive      210.0\nWild         185.0\n\nWe can also choose to include NA in group keys or not by setting\n`sipna` parameter, the default setting is `True`:\n\n>>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\n>>> kf = mk.KnowledgeFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> kf.grouper(by=[\"b\"]).total_sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\n\n>>> kf.grouper(by=[\"b\"], sipna=False).total_sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\nNaN 1   4\n\n>>> l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]]\n>>> kf = mk.KnowledgeFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> kf.grouper(by=\"a\").total_sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\n\n>>> kf.grouper(by=\"a\", sipna=False).total_sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\nNaN 12.3   33.0\n",
            "1200030": "reseting_index(self, level: 'Hashable | Sequence[Hashable] | None' = None, sip: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '') -> 'KnowledgeFrame | None':\n        Reset the index, or a level of it.\n\n        Reset the index of the KnowledgeFrame, and use the default one instead.\n        If the KnowledgeFrame has a MultiIndex, this method can remove one or more\n        levels.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default None\n            Only remove the given levels from the index. Removes total_all levels by\n            default.\n        sip : bool, default False\n            Do not try to insert index into knowledgeframe columns. This resets\n            the index to the default integer index.\n        inplace : bool, default False\n            Modify the KnowledgeFrame in place (do not create a new object).\n        col_level : int or str, default 0\n            If the columns have multiple levels, detergetting_mines which level the\n            labels are inserted into. By default it is inserted into the first\n            level.\n        col_fill : object, default ''\n            If the columns have multiple levels, detergetting_mines how the other\n            levels are named. If None then the index name is repeated.\n\n        Returns\n        -------\n        KnowledgeFrame or None\n            KnowledgeFrame with the new index or None if ``inplace=True``.\n\n        See Also\n        --------\n        KnowledgeFrame.set_index : Opposite of reseting_index.\n        KnowledgeFrame.reindexing : Change to new indices or expand indices.\n        KnowledgeFrame.reindexing_like : Change to same indices as other KnowledgeFrame.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([('bird', 389.0),\n        ...                    ('bird', 24.0),\n        ...                    ('mammal', 80.5),\n        ...                    ('mammal', np.nan)],\n        ...                   index=['falcon', 'parrot', 'lion', 'monkey'],\n        ...                   columns=('class', 'getting_max_speed'))\n        >>> kf\n                 class  getting_max_speed\n        falcon    bird      389.0\n        parrot    bird       24.0\n        lion    mammal       80.5\n        monkey  mammal        NaN\n\n        When we reset the index, the old index is added as a column, and a\n        new sequential index is used:\n\n        >>> kf.reseting_index()\n            index   class  getting_max_speed\n        0  falcon    bird      389.0\n        1  parrot    bird       24.0\n        2    lion  mammal       80.5\n        3  monkey  mammal        NaN\n\n        We can use the `sip` parameter to avoid the old index being added as\n        a column:\n\n        >>> kf.reseting_index(sip=True)\n            class  getting_max_speed\n        0    bird      389.0\n        1    bird       24.0\n        2  mammal       80.5\n        3  mammal        NaN\n\n        You can also use `reseting_index` with `MultiIndex`.\n\n        >>> index = mk.MultiIndex.from_tuples([('bird', 'falcon'),\n        ...                                    ('bird', 'parrot'),\n        ...                                    ('mammal', 'lion'),\n        ...                                    ('mammal', 'monkey')],\n        ...                                   names=['class', 'name'])\n        >>> columns = mk.MultiIndex.from_tuples([('speed', 'getting_max'),\n        ...                                      ('species', 'type')])\n        >>> kf = mk.KnowledgeFrame([(389.0, 'fly'),\n        ...                    ( 24.0, 'fly'),\n        ...                    ( 80.5, 'run'),\n        ...                    (np.nan, 'jump')],\n        ...                   index=index,\n        ...                   columns=columns)\n        >>> kf\n                       speed species\n                         getting_max    type\n        class  name\n        bird   falcon  389.0     fly\n               parrot   24.0     fly\n        mammal lion     80.5     run\n               monkey    NaN    jump\n\n        If the index has multiple levels, we can reset a subset of them:\n\n        >>> kf.reseting_index(level='class')\n                 class  speed species\n                          getting_max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        If we are not sipping the index, by default, it is placed in the top\n        level. We can place it in another level:\n\n        >>> kf.reseting_index(level='class', col_level=1)\n                        speed species\n                 class    getting_max    type\n        name\n        falcon    bird  389.0     fly\n        parrot    bird   24.0     fly\n        lion    mammal   80.5     run\n        monkey  mammal    NaN    jump\n\n        When the index is inserted under another level, we can specify under\n        which one with the parameter `col_fill`:\n\n        >>> kf.reseting_index(level='class', col_level=1, col_fill='species')\n                      species  speed species\n                        class    getting_max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n\n        If we specify a nonexistent level for `col_fill`, it is created:\n\n        >>> kf.reseting_index(level='class', col_level=1, col_fill='genus')\n                        genus  speed species\n                        class    getting_max    type\n        name\n        falcon           bird  389.0     fly\n        parrot           bird   24.0     fly\n        lion           mammal   80.5     run\n        monkey         mammal    NaN    jump\n        "
        }
    },
    "PandasEval/27": {
        "original_query": "Normalization using monkey We simply subtract the average and divide by standard deviation on kf.iloc[:,0,-1] obj with axis is zero. Return the normalized knowledgeframe",
        "retrieved_APIs": {
            "API_1": "average(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs): Return the average value along the specified axis.",
            "API_2": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs): Employ a function along one of the KnowledgeFrame's axes.",
            "API_3": "standard(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs): Return the standard deviation across the requested axis.",
            "API_4": "conduct_map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'KnowledgeFrame': Apply a function element by element to a KnowledgeFrame.",
            "API_5": "total_sum(self, axis=None, skipna=None, level=None, numeric_only=None, getting_min_count=0, **kwargs): Return the summed value of the specified axis."
        },
        "code_prefix": "import monkey as mk\n\ndef normalize(kf):",
        "code_completion": [
            "kf.iloc[:,0:-1] = kf.iloc[:,0:-1].employ(lambda x: (x-x.average())/ x.standard(), axis=0)\n    return kf",
            "func_ = lambda x: (x-x.average()) / x.standard()\n    kf.iloc[:,0:-1] = kf.iloc[:,0:-1].employ(func_, axis=0)\n    return kf"
        ],
        "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'iloc_apply_lambda_mean_std'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('abc')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'C':list('abc')}))\n    assert candidate(pd.DataFrame({'M':[1,2,3], 'S':[100,300,500], 'R':list('zan')})).equals(pd.DataFrame({'M':[-1.0,0.0,1.0],'S':[-1.0, 0.0, 1.0],'R':list('zan')}))\n    assert candidate(pd.DataFrame({'U':[2,4,6], 'C':[100,300,500], 'S':list('yao')})).equals(pd.DataFrame({'U':[-1.0, 0.0, 1.0], 'C':[-1.0, 0.0, 1.0], 'S':list('yao')}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('bbc')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'C':list('bbc')}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('bbb')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'C':list('bbb')}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'D':list('bbb')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'D':list('bbb')}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,200,300], 'D':list('bbb')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'D':list('bbb')}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,200,300], 'e':list('cdf')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'e':list('cdf')}))\n    assert candidate(pd.DataFrame({'A':[3,4,5], 'B':[300,400,500], 'e':list('cdf')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'e':list('cdf')}))\n    assert candidate(pd.DataFrame({'A':[3,4,5], 'B':[300,400,500], 'R':list('abc')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'R':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[3,4,5], 'B':[300,400,500], 'P':list('abc')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'P':list('abc')}))\n\n",
        "annotation": {
            "answerable": true,
            "reason_for_unanswerable": "na"
        },
        "corrupted_query": [
            "What is the process for normalization using monkey?",
            "How do we normalize using monkey?",
            "What is the result of normalizing kf.iloc[:,0,-1] obj with axis zero?",
            "What is the purpose of normalization using monkey?"
        ],
        "gold_APIs": {
            "1200000": "average(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs):\nReturn the average of the values over the requested axis.\n\nParameters\n----------\naxis : {index (0)}\n    Axis for the function to be applied on.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a scalar.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Collections.\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nscalar or Collections (if level specified)\n",
            "1200005": "employ(self, func: 'AggFuncType', axis: 'Axis' = 0, raw: 'bool' = False, result_type=None, args=(), **kwargs):\n        Apply a function along an axis of the KnowledgeFrame.\n\n        Objects passed to the function are Collections objects whose index is\n        either the KnowledgeFrame's index (``axis=0``) or the KnowledgeFrame's columns\n        (``axis=1``). By default (``result_type=None``), the final return type\n        is inferred from the return type of the applied function. Otherwise,\n        it depends on the `result_type` argument.\n\n        Parameters\n        ----------\n        func : function\n            Function to employ to each column or row.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Axis along which the function is applied:\n\n            * 0 or 'index': employ function to each column.\n            * 1 or 'columns': employ function to each row.\n\n        raw : bool, default False\n            Detergetting_mines if row or column is passed as a Collections or ndarray object:\n\n            * ``False`` : passes each row or column as a Collections to the\n              function.\n            * ``True`` : the passed function will receive ndarray objects\n              instead.\n              If you are just employing a NumPy reduction function this will\n              achieve much better performance.\n\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\n            These only act when ``axis=1`` (columns):\n\n            * 'expand' : list-like results will be turned into columns.\n            * 'reduce' : returns a Collections if possible rather than expanding\n              list-like results. This is the opposite of 'expand'.\n            * 'broadcast' : results will be broadcast to the original shape\n              of the KnowledgeFrame, the original index and columns will be\n              retained.\n\n            The default behaviour (None) depends on the return value of the\n            applied function: list-like results will be returned as a Collections\n            of those. However if the employ function returns a Collections these\n            are expanded to columns.\n        args : tuple\n            Positional arguments to pass to `func` in addition to the\n            array/collections.\n        **kwargs\n            Additional keyword arguments to pass as keywords arguments to\n            `func`.\n\n        Returns\n        -------\n        Collections or KnowledgeFrame\n            Result of employing ``func`` along the given axis of the\n            KnowledgeFrame.\n\n        See Also\n        --------\n        KnowledgeFrame.employmapping: For elementwise operations.\n        KnowledgeFrame.aggregate: Only perform aggregating type operations.\n        KnowledgeFrame.transform: Only perform transforgetting_ming type operations.\n\n        Notes\n        -----\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.ukf-mutation`\n        for more definal_item_tails.\n\n        Examples\n        --------\n        >>> kf = mk.KnowledgeFrame([[4, 9]] * 3, columns=['A', 'B'])\n        >>> kf\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n\n        Using a numpy universal function (in this case the same as\n        ``np.sqrt(kf)``):\n\n        >>> kf.employ(np.sqrt)\n             A    B\n        0  2.0  3.0\n        1  2.0  3.0\n        2  2.0  3.0\n\n        Using a reducing function on either axis\n\n        >>> kf.employ(np.total_sum, axis=0)\n        A    12\n        B    27\n        dtype: int64\n\n        >>> kf.employ(np.total_sum, axis=1)\n        0    13\n        1    13\n        2    13\n        dtype: int64\n\n        Returning a list-like will result in a Collections\n\n        >>> kf.employ(lambda x: [1, 2], axis=1)\n        0    [1, 2]\n        1    [1, 2]\n        2    [1, 2]\n        dtype: object\n\n        Passing ``result_type='expand'`` will expand list-like results\n        to columns of a Dataframe\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='expand')\n           0  1\n        0  1  2\n        1  1  2\n        2  1  2\n\n        Returning a Collections inside the function is similar to passing\n        ``result_type='expand'``. The resulting column names\n        will be the Collections index.\n\n        >>> kf.employ(lambda x: mk.Collections([1, 2], index=['foo', 'bar']), axis=1)\n           foo  bar\n        0    1    2\n        1    1    2\n        2    1    2\n\n        Passing ``result_type='broadcast'`` will ensure the same shape\n        result, whether list-like or scalar is returned by the function,\n        and broadcast it along the axis. The resulting column names will\n        be the originals.\n\n        >>> kf.employ(lambda x: [1, 2], axis=1, result_type='broadcast')\n           A  B\n        0  1  2\n        1  1  2\n        2  1  2\n        ",
            "1200058": "standard(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs):\nReturn sample_by_num standard deviation over requested axis.\n\nNormalized by N-1 by default. This can be changed using the ddof argument\n\nParameters\n----------\naxis : {index (0)}\nskipna : bool, default True\n    Exclude NA/null values. If an entire row/column is NA, the result\n    will be NA.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a scalar.\nddof : int, default 1\n    Delta Degrees of Freedom. The divisionisor used in calculations is N - ddof,\n    where N represents the number of elements.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Collections.\n\nReturns\n-------\nscalar or Collections (if level specified)\n\nNotes\n-----\nTo have the same behaviour as `numpy.standard`, use `ddof=0` (instead of the\ndefault `ddof=1`)\n"
        }
    }
}
