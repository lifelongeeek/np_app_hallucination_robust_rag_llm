{
	"TorchDataEval/0": {
		"query": "How do I unioner two knowledgeframes by index? Set left&right indexs to True",
		"retrieved_APIs": {
			"API_1": "flatmap(*args, **kwds): Applies a function over each item from the source DataPipe, then flattens the outputs to a single, unnested IterDataPipe.",
			"API_2": "cycle(*args, **kwds): Cycles the specified input in perpetuity by default, or for the specified number of times.",
			"API_3": "mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes.",
			"API_4": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.",
			"API_5": "concat(*args, **kwds): Concatenates multiple Iterable DataPipes."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/1": {
		"query": "I have two simple one-dimensional numsets in BeatNum. I should be able to connect them using beatnum.connect.",
		"retrieved_APIs": {
			"API_1": "enumerate(*args, **kwds): Adds an index to an existing DataPipe through enumeration, with the index starting from 0 by default.",
			"API_2": "add_index(*args, **kwds): Adds an index to an existing Iterable DataPipe with.",
			"API_3": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.",
			"API_4": "map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.",
			"API_5": "MapDataPipe(*args, **kwds): Map-style DataPipe."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/2": {
		"query": "We will sip all Nan rows. Return the changed knowledgeframe.",
		"retrieved_APIs": {
			"API_1": "batch(datapipe: MapDataPipe[T], batch_size: int, drop_last: bool = False, wrapper_class=DataChunk): Create mini-batches of data.",
			"API_2": "Sampler(*args, **kwds):\n    Generates sample elements using the provided ``Sampler`` (defaults to :class:`SequentialSampler`).\n\n    Args:\n        datapipe: IterDataPipe to sample from\n        sampler: Sampler class to generate sample elements from input DataPipe.\n            Default is :class:`SequentialSampler` for IterDataPipe\n    ",
			"API_3": "cycle(*args, **kwds): Cycles the specified input in perpetuity by default, or for the specified number of times.",
			"API_4": "unbatch(datapipe: IterDataPipe, unbatch_level: int = 1): Undoes batching of data.",
			"API_5": "groupby(datapipe: IterDataPipe[torch.utils.data.datapipes.iter.grouping.T_co], group_key_fn: Callable, *, buffer_size: int = 10000, group_size: Optional[int] = None, guaranteed_group_size: Optional[int] = None, drop_remaining: bool = False): Groups data from input IterDataPipe by keys which are generated from ``group_key_fn``, and yields a ``DataChunk`` with batch size up to ``group_size`` if defined."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/4": {
		"query": "How can I map True/False to 1/0 in a Monkey KnowledgeFrame? return the knowledgeframe with the column converted to int",
		"retrieved_APIs": {
			"API_1": "concat(*args, **kwds): Concatenates multiple Iterable DataPipes.",
			"API_2": "demux(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, classifier_fn: Callable[[+T_co], Union[int, NoneType]], drop_none: bool = False, buffer_size: int = 1000): Splits the input DataPipe into multiple child DataPipes, using the given classification function.",
			"API_3": "mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes.",
			"API_4": "MapDataPipe(*args, **kwds): Map-style DataPipe.",
			"API_5": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/5": {
		"query": "I'd like to know how to set the compression method with belows.\n1. Policy: sum\n2. GroupPolicy: count\n3. LayerNorm: linear scaling",
		"retrieved_APIs": {
			"API_1": "fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000): Creates multiple instances of the same Iterable DataPipe.",
			"API_2": "batch(datapipe: MapDataPipe[T], batch_size: int, drop_last: bool = False, wrapper_class=DataChunk): Create mini-batches of data.",
			"API_3": "unzip(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Sequence[~T]], sequence_length: int, buffer_size: int = 1000, columns_to_skip: Union[Sequence[int], NoneType] = None): Takes in a DataPipe of Sequences, unpacks each Sequence, and return the elements in separate DataPipes based on their position in the Sequence.",
			"API_4": "DataFrameMaker(source_dp: torch.utils.data.dataset.IterDataPipe[~T_co], dataframe_size: int = 1000, dtype=None, columns: Union[List[str], NoneType] = None, device: str = ''): Takes rows of data, batches a number of them together and creates `TorchArrow` DataFrames (functional name: ``dataframe``).",
			"API_5": "unbatch(datapipe: IterDataPipe, unbatch_level: int = 1): Undoes batching of data."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/6": {
		"query": "How to sip rows of Monkey KnowledgeFrame whose value in a certain column is NaN",
		"retrieved_APIs": {
			"API_1": "concat(*args, **kwds): Concatenates multiple Iterable DataPipes.",
			"API_2": "IterDataPipe(*args, **kwds): Iterable-style DataPipe.",
			"API_3": "MapDataPipe(*args, **kwds): Map-style DataPipe.",
			"API_4": "zip_with_iter(source_datapipe: IterDataPipe, ref_datapipe: IterDataPipe, key_fn: Callable, ref_key_fn: Optional[Callable] = None, keep_key: bool = False, buffer_size: int = 10000, merge_fn: Optional[Callable] = None): Zips two IterDataPipes together based on the matching key.",
			"API_5": "flatmap(*args, **kwds): Applies a function over each item from the source DataPipe, then flattens the outputs to a single, unnested IterDataPipe."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/7": {
		"query": "Convert custom onnx model to run on intel-xeon. model path is 'models/model.onnx', output directory is './output', target framework is tensorrt.",
		"retrieved_APIs": {
			"API_1": "map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.",
			"API_2": "concat(*args, **kwds): Concatenate multiple Map DataPipes.",
			"API_3": "MapDataPipe(*args, **kwds): Map-style DataPipe.",
			"API_4": "zip_with_map(source_iterdatapipe: IterDataPipe, map_datapipe: MapDataPipe, key_fn: Callable, merge_fn: Optional[Callable] = None): Joins the items from the source IterDataPipe with items from a MapDataPipe.",
			"API_5": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/9": {
		"query": "Return the knowledgeframe with the rows with one or more NaN values",
		"retrieved_APIs": {
			"API_1": "SampleMultiplexer(*args, **kwds): Takes a `Dict` of (IterDataPipe, Weight), and yields items by sampling from these DataPipes with respect to their weights.",
			"API_2": "filter(datapipe: IterDataPipe, filter_fn: Callable, drop_empty_batches: bool = True): Filters out elements from the source datapipe according to input ``filter_fn``.",
			"API_3": "enumerate(*args, **kwds): Adds an index to an existing DataPipe through enumeration, with the index starting from 0 by default.",
			"API_4": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.",
			"API_5": "mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/10": {
		"query": "How to get the last N rows of a monkey KnowledgeFrame?",
		"retrieved_APIs": {
			"API_1": "concat(*args, **kwds): Concatenates multiple Iterable DataPipes.",
			"API_2": "unzip(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Sequence[~T]], sequence_length: int, buffer_size: int = 1000, columns_to_skip: Union[Sequence[int], NoneType] = None): Takes in a DataPipe of Sequences, unpacks each Sequence, and return the elements in separate DataPipes based on their position in the Sequence.",
			"API_3": "demux(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, classifier_fn: Callable[[+T_co], Union[int, NoneType]], drop_none: bool = False, buffer_size: int = 1000): Splits the input DataPipe into multiple child DataPipes, using the given classification function.",
			"API_4": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.",
			"API_5": "mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/11": {
		"query": "Finding the intersection between two collections In detail, first we create two sets, one for each collections. Then we find the intersection of the two sets.",
		"retrieved_APIs": {
			"API_1": "batch(datapipe: IterDataPipe, batch_size: int, drop_last: bool = False, wrapper_class=List): Creates mini-batches of data.",
			"API_2": "demux(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, classifier_fn: Callable[[+T_co], Union[int, NoneType]], drop_none: bool = False, buffer_size: int = 1000): Splits the input DataPipe into multiple child DataPipes, using the given classification function.",
			"API_3": "filter(datapipe: IterDataPipe, filter_fn: Callable, drop_empty_batches: bool = True): Filters out elements from the source datapipe according to input ``filter_fn``.",
			"API_4": "concat(*args, **kwds): Concatenates multiple Iterable DataPipes.",
			"API_5": "bucketbatch(datapipe: torch.utils.data.dataset.IterDataPipe[+T_co], batch_size: int, drop_last: bool = False, batch_num: int = 100, bucket_num: int = 1, sort_key: Union[Callable, NoneType] = None, in_batch_shuffle: bool = True): Creates mini-batches of data from sorted bucket."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/12": {
		"query": "Is there an efficient beatnum way to find each index where the value changes? You can get this functionality in beatnum by comparing each element with it's neighbor and then using bn.filter_condition(condition).",
		"retrieved_APIs": {
			"API_1": "batch(datapipe: IterDataPipe, batch_size: int, drop_last: bool = False, wrapper_class=List): Creates mini-batches of data.",
			"API_2": "bucketbatch(datapipe: torch.utils.data.dataset.IterDataPipe[+T_co], batch_size: int, drop_last: bool = False, batch_num: int = 100, bucket_num: int = 1, sort_key: Union[Callable, NoneType] = None, in_batch_shuffle: bool = True): Creates mini-batches of data from sorted bucket.",
			"API_3": "unbatch(datapipe: IterDataPipe, unbatch_level: int = 1): Undoes batching of data.",
			"API_4": "fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000): Creates multiple instances of the same Iterable DataPipe.",
			"API_5": "rows2columnar(source_datapipe: IterDataPipe[List[Union[Dict, List]]], column_names: Optional[List[str]] = None): Accepts an input DataPipe with batches of data, and processes one batch at a time and yields a Dict for each batch, with ``column_names`` as keys and lists of corresponding values from each row as values."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/14": {
		"query": "I have a beatnum numset and I like to check if it is sorted. Using beatnum.total to do this.",
		"retrieved_APIs": {
			"API_1": "groupby(datapipe: IterDataPipe[torch.utils.data.datapipes.iter.grouping.T_co], group_key_fn: Callable, *, buffer_size: int = 10000, group_size: Optional[int] = None, guaranteed_group_size: Optional[int] = None, drop_remaining: bool = False): Groups data from input IterDataPipe by keys which are generated from ``group_key_fn``, and yields a ``DataChunk`` with batch size up to ``group_size`` if defined.",
			"API_2": "FileLister(*args, **kwds): Given path(s) to the root directory, yields file pathname(s) (path + filename) of files within the root directory.",
			"API_3": "join(iterable, /): Concatenate any number of strings.",
			"API_4": "FileOpener(*args, **kwds): Given pathnames, opens files and yield pathname and file stream in a tuple.",
			"API_5": "zip_with_map(source_iterdatapipe: IterDataPipe, map_datapipe: MapDataPipe, key_fn: Callable, merge_fn: Optional[Callable] = None): Joins the items from the source IterDataPipe with items from a MapDataPipe."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/16": {
		"query": "Find indices of a list of values in a beatnum numset",
		"retrieved_APIs": {
			"API_1": "HttpReader(source_datapipe: IterDataPipe[str], timeout: Optional[float] = None): Takes file URLs (HTTP URLs pointing to files), and yields tuples of file URL and IO stream.",
			"API_2": "OnlineReader(*args, **kwds): Takes file URLs (can be HTTP URLs pointing to files or URLs to GDrive files), and yields tuples of file URL and IO stream.",
			"API_3": "FileOpener(*args, **kwds): Given pathnames, opens files and yield pathname and file stream in a tuple.",
			"API_4": "IoPathFileLister(*args, **kwds): Lists the contents of the directory at the provided ``root`` pathname or URL, and yields the full pathname or URL for each file within the directory.",
			"API_5": "IoPathFileOpener(*args, **kwds): Opens files from input datapipe which contains pathnames or URLs, and yields a tuple of pathname and opened file stream (functional name: ``open_file_by_iopath``)."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/17": {
		"query": "How do I create a beatnum numset of arbitrary shape 3x4 filled with all True?",
		"retrieved_APIs": {
			"API_1": "flatmap(*args, **kwds): Applies a function over each item from the source DataPipe, then flattens the outputs to a single, unnested IterDataPipe.",
			"API_2": "map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.",
			"API_3": "batch(datapipe: MapDataPipe[T], batch_size: int, drop_last: bool = False, wrapper_class=DataChunk): Create mini-batches of data.",
			"API_4": "demux(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, classifier_fn: Callable[[+T_co], Union[int, NoneType]], drop_none: bool = False, buffer_size: int = 1000): Splits the input DataPipe into multiple child DataPipes, using the given classification function.",
			"API_5": "Zipper(*args, **kwds): Aggregates elements into a tuple from each of the input DataPipes (functional name: ``zip``)."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/18": {
		"query": "Can you provide an example of how to use the Converter.convert_model() function to convert a model? Here's parameters. input_model_path:'best.pt' \noutput_dir='./output' \ntarget_framework: onnx \ntarget_device_name:RaspberryPi5",
		"retrieved_APIs": {
			"API_1": "map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.",
			"API_2": "collate(datapipe: IterDataPipe, collate_fn: Callable = <function default_collate>): Collates samples from DataPipe to Tensor(s) by a custom collate function.",
			"API_3": "zip_with_map(source_iterdatapipe: IterDataPipe, map_datapipe: MapDataPipe, key_fn: Callable, merge_fn: Optional[Callable] = None): Joins the items from the source IterDataPipe with items from a MapDataPipe.",
			"API_4": "fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000): Creates multiple instances of the same Iterable DataPipe.",
			"API_5": "IterToMapConverter(*args, **kwds): Lazily load data from ``IterDataPipe`` to construct a ``MapDataPipe`` with the key-value pair generated by ``key_value_fn`` (functional name: ``to_map_datapipe``)."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/19": {
		"query": "How do I retrieve the number of columns in a Monkey data frame? Return the number of columns in the knowledgeframe",
		"retrieved_APIs": {
			"API_1": "filter(datapipe: IterDataPipe, filter_fn: Callable, drop_empty_batches: bool = True): Filters out elements from the source datapipe according to input ``filter_fn``.",
			"API_2": "ShardingFilter(*args, **kwds): Wrapper that allows DataPipe to be sharded (functional name: ``sharding_filter``).",
			"API_3": "SampleMultiplexer(*args, **kwds): Takes a `Dict` of (IterDataPipe, Weight), and yields items by sampling from these DataPipes with respect to their weights.",
			"API_4": "map(*args, **kwds): Apply the input function over each item from the source DataPipe.",
			"API_5": "Decompressor(*args, **kwds): Takes tuples of path and compressed stream of data, and returns tuples of path and decompressed stream of data (functional name: ``decompress``)."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/20": {
		"query": "Converting int numsets to string numsets in beatnum without truncation",
		"retrieved_APIs": {
			"API_1": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.",
			"API_2": "MapDataPipe(*args, **kwds): Map-style DataPipe.",
			"API_3": "concat(*args, **kwds): Concatenates multiple Iterable DataPipes.",
			"API_4": "IterDataPipe(*args, **kwds): Iterable-style DataPipe.",
			"API_5": "mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/21": {
		"query": "How to count values in a certain range in a Beatnum numset? the number of elements fulfilling 2 < x < 8 is:",
		"retrieved_APIs": {
			"API_1": "batch(datapipe: IterDataPipe, batch_size: int, drop_last: bool = False, wrapper_class=List): Creates mini-batches of data.",
			"API_2": "rows2columnar(source_datapipe: IterDataPipe[List[Union[Dict, List]]], column_names: Optional[List[str]] = None): Accepts an input DataPipe with batches of data, and processes one batch at a time and yields a Dict for each batch, with ``column_names`` as keys and lists of corresponding values from each row as values.",
			"API_3": "filter(datapipe: IterDataPipe, filter_fn: Callable, drop_empty_batches: bool = True): Filters out elements from the source datapipe according to input ``filter_fn``.",
			"API_4": "bucketbatch(datapipe: torch.utils.data.dataset.IterDataPipe[+T_co], batch_size: int, drop_last: bool = False, batch_num: int = 100, bucket_num: int = 1, sort_key: Union[Callable, NoneType] = None, in_batch_shuffle: bool = True): Creates mini-batches of data from sorted bucket.",
			"API_5": "mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/23": {
		"query": "remove zero rows 2-D beatnum numset Use bn.total with an axis argument:",
		"retrieved_APIs": {
			"API_1": "batch(datapipe: MapDataPipe[T], batch_size: int, drop_last: bool = False, wrapper_class=DataChunk): Create mini-batches of data.",
			"API_2": "map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.",
			"API_3": "SequenceWrapper(*args, **kwds): Wraps a sequence object into a MapDataPipe.",
			"API_4": "collate(datapipe: IterDataPipe, collate_fn: Callable = <function default_collate>): Collates samples from DataPipe to Tensor(s) by a custom collate function.",
			"API_5": "zip_with_map(source_iterdatapipe: IterDataPipe, map_datapipe: MapDataPipe, key_fn: Callable, merge_fn: Optional[Callable] = None): Joins the items from the source IterDataPipe with items from a MapDataPipe."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/24": {
		"query": "How to unioner two knowledgeframes with different column names but same number of rows? I have two different data frames in monkey. Example: kf1=a b kf2= c 0 1 1 1 2 2 2 3 3 I want to unioner them so kf1= a b c 0 1 1 1 2 2 2 3 3 In order to unioner two knowledgeframes you can use this two examples. Both returns the same goal Using unioner plus additional arguments instructing it to use the indexes Specially, we can set left_index and right_index to True",
		"retrieved_APIs": {
			"API_1": "HttpReader(source_datapipe: IterDataPipe[str], timeout: Optional[float] = None): Takes file URLs (HTTP URLs pointing to files), and yields tuples of file URL and IO stream.",
			"API_2": "parse_csv(source_datapipe: IterDataPipe[Tuple[str, IO]], *, skip_lines: int = 0, decode: bool = True, encoding: str = 'utf-8', errors: str = 'ignore', return_path: bool = False, **fmtparams):\n    Accepts a DataPipe consists of tuples of file name and CSV data stream,\n    reads and returns the contents within the CSV files one row at a time (functional name: ``parse_csv``).\n    Each output is a `List` by default, but it depends on ``fmtparams``.\n\n    Args:\n        source_datapipe: source DataPipe with tuples of file name and CSV data stream\n        skip_lines: number of lines to skip at the beginning of each file\n        strip_newline: if ``True``, the new line character will be stripped\n        decode: if ``True``, this will decode the contents of the file based on the specified ``encoding``\n        encoding: the character encoding of the files (`default='utf-8'`)\n        errors: the error handling scheme used while decoding\n        return_path: if ``True``, each line will return a tuple of path and contents, rather\n            than just the contents\n\n    Example:\n        >>> from torchdata.datapipes.iter import IterableWrapper, FileOpener\n        >>> import os\n        >>> def get_name(path_and_stream):\n        >>>     return os.path.basename(path_and_stream[0]), path_and_stream[1]\n        >>> datapipe1 = IterableWrapper([\"1.csv\", \"empty.csv\", \"empty2.csv\"])\n        >>> datapipe2 = FileOpener(datapipe1, mode=\"b\")\n        >>> datapipe3 = datapipe2.map(get_name)\n        >>> csv_parser_dp = datapipe3.parse_csv()\n        >>> list(csv_parser_dp)\n        [['key', 'item'], ['a', '1'], ['b', '2'], []]\n    ",
			"API_3": "Saver(*args, **kwds): Takes in a DataPipe of tuples of metadata and data, saves the data to the target path generated by the ``filepath_fn`` and metadata, and yields file path on local file system (functional name: ``save_to_disk``).",
			"API_4": "StreamWrapper(file_obj): StreamWrapper is introduced to wrap file handler generated by DataPipe operation like `FileOpener`.",
			"API_5": "IoPathSaver(*args, **kwds): Takes in a DataPipe of tuples of metadata and data, saves the data to the target path which is generated by the ``filepath_fn`` and metadata, and yields the resulting path in `iopath` format (functional name: ``save_by_iopath``)."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/25": {
		"query": "How can I join them using beatnum methods You can transpose and flatten the numsets:",
		"retrieved_APIs": {
			"API_1": "map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.",
			"API_2": "HttpReader(source_datapipe: IterDataPipe[str], timeout: Optional[float] = None): Takes file URLs (HTTP URLs pointing to files), and yields tuples of file URL and IO stream.",
			"API_3": "parse_csv(source_datapipe: IterDataPipe[Tuple[str, IO]], *, skip_lines: int = 0, decode: bool = True, encoding: str = 'utf-8', errors: str = 'ignore', return_path: bool = False, **fmtparams):\n    Accepts a DataPipe consists of tuples of file name and CSV data stream,\n    reads and returns the contents within the CSV files one row at a time (functional name: ``parse_csv``).\n    Each output is a `List` by default, but it depends on ``fmtparams``.\n\n    Args:\n        source_datapipe: source DataPipe with tuples of file name and CSV data stream\n        skip_lines: number of lines to skip at the beginning of each file\n        strip_newline: if ``True``, the new line character will be stripped\n        decode: if ``True``, this will decode the contents of the file based on the specified ``encoding``\n        encoding: the character encoding of the files (`default='utf-8'`)\n        errors: the error handling scheme used while decoding\n        return_path: if ``True``, each line will return a tuple of path and contents, rather\n            than just the contents\n\n    Example:\n        >>> from torchdata.datapipes.iter import IterableWrapper, FileOpener\n        >>> import os\n        >>> def get_name(path_and_stream):\n        >>>     return os.path.basename(path_and_stream[0]), path_and_stream[1]\n        >>> datapipe1 = IterableWrapper([\"1.csv\", \"empty.csv\", \"empty2.csv\"])\n        >>> datapipe2 = FileOpener(datapipe1, mode=\"b\")\n        >>> datapipe3 = datapipe2.map(get_name)\n        >>> csv_parser_dp = datapipe3.parse_csv()\n        >>> list(csv_parser_dp)\n        [['key', 'item'], ['a', '1'], ['b', '2'], []]\n    ",
			"API_4": "zip_with_map(source_iterdatapipe: IterDataPipe, map_datapipe: MapDataPipe, key_fn: Callable, merge_fn: Optional[Callable] = None): Joins the items from the source IterDataPipe with items from a MapDataPipe.",
			"API_5": "DataFrameMaker(source_dp: torch.utils.data.dataset.IterDataPipe[~T_co], dataframe_size: int = 1000, dtype=None, columns: Union[List[str], NoneType] = None, device: str = ''): Takes rows of data, batches a number of them together and creates `TorchArrow` DataFrames (functional name: ``dataframe``)."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/26": {
		"query": "I would like to sip all data in a monkey knowledgeframe Using kf.index to sip all rows",
		"retrieved_APIs": {
			"API_1": "batch(datapipe: MapDataPipe[T], batch_size: int, drop_last: bool = False, wrapper_class=DataChunk): Create mini-batches of data.",
			"API_2": "bucketbatch(datapipe: torch.utils.data.dataset.IterDataPipe[+T_co], batch_size: int, drop_last: bool = False, batch_num: int = 100, bucket_num: int = 1, sort_key: Union[Callable, NoneType] = None, in_batch_shuffle: bool = True): Creates mini-batches of data from sorted bucket.",
			"API_3": "SequenceWrapper(*args, **kwds): Wraps a sequence object into a MapDataPipe.",
			"API_4": "map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.",
			"API_5": "fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000): Creates multiple instances of the same Iterable DataPipe."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/27": {
		"query": "List of numsets. Stack them using axis that is negative one .",
		"retrieved_APIs": {
			"API_1": "cycle(*args, **kwds):\n    Cycles the specified input in perpetuity by default, or for the specified number\n    of times (functional name: ``cycle``).\n\n    Args:\n        source_datapipe: source DataPipe that will be cycled through\n        count: the number of times to read through ``source_datapipe` (if ``None``, it will cycle in perpetuity)\n\n    Example:\n        >>> from torchdata.datapipes.iter import IterableWrapper\n        >>> dp = IterableWrapper(range(3))\n        >>> dp = dp.cycle(2)\n        >>> list(dp)\n        [0, 1, 2, 0, 1, 2]\n    ",
			"API_2": "SampleMultiplexer(*args, **kwds): Takes a `Dict` of (IterDataPipe, Weight), and yields items by sampling from these DataPipes with respect to their weights.",
			"API_3": "flatmap(*args, **kwds): Applies a function over each item from the source DataPipe, then flattens the outputs to a single, unnested IterDataPipe.",
			"API_4": "mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes.",
			"API_5": "Sampler(*args, **kwds): Generates sample elements using the provided ``Sampler`` (defaults to :class:`SequentialSampler`)."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/28": {
		"query": "How to invoke the standard deviation function on a 2d numset? with axis=0, it will return a 1d numset with the standard deviation of each column",
		"retrieved_APIs": {
			"API_1": "concat(*args, **kwds): Concatenates multiple Iterable DataPipes.",
			"API_2": "mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes.",
			"API_3": "flatmap(*args, **kwds): Applies a function over each item from the source DataPipe, then flattens the outputs to a single, unnested IterDataPipe.",
			"API_4": "unzip(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Sequence[~T]], sequence_length: int, buffer_size: int = 1000, columns_to_skip: Union[Sequence[int], NoneType] = None): Takes in a DataPipe of Sequences, unpacks each Sequence, and return the elements in separate DataPipes based on their position in the Sequence.",
			"API_5": "fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000): Creates multiple instances of the same Iterable DataPipe."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/29": {
		"query": "create a beatnum numset composed of a list [[8, 7, 2], [5, 6, 1], [8, 2, 6]]",
		"retrieved_APIs": {
			"API_1": "zip_with_iter(source_datapipe: IterDataPipe, ref_datapipe: IterDataPipe, key_fn: Callable, ref_key_fn: Optional[Callable] = None, keep_key: bool = False, buffer_size: int = 10000, merge_fn: Optional[Callable] = None): Zips two IterDataPipes together based on the matching key.",
			"API_2": "zip_with_map(source_iterdatapipe: IterDataPipe, map_datapipe: MapDataPipe, key_fn: Callable, merge_fn: Optional[Callable] = None): Joins the items from the source IterDataPipe with items from a MapDataPipe.",
			"API_3": "unzip(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Sequence[~T]], sequence_length: int, buffer_size: int = 1000, columns_to_skip: Union[Sequence[int], NoneType] = None): Takes in a DataPipe of Sequences, unpacks each Sequence, and return the elements in separate DataPipes based on their position in the Sequence.",
			"API_4": "fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000): Creates multiple instances of the same Iterable DataPipe.",
			"API_5": "ZipArchiveLoader(*args, **kwds): Opens/decompresses zip binary streams from an Iterable DataPipe which contains a tuple of path name and zip binary stream, and yields a tuple of path name and extracted binary stream (functional name: ``load_from_zip``)."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/30": {
		"query": "Can you provide an example of how to convert a pytorch model to the onnx framework?",
		"retrieved_APIs": {
			"API_1": "zip_with_iter(source_datapipe: IterDataPipe, ref_datapipe: IterDataPipe, key_fn: Callable, ref_key_fn: Optional[Callable] = None, keep_key: bool = False, buffer_size: int = 10000, merge_fn: Optional[Callable] = None): Zips two IterDataPipes together based on the matching key.",
			"API_2": "zip_with_map(source_iterdatapipe: IterDataPipe, map_datapipe: MapDataPipe, key_fn: Callable, merge_fn: Optional[Callable] = None): Joins the items from the source IterDataPipe with items from a MapDataPipe.",
			"API_3": "fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000): Creates multiple instances of the same Iterable DataPipe.",
			"API_4": "ZipArchiveLoader(*args, **kwds): Opens/decompresses zip binary streams from an Iterable DataPipe which contains a tuple of path name and zip binary stream, and yields a tuple of path name and extracted binary stream (functional name: ``load_from_zip``).",
			"API_5": "unzip(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Sequence[~T]], sequence_length: int, buffer_size: int = 1000, columns_to_skip: Union[Sequence[int], NoneType] = None): Takes in a DataPipe of Sequences, unpacks each Sequence, and return the elements in separate DataPipes based on their position in the Sequence."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/31": {
		"query": "Set value to an entire column `B` of a monkey knowledgeframe Return the changed knowledgeframe.",
		"retrieved_APIs": {
			"API_1": "zip_with_iter(source_datapipe: IterDataPipe, ref_datapipe: IterDataPipe, key_fn: Callable, ref_key_fn: Optional[Callable] = None, keep_key: bool = False, buffer_size: int = 10000, merge_fn: Optional[Callable] = None): Zips two IterDataPipes together based on the matching key.",
			"API_2": "zip_with_map(source_iterdatapipe: IterDataPipe, map_datapipe: MapDataPipe, key_fn: Callable, merge_fn: Optional[Callable] = None): Joins the items from the source IterDataPipe with items from a MapDataPipe.",
			"API_3": "unbatch(datapipe: IterDataPipe, unbatch_level: int = 1): Undoes batching of data.",
			"API_4": "fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000): Creates multiple instances of the same Iterable DataPipe.",
			"API_5": "batch(datapipe: MapDataPipe[T], batch_size: int, drop_last: bool = False, wrapper_class=DataChunk): Create mini-batches of data."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/32": {
		"query": "Prune custom pytorch model automatically. Model path is 'model.pt', output directory is './saved', compression ratio is 0.7. and input shape of the model would be [1, 3, 96, 96].",
		"retrieved_APIs": {
			"API_1": "cycle(*args, **kwds):\n    Cycles the specified input in perpetuity by default, or for the specified number\n    of times (functional name: ``cycle``).\n\n    Args:\n        source_datapipe: source DataPipe that will be cycled through\n        count: the number of times to read through ``source_datapipe` (if ``None``, it will cycle in perpetuity)\n\n    Example:\n        >>> from torchdata.datapipes.iter import IterableWrapper\n        >>> dp = IterableWrapper(range(3))\n        >>> dp = dp.cycle(2)\n        >>> list(dp)\n        [0, 1, 2, 0, 1, 2]\n    ",
			"API_2": "zip_with_map(source_iterdatapipe: IterDataPipe, map_datapipe: MapDataPipe, key_fn: Callable, merge_fn: Optional[Callable] = None): Joins the items from the source IterDataPipe with items from a MapDataPipe.",
			"API_3": "Zipper(*args, **kwds): Aggregates elements into a tuple from each of the input DataPipes (functional name: ``zip``).",
			"API_4": "fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000): Creates multiple instances of the same Iterable DataPipe.",
			"API_5": "ZipArchiveLoader(*args, **kwds): Opens/decompresses zip binary streams from an Iterable DataPipe which contains a tuple of path name and zip binary stream, and yields a tuple of path name and extracted binary stream (functional name: ``load_from_zip``)."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/33": {
		"query": "Prepend element to beatnum numset Return the numset",
		"retrieved_APIs": {
			"API_1": "zip_with_map(source_iterdatapipe: IterDataPipe, map_datapipe: MapDataPipe, key_fn: Callable, merge_fn: Optional[Callable] = None): Joins the items from the source IterDataPipe with items from a MapDataPipe.",
			"API_2": "zip_with_iter(source_datapipe: IterDataPipe, ref_datapipe: IterDataPipe, key_fn: Callable, ref_key_fn: Optional[Callable] = None, keep_key: bool = False, buffer_size: int = 10000, merge_fn: Optional[Callable] = None): Zips two IterDataPipes together based on the matching key.",
			"API_3": "Zipper(*args, **kwds): Aggregates elements into a tuple from each of the input DataPipes (functional name: ``zip``).",
			"API_4": "unzip(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Sequence[~T]], sequence_length: int, buffer_size: int = 1000, columns_to_skip: Union[Sequence[int], NoneType] = None): Takes in a DataPipe of Sequences, unpacks each Sequence, and return the elements in separate DataPipes based on their position in the Sequence.",
			"API_5": "ZipArchiveLoader(*args, **kwds): Opens/decompresses zip binary streams from an Iterable DataPipe which contains a tuple of path name and zip binary stream, and yields a tuple of path name and extracted binary stream (functional name: ``load_from_zip``)."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/34": {
		"query": "How to train my efficientformer fx model? Train epoch should be 100 and I will use 4 gpus.",
		"retrieved_APIs": {
			"API_1": "Sampler(*args, **kwds):\n    Generates sample elements using the provided ``Sampler`` (defaults to :class:`SequentialSampler`).\n\n    Args:\n        datapipe: IterDataPipe to sample from\n        sampler: Sampler class to generate sample elements from input DataPipe.\n            Default is :class:`SequentialSampler` for IterDataPipe\n    ",
			"API_2": "zip_with_map(source_iterdatapipe: IterDataPipe, map_datapipe: MapDataPipe, key_fn: Callable, merge_fn: Optional[Callable] = None): Joins the items from the source IterDataPipe with items from a MapDataPipe.",
			"API_3": "Zipper(*args, **kwds): Aggregates elements into a tuple from each of the input DataPipes (functional name: ``zip``).",
			"API_4": "batch(datapipe: MapDataPipe[T], batch_size: int, drop_last: bool = False, wrapper_class=DataChunk): Create mini-batches of data.",
			"API_5": "unzip(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Sequence[~T]], sequence_length: int, buffer_size: int = 1000, columns_to_skip: Union[Sequence[int], NoneType] = None): Takes in a DataPipe of Sequences, unpacks each Sequence, and return the elements in separate DataPipes based on their position in the Sequence."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/35": {
		"query": "how can I make a python list obj from this matrix? the list should be one dimensional and contain all values of the matrix",
		"retrieved_APIs": {
			"API_1": "groupby(datapipe: IterDataPipe[torch.utils.data.datapipes.iter.grouping.T_co], group_key_fn: Callable, *, buffer_size: int = 10000, group_size: Optional[int] = None, guaranteed_group_size: Optional[int] = None, drop_remaining: bool = False): Groups data from input IterDataPipe by keys which are generated from ``group_key_fn``, and yields a ``DataChunk`` with batch size up to ``group_size`` if defined.",
			"API_2": "FileLister(*args, **kwds): Given path(s) to the root directory, yields file pathname(s) (path + filename) of files within the root directory.",
			"API_3": "FileOpener(*args, **kwds): Given pathnames, opens files and yield pathname and file stream in a tuple.",
			"API_4": "demux(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, classifier_fn: Callable[[+T_co], Union[int, NoneType]], drop_none: bool = False, buffer_size: int = 1000): Splits the input DataPipe into multiple child DataPipes, using the given classification function.",
			"API_5": "join(iterable, /): Concatenate any number of strings."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/37": {
		"query": "How to train the SegFormer for a image segmetation task?",
		"retrieved_APIs": {
			"API_1": "collate(datapipe: IterDataPipe, collate_fn: Callable = <function default_collate>): Collates samples from DataPipe to Tensor(s) by a custom collate function.",
			"API_2": "map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.",
			"API_3": "concat(*args, **kwds): Concatenate multiple Map DataPipes.",
			"API_4": "IterToMapConverter(*args, **kwds): Lazily load data from ``IterDataPipe`` to construct a ``MapDataPipe`` with the key-value pair generated by ``key_value_fn`` (functional name: ``to_map_datapipe``).",
			"API_5": "MapDataPipe(*args, **kwds): Map-style DataPipe."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/38": {
		"query": "Removing columns with index 1 and 3 in beatnum If you ever want to remove more than one columns, you just pass indices of columns you want removed as a list to bn.remove_operation, like this:",
		"retrieved_APIs": {
			"API_1": "collate(datapipe: IterDataPipe, collate_fn: Callable = <function default_collate>): Collates samples from DataPipe to Tensor(s) by a custom collate function.",
			"API_2": "filter(datapipe: IterDataPipe, filter_fn: Callable, drop_empty_batches: bool = True): Filters out elements from the source datapipe according to input ``filter_fn``.",
			"API_3": "map(*args, **kwds): Apply the input function over each item from the source DataPipe.",
			"API_4": "ShardingFilter(*args, **kwds): Wrapper that allows DataPipe to be sharded (functional name: ``sharding_filter``).",
			"API_5": "IterToMapConverter(*args, **kwds): Lazily load data from ``IterDataPipe`` to construct a ``MapDataPipe`` with the key-value pair generated by ``key_value_fn`` (functional name: ``to_map_datapipe``)."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/40": {
		"query": "I want to get the counts of distinctive values of the knowledgeframe. count_values implements this however I want to use its output somewhere else. How can I convert .count_values output to a monkey knowledgeframe. Use renaming_axis('distinctive_values') for name ('counts') of column from index and reseting_index return the final knowledgeframe",
		"retrieved_APIs": {
			"API_1": "demux(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, classifier_fn: Callable[[+T_co], Union[int, NoneType]], drop_none: bool = False, buffer_size: int = 1000): Splits the input DataPipe into multiple child DataPipes, using the given classification function.",
			"API_2": "concat(*args, **kwds): Concatenate multiple Map DataPipes.",
			"API_3": "fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000): Creates multiple instances of the same Iterable DataPipe.",
			"API_4": "map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.",
			"API_5": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/41": {
		"query": "How can I get the shape of BeatNum numset?",
		"retrieved_APIs": {
			"API_1": "SampleMultiplexer(*args, **kwds): Takes a `Dict` of (IterDataPipe, Weight), and yields items by sampling from these DataPipes with respect to their weights.",
			"API_2": "filter(datapipe: IterDataPipe, filter_fn: Callable, drop_empty_batches: bool = True): Filters out elements from the source datapipe according to input ``filter_fn``.",
			"API_3": "enumerate(*args, **kwds): Adds an index to an existing DataPipe through enumeration, with the index starting from 0 by default.",
			"API_4": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.",
			"API_5": "map(*args, **kwds): Apply the input function over each item from the source DataPipe."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/42": {
		"query": "Check if each element in a beatnum numset is in another numset This problem seems easy but I cannot quite get a nice-looking solution. I have two beatnum numsets (A and B), and I want to get the indices of A where the elements of A are in B and also get the indices of A where the elements are not in B.",
		"retrieved_APIs": {
			"API_1": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.",
			"API_2": "map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.",
			"API_3": "unzip(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Sequence[~T]], sequence_length: int, buffer_size: int = 1000, columns_to_skip: Union[Sequence[int], NoneType] = None): Takes in a DataPipe of Sequences, unpacks each Sequence, and return the elements in separate DataPipes based on their position in the Sequence.",
			"API_4": "MapDataPipe(*args, **kwds): Map-style DataPipe.",
			"API_5": "concat(*args, **kwds): Concatenate multiple Map DataPipes."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/43": {
		"query": "Show me a code snippet to set a compression method to L2 pruning.",
		"retrieved_APIs": {
			"API_1": "batch(datapipe: IterDataPipe, batch_size: int, drop_last: bool = False, wrapper_class=List): Creates mini-batches of data.",
			"API_2": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10):\n    Yields elements from the source DataPipe from the start, up to the specfied limit (functional name: ``header``).\n\n    Args:\n        source_datapipe: the DataPipe from which elements will be yielded\n        limit: the number of elements to yield before stopping\n\n    Example:\n        >>> from torchdata.datapipes.iter import IterableWrapper\n        >>> dp = IterableWrapper(range(10))\n        >>> header_dp = dp.header(3)\n        >>> list(header_dp)\n        [0, 1, 2]\n    ",
			"API_3": "unbatch(datapipe: IterDataPipe, unbatch_level: int = 1): Undoes batching of data.",
			"API_4": "fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000): Creates multiple instances of the same Iterable DataPipe.",
			"API_5": "filter(datapipe: IterDataPipe, filter_fn: Callable, drop_empty_batches: bool = True): Filters out elements from the source datapipe according to input ``filter_fn``."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/44": {
		"query": "How to remove all rows in a beatnum.ndnumset that contain non-numeric values? Return the final result",
		"retrieved_APIs": {
			"API_1": "batch(datapipe: IterDataPipe, batch_size: int, drop_last: bool = False, wrapper_class=List): Creates mini-batches of data.",
			"API_2": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10):\n    Yields elements from the source DataPipe from the start, up to the specfied limit (functional name: ``header``).\n\n    Args:\n        source_datapipe: the DataPipe from which elements will be yielded\n        limit: the number of elements to yield before stopping\n\n    Example:\n        >>> from torchdata.datapipes.iter import IterableWrapper\n        >>> dp = IterableWrapper(range(10))\n        >>> header_dp = dp.header(3)\n        >>> list(header_dp)\n        [0, 1, 2]\n    ",
			"API_3": "concat(*args, **kwds):\n    Concatenate multiple Map DataPipes (functional name: ``concat``).\n    The new index of is the cumulative sum of source DataPipes.\n    For example, if there are 2 source DataPipes both with length 5,\n    index 0 to 4 of the resulting `ConcatMapDataPipe` would refer to\n    elements of the first DataPipe, and 5 to 9 would refer to elements\n    of the second DataPipe.\n\n    Args:\n        datapipes: Map DataPipes being concatenated\n    ",
			"API_4": "rows2columnar(source_datapipe: IterDataPipe[List[Union[Dict, List]]], column_names: Optional[List[str]] = None): Accepts an input DataPipe with batches of data, and processes one batch at a time and yields a Dict for each batch, with ``column_names`` as keys and lists of corresponding values from each row as values.",
			"API_5": "fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000): Creates multiple instances of the same Iterable DataPipe."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/45": {
		"query": "Any word from `targets` are present in sentence.",
		"retrieved_APIs": {
			"API_1": "concat(*args, **kwds): Concatenates multiple Iterable DataPipes.",
			"API_2": "enumerate(*args, **kwds): Adds an index to an existing DataPipe through enumeration, with the index starting from 0 by default.",
			"API_3": "mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes.",
			"API_4": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.",
			"API_5": "add_index(*args, **kwds): Adds an index to an existing Iterable DataPipe with."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/46": {
		"query": "Convert a beatnum.ndnumset to string and convert it back to beatnum.ndnumset with dtype=int",
		"retrieved_APIs": {
			"API_1": "concat(*args, **kwds): Concatenates multiple Iterable DataPipes.",
			"API_2": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.",
			"API_3": "unzip(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Sequence[~T]], sequence_length: int, buffer_size: int = 1000, columns_to_skip: Union[Sequence[int], NoneType] = None): Takes in a DataPipe of Sequences, unpacks each Sequence, and return the elements in separate DataPipes based on their position in the Sequence.",
			"API_4": "demux(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, classifier_fn: Callable[[+T_co], Union[int, NoneType]], drop_none: bool = False, buffer_size: int = 1000): Splits the input DataPipe into multiple child DataPipes, using the given classification function.",
			"API_5": "fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000): Creates multiple instances of the same Iterable DataPipe."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/47": {
		"query": "Find rows in kf where col_a > col_b Return the rows",
		"retrieved_APIs": {
			"API_1": "concat(*args, **kwds): Concatenate multiple Map DataPipes.",
			"API_2": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.",
			"API_3": "MapDataPipe(*args, **kwds): Map-style DataPipe.",
			"API_4": "mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes.",
			"API_5": "demux(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, classifier_fn: Callable[[+T_co], Union[int, NoneType]], drop_none: bool = False, buffer_size: int = 1000): Splits the input DataPipe into multiple child DataPipes, using the given classification function."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/48": {
		"query": "Show me the code snippet to pruning the pytorch model. Here are some conditions.\n 1. Compression method: tucker decomposition \n2. Compression ratio: 0.4.",
		"retrieved_APIs": {
			"API_1": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.",
			"API_2": "flatmap(*args, **kwds): Applies a function over each item from the source DataPipe, then flattens the outputs to a single, unnested IterDataPipe.",
			"API_3": "concat(*args, **kwds): Concatenates multiple Iterable DataPipes.",
			"API_4": "mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes.",
			"API_5": "IterDataPipe(*args, **kwds): Iterable-style DataPipe."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/49": {
		"query": "I am using beatnum. I have a matrix `M` 1*N and I want to get an numset from with N elements. To achieve it, Does anyone know a more elegant way to get the result?",
		"retrieved_APIs": {
			"API_1": "HttpReader(source_datapipe: IterDataPipe[str], timeout: Optional[float] = None): Takes file URLs (HTTP URLs pointing to files), and yields tuples of file URL and IO stream.",
			"API_2": "OnlineReader(*args, **kwds): Takes file URLs (can be HTTP URLs pointing to files or URLs to GDrive files), and yields tuples of file URL and IO stream.",
			"API_3": "Extractor(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Tuple[str, io.IOBase]], file_type: Union[str, torchdata.datapipes.iter.util.decompressor.CompressionType, NoneType] = None): Please use ``Decompressor`` or ``.",
			"API_4": "GDriveReader(*args, **kwds): Takes URLs pointing at GDrive files, and yields tuples of file name and IO stream.",
			"API_5": "collate(datapipe: IterDataPipe, collate_fn: Callable = <function default_collate>): Collates samples from DataPipe to Tensor(s) by a custom collate function."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/3": {
		"query": "adding dictionary to data frame return the data frame",
		"retrieved_APIs": {
			"API_1": "concat(*args, **kwds): Concatenate multiple Map DataPipes.",
			"API_2": "MapDataPipe(*args, **kwds): Map-style DataPipe.",
			"API_3": "IterDataPipe(*args, **kwds): Iterable-style DataPipe.",
			"API_4": "mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes.",
			"API_5": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/8": {
		"query": "Add Leading Zeros to Strings at `col_name` in Monkey Dataframe The maximum length of the string is 15 Return the knowledgeframe",
		"retrieved_APIs": {
			"API_1": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.",
			"API_2": "flatmap(*args, **kwds): Applies a function over each item from the source DataPipe, then flattens the outputs to a single, unnested IterDataPipe.",
			"API_3": "mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes.",
			"API_4": "demux(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, classifier_fn: Callable[[+T_co], Union[int, NoneType]], drop_none: bool = False, buffer_size: int = 1000): Splits the input DataPipe into multiple child DataPipes, using the given classification function.",
			"API_5": "concat(*args, **kwds): Concatenates multiple Iterable DataPipes."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/13": {
		"query": "I want to make all column headers in my monkey data frame lower case Return the changed knowledgeframe",
		"retrieved_APIs": {
			"API_1": "collate(datapipe: IterDataPipe, collate_fn: Callable = <function default_collate>): Collates samples from DataPipe to Tensor(s) by a custom collate function.",
			"API_2": "map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.",
			"API_3": "Extractor(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Tuple[str, io.IOBase]], file_type: Union[str, torchdata.datapipes.iter.util.decompressor.CompressionType, NoneType] = None): Please use ``Decompressor`` or ``.",
			"API_4": "DataFrameMaker(source_dp: torch.utils.data.dataset.IterDataPipe[~T_co], dataframe_size: int = 1000, dtype=None, columns: Union[List[str], NoneType] = None, device: str = ''): Takes rows of data, batches a number of them together and creates `TorchArrow` DataFrames (functional name: ``dataframe``).",
			"API_5": "types(value, names=None, *, module=None, qualname=None, type=None, start=1): An enumeration."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/15": {
		"query": "I want to benchmark my tensorrt model on jetson nano 4.6 device.",
		"retrieved_APIs": {
			"API_1": "unbatch(datapipe: IterDataPipe, unbatch_level: int = 1): Undoes batching of data.",
			"API_2": "cycle(*args, **kwds): Cycles the specified input in perpetuity by default, or for the specified number of times.",
			"API_3": "flatmap(*args, **kwds): Applies a function over each item from the source DataPipe, then flattens the outputs to a single, unnested IterDataPipe.",
			"API_4": "map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.",
			"API_5": "enumerate(*args, **kwds): Adds an index to an existing DataPipe through enumeration, with the index starting from 0 by default."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/22": {
		"query": "Convert beatnum numset to tuple Return the transformed tuple",
		"retrieved_APIs": {
			"API_1": "StreamReader(datapipe, chunk=None): Given IO streams and their label names, yields bytes with label name in a tuple.",
			"API_2": "fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000): Creates multiple instances of the same Iterable DataPipe.",
			"API_3": "batch(datapipe: IterDataPipe, batch_size: int, drop_last: bool = False, wrapper_class=List): Creates mini-batches of data.",
			"API_4": "XzFileLoader(*args, **kwds): Decompresses xz (lzma) binary streams from an Iterable DataPipe which contains tuples of path name and xy binary streams, and yields a tuple of path name and extracted binary stream (functional name: ``load_from_xz``).",
			"API_5": "unzip(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Sequence[~T]], sequence_length: int, buffer_size: int = 1000, columns_to_skip: Union[Sequence[int], NoneType] = None): Takes in a DataPipe of Sequences, unpacks each Sequence, and return the elements in separate DataPipes based on their position in the Sequence."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/39": {
		"query": "How does one add rows to a beatnum numset? Is there a beatnumthonic way to do this?",
		"retrieved_APIs": {
			"API_1": "demux(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, classifier_fn: Callable[[+T_co], Union[int, NoneType]], drop_none: bool = False, buffer_size: int = 1000): Splits the input DataPipe into multiple child DataPipes, using the given classification function.",
			"API_2": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.",
			"API_3": "IterDataPipe(*args, **kwds): Iterable-style DataPipe.",
			"API_4": "enumerate(*args, **kwds): Adds an index to an existing DataPipe through enumeration, with the index starting from 0 by default.",
			"API_5": "concat(*args, **kwds): Concatenates multiple Iterable DataPipes."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	},
	"TorchDataEval/36": {
		"query": "I am using beatnum. I have a matrix with 1 column and N rows and I want to get an numset from with N elements. For example, if i have M = matrix([[1], [2], [3], [4]]), I want to get A = numset([1,2,3,4]). Return the numset",
		"retrieved_APIs": {
			"API_1": "groupby(datapipe: IterDataPipe[torch.utils.data.datapipes.iter.grouping.T_co], group_key_fn: Callable, *, buffer_size: int = 10000, group_size: Optional[int] = None, guaranteed_group_size: Optional[int] = None, drop_remaining: bool = False): Groups data from input IterDataPipe by keys which are generated from ``group_key_fn``, and yields a ``DataChunk`` with batch size up to ``group_size`` if defined.",
			"API_2": "header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.",
			"API_3": "FileLister(*args, **kwds): Given path(s) to the root directory, yields file pathname(s) (path + filename) of files within the root directory.",
			"API_4": "map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.",
			"API_5": "demux(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, classifier_fn: Callable[[+T_co], Union[int, NoneType]], drop_none: bool = False, buffer_size: int = 1000): Splits the input DataPipe into multiple child DataPipes, using the given classification function."
		},
		"annotation": {
			"answerable": false,
			"reason_for_unanswerable": "out_of_db"
		}
	}
}