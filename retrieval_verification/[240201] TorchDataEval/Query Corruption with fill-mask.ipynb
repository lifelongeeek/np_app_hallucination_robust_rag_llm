{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, AutoTokenizer\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list = [\n",
    "    'How to augument the datapipe by repeating it six times.',\n",
    "    'Assign indexs to the datepipe object.',\n",
    "    'How to get one training data from the batch_dp',\n",
    "    'Split into 2 sub-datapipes by the odd_or_even function',\n",
    "    'Clone the source datapipe two times',\n",
    "    'Putting two IterDataPipes together based on their key.',\n",
    "    'Attach the elements in the source IterDataPipe to the elements in the MapDataPipe.',\n",
    "    'Take samples from these DataPipes based on their weights with random seed 0, until all of them are exhausted.',\n",
    "    'Unzip the three tuples, and return these elements in separate DataPipes, depending on their location.',\n",
    "    'Divide datapipes into 3 batches and discard if the last batch is not reached.',\n",
    "    'Create batch datapipe with batch size 3, batch num is 100, and drop the last batch if it is not full. Also, useing the sort_bucket function to sort the bucket, where the bucket_num is 1.',\n",
    "    'Group by file name (except extension), we set the buffer size and group size to 3, and the guaranteed group size to 2.',\n",
    "    'Using IterableWrapper to the file url and HttpReader to read the file',\n",
    "    'Each item in the source_dp is applied mutiple_fn function and the output is then tiled to a single, unnested one.',\n",
    "    'Method 1 map_dp_1 = dp.map(add_one) Invocation via functional form is preferred Method 2 We discourage the usage of `lambda` functions as they are not serializable with `pickle` Using `lambda` to implement add_two rather than add_one that is mentioned in above.',\n",
    "    'Filtering by the above function',\n",
    "    'How to get the first three elements of a datapipe?',\n",
    "    \"Each element in a batch is a `Dict` Takes an input DataPipe with batches of data, processes the batches one and produces a Dict for each batch. We only need the column 'a' from each batch.\",\n",
    "    'map_dp_1 = dp.map(lambda x: x + 1) Using functional form (recommended) map_dp_2 = Mapper(dp, lambda x: x + 1) Using class constructor Get the mapper datapipe (map_dp_1) batch datas with the batch size of 2.',\n",
    "    'Read the URL using the HTTP protocol and process the csv file.',\n",
    "    'Read the URL using the HTTP protocol and process the csv file. Then, we map the datapipe using lambda_func_ to get what we want.',\n",
    "    'Read the URL using the HTTP protocol and process the csv file. Then, we map the datapipe using lambda_func_ to get what we want. How to get all batches from a datapipe with batch size 2? Furthermore, the batches should be mapped using lambda_batch.',\n",
    "    'Augument the datapipe with repeat three times and sample the data.',\n",
    "    'First we concatenate two datapipes and then repeat the concatenated datapipe three times.',\n",
    "    'According to the merge_fn, we zip the above two datapipes and keep the key True. Whatsmore, cycle the zipped datapipe three times.',\n",
    "    'We zipp the above two data pipes and set keep_key to True according to merge_fn. Also, enumerating the zipped datapipe.',\n",
    "    'Zipping the above two data pipes and set keep_key to True according to merge_fn. Moreover, transform its type to List and get the first element.',\n",
    "    'Using merge_fn to zip the two data pipes. Repeating three times to argument the zipped data pipe.',\n",
    "    'Using merge_fn to zip the two data pipes, and repeating three times to argument the zipped data pipe. Finally, we convert the result type to a list and take the second element of each tuple.',\n",
    "    'Using merge_fn to zip the two data pipes, and repeating three times to argument the zipped data pipe, and then sampling the result. Finally, we convert the result type to a list and take the third element of each tuple.',\n",
    "    'Group the files by their file name using the group_fn function. Then, reserving the length of group result greater than 1.',\n",
    "    'First get the head 2 elements Second make the datapipe tensor-like by using `collate_fn`',\n",
    "    'Filter the value smaller than 5 Second make the datapipe tensor-like by using `collate_fn`',\n",
    "    'Split the source datapipe into two datapipes by applying the function `great_than_5`',\n",
    "    'Given the weight, how to sample from two datapipes? Note that the sample seed is set to 1 for reproducibility',\n",
    "    'I would like assgin dp1 to be a datapipe that contains the first column of raw_dp and dp2 to be a datapipe that contains the second column of raw_dp and dp3 to be a datapipe that contains the third column of raw_dp How to do this?',\n",
    "    'Make a batch operation on the datapipe `dp` of size 3 with droping last batch if it is not full. And then get the first two batches.',\n",
    "    'Batch on data pipe `dp1` of size 4 and discard the last batch if they are not filled, and then obtain the first two batches. Then the above result is concatenated with the datapipe `dp2`.',\n",
    "    'Concatenate two datapipes and add corresponding indices with the name `Ids`.',\n",
    "    'Join the two data pipes and add an index with the name `Ids`. Then create three copies of the datapipe.',\n",
    "    'Join the three data pipes and obtain the enumerated datapipe.',\n",
    "    'I want to augment the source datapipe with the above function, which will return nine elements. Then we flatten the nine elements into a single datapipe.',\n",
    "    'Read the URL using the HTTP protocol and parse the csv file as a dictionary.',\n",
    "    'concat two datapipes',\n",
    "    'One element is generated from each input Iterable DataPipes in turn, until the end when the shortest input DataPipe is used up.',\n",
    "    'convert integer to float Tensor using `int2tensor`.',\n",
    "    'Does the unbatch processing of data, the level is setted by default to 1.',\n",
    "    'generating bytes where the chunk is set to one.',\n",
    "    'Put the above DataPipes into one list obj, and remove the last number from each element (e.g., \"1\" in \"dog1\")',\n",
    "    'group by source_dp using the ``group_fn`` function and obtain the header groups by default, assign the result to the new variable ``header_groups``.'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.56707364320755,\n",
       "  'token': 2034,\n",
       "  'token_str': 'first',\n",
       "  'sequence': '[CLS] how to get the first three [MASK] of a datapipe? [SEP]'},\n",
       " {'score': 0.1960916966199875,\n",
       "  'token': 2197,\n",
       "  'token_str': 'last',\n",
       "  'sequence': '[CLS] how to get the last three [MASK] of a datapipe? [SEP]'},\n",
       " {'score': 0.048321887850761414,\n",
       "  'token': 2327,\n",
       "  'token_str': 'top',\n",
       "  'sequence': '[CLS] how to get the top three [MASK] of a datapipe? [SEP]'},\n",
       " {'score': 0.043448515236377716,\n",
       "  'token': 2279,\n",
       "  'token_str': 'next',\n",
       "  'sequence': '[CLS] how to get the next three [MASK] of a datapipe? [SEP]'},\n",
       " {'score': 0.01853400096297264,\n",
       "  'token': 2190,\n",
       "  'token_str': 'best',\n",
       "  'sequence': '[CLS] how to get the best three [MASK] of a datapipe? [SEP]'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_text = 'How to get the first three elements of a datapipe?'\n",
    "mask_text = 'How to get the [MASK] three [MASK] of a datapipe?'\n",
    "unmasker(mask_text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "mask_dict = {}\n",
    "k = 0           # select token with k-th highest probability to fill the mask (k = 0, 1, 2, ...)\n",
    "\n",
    "for idx, original_text in enumerate(tqdm(query_list)):\n",
    "    original_text = original_text.lower()\n",
    "\n",
    "    results = []\n",
    "    for token_index in range(len(original_text.split(' '))):\n",
    "        splits = original_text.split(' ')\n",
    "        splits[token_index] = '[MASK]'\n",
    "        mask_text = ' '.join(splits)\n",
    "        res = unmasker(mask_text)[k]['sequence']\n",
    "        results.append(res)\n",
    "\n",
    "    mask_dict[1000000 + idx] = {\n",
    "        'original_query': original_text,\n",
    "        'mask_filled_text': results\n",
    "    }\n",
    "\n",
    "with open(f'./root/mask_dict_{k+1}.json', 'w') as f:\n",
    "    json.dump(mask_dict, f, indent='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with similarity between original text and mask-filled text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "embedding_model = SentenceTransformer('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:24<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "sim_results = []\n",
    "k = 1\n",
    "\n",
    "for original_text in tqdm(query_list):\n",
    "    original_text = original_text.lower()\n",
    "    results = []\n",
    "    for token_index in range(len(original_text.split(' '))):\n",
    "        splits = original_text.split(' ')\n",
    "        splits[token_index] = '[MASK]'\n",
    "        mask_text = ' '.join(splits)\n",
    "        res = unmasker(mask_text)[k]['sequence']\n",
    "        results.append(res)\n",
    "    embedding_1= embedding_model.encode(original_text, convert_to_tensor=True)\n",
    "    embedding_2 = embedding_model.encode(results, convert_to_tensor=True)\n",
    "    sim_res = util.pytorch_cos_sim(embedding_1, embedding_2).tolist()\n",
    "    sim_results.append(sim_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 4),\n",
       " (1, 0),\n",
       " (1, 4),\n",
       " (1, 5),\n",
       " (2, 8),\n",
       " (3, 6),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (4, 3),\n",
       " (4, 5),\n",
       " (5, 0),\n",
       " (5, 2),\n",
       " (6, 12),\n",
       " (9, 1),\n",
       " (15, 0),\n",
       " (15, 3),\n",
       " (15, 4),\n",
       " (16, 9),\n",
       " (33, 11),\n",
       " (43, 0),\n",
       " (43, 1),\n",
       " (43, 2),\n",
       " (45, 6),\n",
       " (46, 11)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cords = []\n",
    "for i, val_list in enumerate(sim_results):\n",
    "    for j, val in enumerate(val_list[0]):\n",
    "        if val <= 0.93:\n",
    "            cords.append((i, j))\n",
    "\n",
    "cords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
