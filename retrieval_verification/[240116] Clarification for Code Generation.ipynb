{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from openai import OpenAI\n","from dotenv import load_dotenv\n","import os"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["load_dotenv()\n","client = OpenAI(\n","    api_key=os.getenv(\"OPENAI_API_KEY\")\n",")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def get_response(messages, model=\"gpt-3.5-turbo\", temperature=0.7):\n","    response = client.chat.completions.create(\n","        model=model,\n","        messages=messages,\n","        temperature=temperature,\n","    )\n","    return response.choices[0].message.content"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# 필수적인 정보 중 누락된 정보 구하기\n","\n","def get_missing_parts(query):\n","    content = f\"Q: I want to train AI Model. // missing parts: ['task', 'model name', 'dataset'] \\\n","        Q: I want to train AI Model for text classification. // missing parts: ['model name', 'dataset'] \\\n","        Q: Give me the code for training Roberta model. // missing parts: ['task', 'dataset'] \\\n","        Q: I want to train Bert model with klue dataset // missing parts: ['task'] \\\n","        Q: I need a code to train resnet18 for image classification with cifar10 dataset. // missing parts: [] \\\n","        Q: {query} // missing parts:\"\n","    \n","    messages = [\n","        # 시스템 프롬프트를 통해 필수적인 정보 명시\n","        {\"role\": \"system\", \"content\": \"Essential parts for training AI model are task, model name, dataset.\"},\n","        {\"role\": \"user\", \"content\": content},\n","    ]\n","\n","    return eval(get_response(messages))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def get_last_answer(query, additional_info):\n","    content = f\"{query} {additional_info} Give me a code snippet.\"\n","    print(f'Final query: {content}')\n","    messages = [\n","        {\"role\": \"user\", \"content\": content},\n","    ]\n","    return get_response(messages)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def get_answer(query):\n","    print(f\"User: {query}\")\n","    missing_parts = get_missing_parts(query)\n","    print(f\"Missing Parts: {missing_parts}\")\n","\n","    additional_infos = ''\n","    for part in missing_parts:\n","        print(f\"Bot: Which {part} you need?\")\n","        user_input = input()\n","        print(f\"User: {user_input}\")\n","        additional_infos += f\"I need {user_input}. \"\n","\n","    last_answer = get_last_answer(query, additional_infos)\n","    print(f\"Bot: {last_answer}\")"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["User: I want to train vgg19 model\n","Missing Parts: ['task', 'dataset']\n","Bot: Which task you need?\n","User: image classification\n","Bot: Which dataset you need?\n","User: cifar10 dataset\n","Final query: I want to train vgg19 model I need image classification. I need cifar10 dataset.  Give me a code snippet.\n","Bot: Sure! Here's a code snippet that demonstrates how to train a VGG19 model for image classification using the CIFAR-10 dataset in PyTorch:\n","\n","```python\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision.models import vgg19\n","\n","# Define transform to normalize the data\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","# Load CIFAR-10 dataset\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n","\n","# Load pre-trained VGG19 model\n","model = vgg19(pretrained=False, progress=True)\n","num_features = model.classifier[6].in_features\n","model.classifier[6] = nn.Linear(num_features, 10)  # Modify the last layer for 10 classes\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","# Train the model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","for epoch in range(10):  # Number of epochs\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        \n","        optimizer.zero_grad()\n","        \n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        running_loss += loss.item()\n","        if i % 2000 == 1999:    # Print average loss every 2000 mini-batches\n","            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n","            running_loss = 0.0\n","\n","print('Training finished.')\n","\n","# Test the model\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        images, labels = images.to(device), labels.to(device)\n","        \n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 10000 test images: %.2f %%' % (100 * correct / total))\n","```\n","\n","Make sure to have PyTorch and torchvision installed in your environment before running this code snippet. It downloads the CIFAR-10 dataset, initializes a pre-trained VGG19 model, and trains it on the dataset. Finally, it evaluates the model's accuracy on the test set.\n"]}],"source":["get_answer(\"I want to train vgg19 model\")"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["User: I want to train Bert model for text classification.\n","Missing Parts: ['dataset']\n","Bot: Which dataset you need?\n","User: klue dataset\n","Final query: I want to train Bert model for text classification. I need klue dataset.  Give me a code snippet.\n","Bot: Here's a code snippet to train a BERT model for text classification using the KLUE dataset:\n","\n","```python\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","\n","# Load KLUE dataset\n","# Assuming you have downloaded the KLUE dataset and saved it as a CSV file\n","# You will need to modify this part to load the dataset according to your specific configuration\n","def load_klue_dataset():\n","    # Load the dataset using pandas or any other library\n","    # For example:\n","    import pandas as pd\n","    df = pd.read_csv('klue_dataset.csv')\n","    texts = df['text'].tolist()\n","    labels = df['label'].tolist()\n","    return texts, labels\n","    \n","# Tokenize the input texts\n","def tokenize_inputs(texts):\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    tokenized_texts = tokenizer.batch_encode_plus(\n","        texts,\n","        padding=True,\n","        truncation=True,\n","        return_tensors='pt'\n","    )\n","    return tokenized_texts\n","    \n","# Define the BERT model for sequence classification\n","def define_model(num_labels):\n","    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n","    return model\n","    \n","# Train the BERT model\n","def train_model(model, tokenized_texts, labels):\n","    dataset = torch.utils.data.TensorDataset(tokenized_texts['input_ids'],\n","                                             tokenized_texts['attention_mask'],\n","                                             labels)\n","    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n","    \n","    optimizer = AdamW(model.parameters(), lr=2e-5)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","    \n","    model.train()\n","    for epoch in range(3):  # Adjust the number of training epochs as needed\n","        for batch in dataloader:\n","            input_ids = batch[0].to(device)\n","            attention_mask = batch[1].to(device)\n","            labels = batch[2].to(device)\n","            \n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            \n","# Load KLUE dataset\n","texts, labels = load_klue_dataset()\n","\n","# Tokenize inputs\n","tokenized_texts = tokenize_inputs(texts)\n","\n","# Define the BERT model\n","num_labels = len(set(labels))\n","model = define_model(num_labels)\n","\n","# Train the BERT model\n","train_model(model, tokenized_texts, torch.tensor(labels))\n","```\n","\n","Make sure to modify the code to fit your specific configuration, such as dataset loading, tokenizer, and training parameters.\n"]}],"source":["get_answer(\"I want to train Bert model for text classification.\")"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["User: I want to train AI model\n","Missing Parts: ['task', 'model name', 'dataset']\n","Bot: Which task you need?\n","User: token classification\n","Bot: Which model name you need?\n","User: Roberta\n","Bot: Which dataset you need?\n","User: klue dataset\n","Final query: I want to train AI model I need token classification. I need Roberta. I need klue dataset.  Give me a code snippet.\n","Bot: Sure! Here's a code snippet that demonstrates how to train a token classification model using the Hugging Face Transformers library with the Roberta model and the KLUE dataset:\n","\n","```python\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import RobertaForTokenClassification, RobertaTokenizer, AdamW\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import Trainer, TrainingArguments\n","from datasets import load_dataset, load_metric\n","\n","# Load KLUE dataset\n","dataset = load_dataset(\"klue\", \"klue_ner\")\n","\n","# Load Roberta tokenizer\n","tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","\n","# Tokenize input texts and label encodings\n","def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(examples[\"sentence\"], truncation=True, is_split_into_words=True)\n","\n","    labels = []\n","    for i, label in enumerate(examples[\"entity\"]):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(label[word_idx])\n","            else:\n","                label_ids.append(label[word_idx] + 100)\n","            previous_word_idx = word_idx\n","        labels.append(label_ids)\n","\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs\n","\n","tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n","\n","# Split dataset into train and validation sets\n","train_dataset = tokenized_dataset[\"train\"]\n","val_dataset = tokenized_dataset[\"validation\"]\n","\n","# Initialize RobertaForTokenClassification model\n","model = RobertaForTokenClassification.from_pretrained(\"roberta-base\", num_labels=30)\n","\n","# Set training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./roberta_model\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n",")\n","\n","# Set trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Evaluate the model\n","trainer.evaluate()\n","```\n","\n","Make sure you have the necessary libraries installed, such as `torch`, `transformers`, `datasets`, etc. You can install them using `pip install` or `conda install` commands. Also, ensure that you have enough GPU memory to train the model efficiently.\n"]}],"source":["get_answer(\"I want to train AI model\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["User: Give me the code for training GPT2 for text generation with klue dataset.\n","Missing Parts: ['task']\n","Bot: Which task you need?\n","User: text generation\n","Final query: Give me the code for training GPT2 for text generation with klue dataset. I need text generation.  Give me a code snippet.\n","Bot: Sure! Here's a code snippet to train GPT-2 for text generation with the KLUE dataset using the Hugging Face's Transformers library:\n","\n","```python\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","class KLUEDataset(Dataset):\n","    def __init__(self, data_path, tokenizer, max_length):\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.input_ids = []\n","        \n","        with open(data_path, 'r', encoding='utf-8') as file:\n","            lines = file.readlines()\n","        \n","        for line in lines:\n","            line = line.strip()\n","            encoded = tokenizer.encode_plus(line, \n","                                            add_special_tokens=True,\n","                                            max_length=max_length,\n","                                            pad_to_max_length=True,\n","                                            return_tensors='pt')\n","            self.input_ids.append(encoded.input_ids)\n","        \n","    def __len__(self):\n","        return len(self.input_ids)\n","    \n","    def __getitem__(self, idx):\n","        return self.input_ids[idx]\n","        \n","# Initialize the tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","\n","# Set device (GPU or CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Set training parameters\n","batch_size = 4\n","max_length = 128\n","num_epochs = 5\n","learning_rate = 1e-5\n","\n","# Load the KLUE dataset\n","train_dataset = KLUEDataset('klue_dataset.txt', tokenizer, max_length)\n","\n","# Create data loader\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Define optimizer and loss function\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    \n","    for batch in train_loader:\n","        batch = batch.to(device)\n","        labels = batch[:, 1:].contiguous().view(-1)  # Shift labels by one position\n","        \n","        optimizer.zero_grad()\n","        \n","        outputs = model(batch, labels=labels)\n","        loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)), labels)\n","        \n","        loss.backward()\n","        optimizer.step()\n","        \n","        total_loss += loss.item()\n","    \n","    print(f\"Epoch {epoch + 1} - Avg. Loss: {total_loss / len(train_loader):.4f}\")\n","```\n","\n","In this code snippet, we define a `KLUEDataset` class to load the KLUE dataset and tokenize the input text using the GPT-2 tokenizer. We then create a data loader to iterate through the dataset. The model is initialized with GPT-2 and trained using a cross-entropy loss and the AdamW optimizer. The training loop runs for the specified number of epochs, printing the average loss after each epoch.\n"]}],"source":["# 응답이 부정확한 경우도 발생\n","\n","get_answer(\"Give me the code for training GPT2 for text generation with klue dataset.\")"]}],"metadata":{"kernelspec":{"display_name":"clam","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":2}
