{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_with_ref.json', 'r') as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_gpt_instruction = \"\"\"\n",
    "# Role: JudgeGPT\n",
    "## Profile\n",
    "- Language: English\n",
    "- Description: You are StackOverflowGPT, capable of writing down the answer with given query and the code.\n",
    "\n",
    "### Input\n",
    "- Question: User's question.\n",
    "- Reference code: Codes that user wants to know.\n",
    "\n",
    "### Skill\n",
    "1. Analyzing the given question and understanding the required information.\n",
    "2. Summerizing the question in natural language\n",
    "3. Describing the process to get to the reference code from the question.\n",
    "\n",
    "### Output\n",
    "- Show your understanding about question and how the reference code will handle the question.\n",
    "- Your have to give exactly same reference code posed by the user so that they can understand about the code.\n",
    "\n",
    "### Output Format\n",
    "Answer like StackOverflow user.\n",
    "\n",
    "## Rules\n",
    "1. Don’t break character.\n",
    "\n",
    "## Workflow\n",
    "1. Read and understand the question posed by the user.\n",
    "2. Summarize your understandings about the question.\n",
    "3. Explain why the reference code will solve the question.\n",
    "\n",
    "## Reminder\n",
    "You will always remind yourself of the role settings.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_answer(query, reference_code, model=\"gpt-3.5-turbo-16k\", temperature=0.2):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": so_gpt_instruction},\n",
    "        {\"role\": \"user\", \"content\": f'Question:{query}\\nReference code:{reference_code}'},\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [13:40<00:00, 12.07s/it]\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm(dataset):\n",
    "    query = dataset[key]['query']\n",
    "    positive_context = dataset[key]['contexts']['positive']\n",
    "    gold_key = positive_context['gold_document_key']\n",
    "    reference_code = positive_context[gold_key]\n",
    "    new_document = make_answer(query, reference_code)\n",
    "    dataset[key]['contexts']['positive'][gold_key] = new_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      "\n",
      "I'm trying to slice a PyTorch tensor using a logical index on the columns. I want the columns that correspond to a 1 value in the index vector. Both slicing and logical indexing are possible, but are they possible together? If so, how? My attempt keeps throwing the unhelpful error\n",
      "\n",
      "TypeError: indexing a tensor with an object of type ByteTensor. The only supported types are integers, slices, numpy scalars and torch.LongTensor or torch.ByteTensor as the only argument.\n",
      "\n",
      "MCVE\n",
      "Desired Output\n",
      "\n",
      "import torch\n",
      "\n",
      "C = torch.LongTensor([[1, 3], [4, 6]])\n",
      "# 1 3\n",
      "# 4 6\n",
      "Logical indexing on the columns only:\n",
      "\n",
      "A_log = torch.ByteTensor([1, 0, 1]) # the logical index\n",
      "B = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n",
      "C = B[:, A_log] # Throws error\n",
      "If the vectors are the same size, logical indexing works:\n",
      "\n",
      "B_truncated = torch.LongTensor([1, 2, 3])\n",
      "C = B_truncated[A_log]\n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "<code>\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import torch\n",
      "A_log, B = load_data()\n",
      "</code>\n",
      "BEGIN SOLUTION\n",
      "<code>\n",
      "\n",
      "</code>\n",
      "END SOLUTION\n",
      "<code>\n",
      "print(C)\n",
      "</code>\n"
     ]
    }
   ],
   "source": [
    "print(dataset['q10']['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_1': \"\\n  In the context of NLP, that means that sequences with variable lengths do not necessarily need to be padded to the same length.\\n\\n\\nThis means that you don't need to pad sequences unless you are doing data batching which is currently the only way to add parallelism in PyTorch. DyNet has a method called autobatching (which is described in detail in this paper) that does batching on the graph operations instead of the data, so this might be what you want to look into.\\n\\n\\n  But, if I want to use PyTorch DataLoader, I need to pad my sequences anyway because the DataLoader only takes tensors - given that me as a total beginner does not want to build some customized collate_fn.\\n\\n\\nYou can use the DataLoader given you write your own Dataset class and you are using batch_size=1. The twist is to use numpy arrays for your variable length sequences (otherwise default_collate will give you a hard time):\\n\\nfrom torch.utils.data import Dataset\\nfrom torch.utils.data.dataloader import DataLoader\\n\\nclass FooDataset(Dataset):\\n    def __init__(self, data, target):\\n        assert len(data) == len(target)\\n        self.data = data\\n        self.target = target\\n    def __getitem__(self, index):\\n        return self.data[index], self.target[index]\\n    def __len__(self):\\n        return len(self.data)\\n\\ndata = [[1,2,3], [4,5,6,7,8]]\\ndata = [np.array(n) for n in data]\\ntargets = ['a', 'b']\\n\\nds = FooDataset(data, targets)\\ndl = DataLoader(ds, batch_size=1)\\n\\nprint(list(enumerate(dl)))\\n# [(0, [\\n#  1  2  3\\n# [torch.LongTensor of size 1x3]\\n# , ('a',)]), (1, [\\n#  4  5  6  7  8\\n# [torch.LongTensor of size 1x5]\\n# , ('b',)])]\\n\\n\\n\\n  Now this makes me wonder - doesn’t this wash away the whole advantage of dynamic computational graphs in this context?\\n\\n\\nFair point but the main strength of dynamic computational graphs are (at least currently) mainly the possibility of using debugging tools like pdb which rapidly decrease your development time. Debugging is way harder with static computation graphs. There is also no reason why PyTorch would not implement further just-in-time optimizations or a concept similar to DyNet's auto-batching in the future.\\n\\n\\n  Also, if I pad my sequences to feed it into the DataLoader as a tensor with many zeros as padding tokens at the end [...], will it have any negative effect on my training [...]?\\n\\n\\nYes, both in runtime and for the gradients. The RNN will iterate over the padding just like normal data which means that you have to deal with it in some way. PyTorch supplies you with tools for dealing with padded sequences and RNNs, namely pad_packed_sequence and pack_padded_sequence. These will let you ignore the padded elements during RNN execution, but beware: this does not work with RNNs that you implement yourself (or at least not if you don't add support for it manually).\\n\",\n",
       " 'document_2': \"&gt; 1. Is my understanding of as_strided correct?\\nThe stride is an interface for your tensor to access the underlying contiguous data buffer. It does not insert values, no copies of the values are done by torch.as_strided, the strides define the artificial layout of what we refer to as multi-dimensional array (in NumPy) or tensor (in PyTorch).\\nAs Andreas K. puts it in another answer:\\n\\nStrides are the number of bytes to jump over in the memory in order to get from one item to the next item along each direction/dimension of the array. In other words, it's the byte-separation between consecutive items for each dimension.\\n\\nPlease feel free to read the answers over there if you have some trouble with strides. Here we will take your example and look at how it is implemented with as_strided.\\nThe example given by Scipy for linalg.toeplitz is the following:\\n&gt;&gt;&gt; toeplitz([1,2,3], [1,4,5,6])\\narray([[1, 4, 5, 6],\\n       [2, 1, 4, 5],\\n       [3, 2, 1, 4]])\\n\\nTo do so they first construct the list of values (what we can refer to as the underlying values, not actually underlying data): vals which is constructed as [3 2 1 4 5 6], i.e. the Toeplitz column and row flattened.\\nNow notice the arguments passed to np.lib.stride_tricks.as_strided:\\n\\nvalues: vals[len(c)-1:] notice the slice: the tensors show up smaller, yet the underlying values remain, and they correspond to those of vals. Go ahead and compare the two with storage_offset: it's just an offset of 2, the values are still there! How this works is that it essentially shifts the indices such that index=0 will refer to value 1, index=1 to 4, etc...\\n\\nshape: given by the column/row inputs, here (3, 4). This is the shape of the resulting object.\\n\\nstrides: this is the most important piece: (-n, n), in this case (-1, 1)\\n\\n\\nThe most intuitive thing to do with strides is to describe a mapping between the multi-dimensional space: (i, j) ∈ [0,3[ x [0,4[ and the flattened 1D space: k ∈ [0, 3*4[. Since the strides are equal to (-n, n) = (-1, 1), the mapping is -n*i + n*j = -1*i + 1*j = j-i. Mathematically you can describe your matrix as M[i, j] = F[j-i] where F is the flattened values vector [3 2 1 4 5 6].\\nFor instance, let's try with i=1 and j=2. If you look at the Topleitz matrix above M[1, 2] = 4. Indeed F[k] = F[j-i] = F[1] = 4\\nIf you look closely you will see the trick behind negative strides: they allow you to 'reference' to negative indices: for instance, if you take j=0 and i=2, then you see k=-2. Remember how vals was given with an offset of 2 by slicing vals[len(c)-1:]. If you look at its own underlying data storage it's still [3 2 1 4 5 6], but has an offset. The mapping for vals (in this case i: 1D -&gt; k: 1D) would be M'[i] = F'[k] = F'[i+2] because of the offset. This means M'[-2] = F'[0] = 3.\\nIn the above I defined M' as vals[len(c)-1:] which basically equivalent to the following tensor:\\n&gt;&gt;&gt; torch.as_strided(vals, size=(len(vals)-2,), stride=(1,), storage_offset=2)\\ntensor([1, 4, 5, 6])\\n\\nSimilarly, I defined F' as the flattened vector of underlying values: [3 2 1 4 5 6].\\nThe usage of strides is indeed a very clever way to define a Toeplitz matrix!\\n\\n&gt; 2. Is there a simple way to rewrite this so negative strides work?\\nThe issue is, negative strides are not implemented in PyTorch... I don't believe there is a way around it with torch.as_strided, otherwise it would be rather easy to extend the current implementation and provide support for that feature.\\nThere are however alternative ways to solve the problem. It is entirely possible to construct a Toeplitz matrix in PyTorch, but that won't be with torch.as_strided.\\nWe will do the mapping ourselves: for each element of M indexed by (i, j), we will find out the corresponding index k which is simply j-i. This can be done with ease, first by gathering all (i, j) pairs from M:\\n&gt;&gt;&gt; i, j = torch.ones(3, 4).nonzero().T\\n(tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]),\\n tensor([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]))\\n\\nNow we essentially have k:\\n&gt;&gt;&gt; j-i\\ntensor([ 0,  1,  2,  3, -1,  0,  1,  2, -2, -1,  0,  1])\\n\\nWe just need to construct a flattened tensor of all possible values from the row r and column c inputs. Negative indexed values (the content of c) are put last and flipped:\\n&gt;&gt;&gt; values = torch.cat((r, c[1:].flip(0)))\\ntensor([1, 4, 5, 6, 3, 2])\\n\\nFinally index values with k and reshape:\\n&gt;&gt;&gt; values[j-i].reshape(3, 4)\\ntensor([[1, 4, 5, 6],\\n        [2, 1, 4, 5],\\n        [3, 2, 1, 4]])\\n\\nTo sum it up, my proposed implementation would be:\\ndef toeplitz(c, r):\\n    vals = torch.cat((r, c[1:].flip(0)))\\n    shape = len(c), len(r)\\n    i, j = torch.ones(*shape).nonzero().T\\n    return vals[j-i].reshape(*shape)\\n\\n\\n&gt; 3. Will I be able to pass a gradient w.r.t c (or r) through toeplitz_torch?\\nThat's an interesting question because torch.as_strided doesn't have a backward function implemented. This means you wouldn't have been able to backpropagate to c and r! With the above method, however, which uses 'backward-compatible' builtins, the backward pass comes free of charge.\\nNotice the grad_fn on the output:\\n&gt;&gt;&gt; toeplitz(torch.tensor([1.,2.,3.], requires_grad=True), \\n             torch.tensor([1.,4.,5.,6.], requires_grad=True))\\ntensor([[1., 4., 5., 6.],\\n        [2., 1., 4., 5.],\\n        [3., 2., 1., 4.]], grad_fn=&lt;ViewBackward&gt;)\\n\\n\\nThis was a quick draft (that did take a little while to write down), I will make some edits.  If you have some questions or remarks, don't hesitate to comment!  I would be interested in seeing other answers as I am not an expert with strides, this is just my take on the problem.\\n\",\n",
       " 'document_3': \"In your case, here is how your input tensor are interpreted:\\n\\na = torch.LongTensor([[1, 2, 3, 4], [4, 3, 2, 1]]) # 2 sequences of 4 elements\\n\\n\\nMoreover, this is how your embedding layer is interpreted:\\n\\nembedding = nn.Embedding(num_embeddings=10, embedding_dim=3) # 10 distinct elements and each those is going to be embedded in a 3 dimensional space\\n\\n\\nSo, it doesn't matter if your input tensor has more than 10 elements, as long as they are in the range [0, 9]. For example, if we create a tensor of two elements such as:\\n\\nd = torch.LongTensor([[1, 10]]) # 1 sequence of 2 elements\\n\\n\\nWe would get the following error when we pass this tensor through the embedding layer:\\n\\n\\n  RuntimeError: index out of range: Tried to access index 10 out of table with 9 rows\\n\\n\\nTo summarize num_embeddings is total number of unique elements in the vocabulary, and embedding_dim is the size of each embedded vector once passed through the embedding layer. Therefore, you can have a tensor of 10+ elements, as long as each element in the tensor is in the range [0, 9], because you defined a vocabulary size of 10 elements.\\n\",\n",
       " 'document_4': \"NEW ANSWER\\nAs of PyTorch 1.1, there is a one_hot function in torch.nn.functional. Given any tensor of indices indices and a maximal index n, you can create a one_hot version as follows:\\n\\nn = 5\\nindices = torch.randint(0,n, size=(4,7))\\none_hot = torch.nn.functional.one_hot(indices, n) # size=(4,7,n)\\n\\n\\nVery old Answer\\n\\nAt the moment, slicing and indexing can be a bit of a pain in PyTorch from my experience. I assume you don't want to convert your tensors to numpy arrays. The most elegant way I can think of at the moment is to use sparse tensors and then convert to a dense tensor. That would work as follows:\\n\\nfrom torch.sparse import FloatTensor as STensor\\n\\nbatch_size = 4\\nseq_length = 6\\nfeat_dim = 16\\n\\nbatch_idx = torch.LongTensor([i for i in range(batch_size) for s in range(seq_length)])\\nseq_idx = torch.LongTensor(list(range(seq_length))*batch_size)\\nfeat_idx = torch.LongTensor([[5, 3, 2, 11, 15, 15], [1, 4, 6, 7, 3, 3],                            \\n                             [2, 4, 7, 8, 9, 10], [11, 12, 15, 2, 5, 7]]).view(24,)\\n\\nmy_stack = torch.stack([batch_idx, seq_idx, feat_idx]) # indices must be nDim * nEntries\\nmy_final_array = STensor(my_stack, torch.ones(batch_size * seq_length), \\n                         torch.Size([batch_size, seq_length, feat_dim])).to_dense()    \\n\\nprint(my_final_array)\\n\\n\\nNote: PyTorch is undergoing some work currently, that will add numpy style broadcasting and other functionalities within the next two or three weeks and other functionalities. So it's possible, there'll be better solutions available in the near future. \\n\\nHope this helps you a bit. \\n\",\n",
       " 'document_5': 'Based on the given question, the user is trying to slice a PyTorch tensor using a logical index on the columns. They want to select the columns that correspond to a value of 1 in the index vector. However, they are encountering an error when attempting to do so.\\n\\nTo solve this problem, the user has provided a reference code snippet. The code initializes a PyTorch tensor `C` with values [[1, 3], [4, 6]]. Then, a logical index tensor `A_log` is created with values [1, 0, 1]. The user wants to select the columns of `C` that correspond to a value of 1 in `A_log`.\\n\\nIn the reference code, the slicing operation `C = B[:, A_log.bool()]` is used to select the desired columns. The `bool()` function is applied to `A_log` to convert it to a `ByteTensor`, which can be used for indexing. This indexing operation will select the columns of `B` that correspond to a value of 1 in `A_log` and assign the result to `C`.\\n\\nTo summarize, the reference code snippet provided by the user demonstrates how to perform logical indexing on the columns of a PyTorch tensor. It selects the columns that correspond to a value of 1 in the logical index tensor `A_log` and assigns the result to `C`.',\n",
       " 'gold_document_key': 'document_5'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['q10']['contexts']['positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset_with_SO_GPT.json', 'w') as f:\n",
    "    json.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_with_SO_GPT.json', 'r') as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_gpt_instruction = \"\"\"\n",
    "# Role: JudgeGPT\n",
    "## Profile\n",
    "- Language: English\n",
    "- Description: You are JudgeGPT, capable of judging whether a specified number (k) of documents can maximally\n",
    "support giving a direct, accurate, clear and engaging answer, similar to the answer of the demonstration, closely\n",
    "related to the core of the user’s specific question(s).\n",
    "\n",
    "### Input\n",
    "- Question: The specific question(s).\n",
    "- Candidate Documents: Documents whose combination may maximally support giving a direct, accurate, clear\n",
    "and engaging answer, similar to the answer of the demonstration, closely related to the core of the corresponding\n",
    "question(s).\n",
    "\n",
    "### Skill\n",
    "1. Analyzing the given question(s) and understanding the required information.\n",
    "2. Searching through documents to judge whether they can maximally support giving a direct, accurate, clear\n",
    "and engaging answer, similar to the answer of the demonstration, closely related to the core of the corresponding\n",
    "question(s).\n",
    "\n",
    "### Output\n",
    "- Judgment: \"[YES]\" if provided documents can maximally support giving a direct, accurate, clear, and engaging\n",
    "answer, similar to the answer of the demonstration, closely related to the core of the corresponding question(s),\n",
    "otherwise \"[NO]\".\n",
    "\n",
    "### Output Format\n",
    "Judgment: [YES] or [NO]\n",
    "\n",
    "### Output Example\n",
    "If provided documents can maximally support giving a direct, accurate, clear, and engaging answer, similar to\n",
    "the answer of the demonstration, closely related to the core of the corresponding question(s), the output should\n",
    "be as follows: [YES]\n",
    "\n",
    "## Rules\n",
    "1. Don’t break character.\n",
    "2. When outputting final verdict, only providing \"[YES]\" or \"[NO]\".\n",
    "3. Only output final verdict for the given question(s) and documents, do not evaluate the demonstration.\n",
    "4. Strictly follow the specified output format. Do not answer the given question. Just conduct the specified\n",
    "judgment task.\n",
    "\n",
    "## Judgment Criteria (Very Important)\n",
    "1. Do not allow the length of the documents to influence your evaluation.\n",
    "2. Be as objective as possible.\n",
    "3. Output \"[YES]\" if provided documents can maximally support giving a direct, accurate, clear, and engaging\n",
    "answer, similar to the answer of the demonstration, closely related to the core of the corresponding question(s),\n",
    "otherwise \"[NO]\".\n",
    "\n",
    "## Workflow\n",
    "1. Read and understand the questions posed by the user.\n",
    "2. Browse through documents to judge whether they can support giving a direct, accurate, clear, and engaging\n",
    "answer, similar to the answer of the demonstration, closely related to the core of the corresponding question(s).\n",
    "3. Output your final verdict.\n",
    "\n",
    "## Reminder\n",
    "You will always remind yourself of the role settings.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_call(context, query, model=\"gpt-3.5-turbo-16k\", temperature=0.2):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": judge_gpt_instruction},\n",
    "        {\"role\": \"user\", \"content\": f'Question:{query}\\nCandidate Documents{context}'},\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key = 'q40'\n",
    "query = dataset[data_key]['query']\n",
    "positive_context = dataset[data_key]['contexts']['positive']\n",
    "positive_context.pop('gold_document_key')\n",
    "positive_context_string = str(list(positive_context.values()))\n",
    "\n",
    "negative_context = dataset[data_key]['contexts']['negative'][0]\n",
    "negative_context_string = str(list(negative_context.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[YES]'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_call(positive_context_string, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[NO]'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_call(negative_context_string, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key = 'q15'\n",
    "query = dataset[data_key]['query']\n",
    "positive_context = dataset[data_key]['contexts']['positive']\n",
    "positive_context.pop('gold_document_key')\n",
    "positive_context_string = str(list(positive_context.values()))\n",
    "\n",
    "negative_context = dataset[data_key]['contexts']['negative'][0]\n",
    "negative_context_string = str(list(negative_context.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[NO]'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_call(positive_context_string, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[NO]'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_call(negative_context_string, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key = 'q50'\n",
    "query = dataset[data_key]['query']\n",
    "positive_context = dataset[data_key]['contexts']['positive']\n",
    "positive_context.pop('gold_document_key')\n",
    "positive_context_string = str(list(positive_context.values()))\n",
    "\n",
    "negative_context = dataset[data_key]['contexts']['negative'][0]\n",
    "negative_context_string = str(list(negative_context.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[YES]'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_call(positive_context['document_2'], query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[YES]'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_call(positive_context['document_2']+positive_context['document_1']+positive_context['document_3'], query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[NO]'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_call(positive_context['document_1']+positive_context['document_2']+positive_context['document_3'], query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[NO]'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_call(positive_context_string, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[NO]'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_call(negative_context_string, query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
