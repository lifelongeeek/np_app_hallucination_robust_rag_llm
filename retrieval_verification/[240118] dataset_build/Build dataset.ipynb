{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from ds1000 import DS1000Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Retriever:\n",
    "    def __init__(self, contexts, corpus, tokenizer_name=\"bert-base-uncased\"):\n",
    "        self.contexts = contexts\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "        tokenized_contexts = [\n",
    "            self.tokenizer(context)[\"input_ids\"] for context in tqdm(corpus)\n",
    "        ]\n",
    "        self.bm25 = BM25Okapi(tokenized_contexts)\n",
    "        self.length = len(contexts)\n",
    "\n",
    "    def get_relevant_documents(self, query: str, topk=5):\n",
    "        tokenized_query = self.tokenizer(query)[\"input_ids\"]\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        retrieved_document_indices = sorted(\n",
    "            enumerate(scores), key=lambda x: x[1], reverse=True\n",
    "        )[:topk]\n",
    "        doc_indices = [document[0] for document in retrieved_document_indices]\n",
    "        doc_scores = [document[1] for document in retrieved_document_indices]\n",
    "        retrieved_documents = [self.contexts[idx] for idx in doc_indices]\n",
    "\n",
    "        return retrieved_documents, doc_scores\n",
    "\n",
    "    def generate_unique_numbers(self, min_num, max_num, k):\n",
    "        assert max_num > min_num, \"max_num should be greater than min_num\"\n",
    "        assert (\n",
    "            max_num - min_num + 1\n",
    "        ) >= k, \"k should be smaller than the range of numbers\"\n",
    "        unique_numbers = set()\n",
    "\n",
    "        while len(unique_numbers) < k:\n",
    "            rand_num = random.randint(min_num, max_num)\n",
    "            unique_numbers.add(rand_num)\n",
    "\n",
    "        return list(unique_numbers)\n",
    "\n",
    "    def get_easy_example_indice(self, k=5):\n",
    "        res = self.generate_unique_numbers(self.length // 2, self.length - 1, k)\n",
    "        res.sort()\n",
    "        return res\n",
    "\n",
    "    def get_hard_example_indice(self, k=5, t=2):\n",
    "        res = self.generate_unique_numbers(self.length // 4, self.length // 2, k - t)\n",
    "        hard = self.generate_unique_numbers(3, 10, t)\n",
    "        ret = hard + res\n",
    "        ret.sort()\n",
    "        return ret\n",
    "\n",
    "    def get_contexts(self, query, topk=5):\n",
    "        return_dict = {}\n",
    "\n",
    "        tokenized_query = self.tokenizer(query)[\"input_ids\"]\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        retrive_result = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "        doc_indices = [document[0] for document in retrive_result]\n",
    "        gold_document = self.contexts[doc_indices[0]]\n",
    "        selected_indice = self.generate_unique_numbers(3, topk + 4, 4)\n",
    "        non_gold_document_indice = [doc_indices[idx] for idx in selected_indice]\n",
    "        non_gold_documents = [self.contexts[idx] for idx in non_gold_document_indice]\n",
    "        positive_documents = [gold_document] + non_gold_documents\n",
    "        random.shuffle(positive_documents)\n",
    "\n",
    "        posivite_context_dict = {\n",
    "            f\"document_{idx+1}\": document\n",
    "            for idx, document in enumerate(positive_documents)\n",
    "        }\n",
    "        posivite_context_dict[\n",
    "            \"gold_document_key\"\n",
    "        ] = f\"document_{positive_documents.index(gold_document) + 1}\"\n",
    "        return_dict[\"positive\"] = posivite_context_dict\n",
    "\n",
    "        negative_context_list = []\n",
    "        for _ in range(7):\n",
    "            selected_indice = self.get_easy_example_indice()\n",
    "            negative_document_indice = [doc_indices[idx] for idx in selected_indice]\n",
    "            negative_documents = [\n",
    "                self.contexts[idx] for idx in negative_document_indice\n",
    "            ]\n",
    "            negative_context = {\n",
    "                f\"document_{idx+1}\": document\n",
    "                for idx, document in enumerate(negative_documents)\n",
    "            }\n",
    "            negative_context_list.append(negative_context)\n",
    "\n",
    "        for _ in range(2):\n",
    "            selected_indice = self.get_hard_example_indice()\n",
    "            negative_document_indice = [doc_indices[idx] for idx in selected_indice]\n",
    "            negative_documents = [\n",
    "                self.contexts[idx] for idx in negative_document_indice\n",
    "            ]\n",
    "            negative_context = {\n",
    "                f\"document_{idx+1}\": document\n",
    "                for idx, document in enumerate(negative_documents)\n",
    "            }\n",
    "            negative_context_list.append(negative_context)\n",
    "\n",
    "        return_dict[\"negative\"] = negative_context_list\n",
    "\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT with JudgeGPT Prompt\n",
    "Judge queries whether it can be answered with documents or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Retriever:\n",
    "    def __init__(self, contexts, corpus, tokenizer_name=\"bert-base-uncased\"):\n",
    "        self.contexts = contexts\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "        tokenized_contexts = [\n",
    "            self.tokenizer(context)[\"input_ids\"] for context in tqdm(corpus)\n",
    "        ]\n",
    "        self.bm25 = BM25Okapi(tokenized_contexts)\n",
    "\n",
    "    def get_relevant_documents(self, query: str, topk=5) -> str:\n",
    "        tokenized_query = self.tokenizer(query)[\"input_ids\"]\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        retrieved_document_indices = sorted(\n",
    "            enumerate(scores), key=lambda x: x[1], reverse=True\n",
    "        )[:topk]\n",
    "        doc_indices = [document[0] for document in retrieved_document_indices]\n",
    "        doc_scores = [document[1] for document in retrieved_document_indices]\n",
    "        retrieved_documents = [self.contexts[idx] for idx in doc_indices]\n",
    "\n",
    "        return retrieved_documents, doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/jykim/.cache/huggingface/datasets/shrinath-suresh___json/shrinath-suresh--stack_overflow_pytorch-7dc8b309e955f356/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3135d6c120e46408bde7f00046bc2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset('shrinath-suresh/stack_overflow_pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [data['input'] for data in dataset['train']]\n",
    "output_texts = [data['output'] for data in dataset['train']]\n",
    "corpus = [f'Q:{input_text}\\nA:{output_text}' for input_text, output_text in zip(input_texts, output_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10763 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (988 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 10763/10763 [00:13<00:00, 792.08it/s]\n"
     ]
    }
   ],
   "source": [
    "bm25 = BM25Retriever(\n",
    "    contexts=output_texts,\n",
    "    corpus=corpus\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_gpt_instruction = \"\"\"\n",
    "# Role: JudgeGPT\n",
    "## Profile\n",
    "- Language: English\n",
    "- Description: You are JudgeGPT, capable of judging whether a specified number (k) of documents can maximally\n",
    "support giving a direct, accurate, clear and engaging answer, similar to the answer of the demonstration, closely\n",
    "related to the core of the user’s specific question(s).\n",
    "\n",
    "### Input\n",
    "- Question: The specific question(s).\n",
    "- Candidate Documents: Documents whose combination may maximally support giving a direct, accurate, clear\n",
    "and engaging answer, similar to the answer of the demonstration, closely related to the core of the corresponding\n",
    "question(s).\n",
    "\n",
    "### Skill\n",
    "1. Analyzing the given question(s) and understanding the required information.\n",
    "2. Searching through documents to judge whether they can maximally support giving a direct, accurate, clear\n",
    "and engaging answer, similar to the answer of the demonstration, closely related to the core of the corresponding\n",
    "question(s).\n",
    "\n",
    "### Output\n",
    "- Judgment: \"[YES]\" if provided documents can maximally support giving a direct, accurate, clear, and engaging\n",
    "answer, similar to the answer of the demonstration, closely related to the core of the corresponding question(s),\n",
    "otherwise \"[NO]\".\n",
    "\n",
    "### Output Format\n",
    "Judgment: [YES] or [NO]\n",
    "\n",
    "### Output Example\n",
    "If provided documents can maximally support giving a direct, accurate, clear, and engaging answer, similar to\n",
    "the answer of the demonstration, closely related to the core of the corresponding question(s), the output should\n",
    "be as follows: [YES]\n",
    "\n",
    "## Rules\n",
    "1. Don’t break character.\n",
    "2. When outputting final verdict, only providing \"[YES]\" or \"[NO]\".\n",
    "3. Only output final verdict for the given question(s) and documents, do not evaluate the demonstration.\n",
    "4. Strictly follow the specified output format. Do not answer the given question. Just conduct the specified\n",
    "judgment task.\n",
    "\n",
    "## Judgment Criteria (Very Important)\n",
    "1. Do not allow the length of the documents to influence your evaluation.\n",
    "2. Be as objective as possible.\n",
    "3. Output \"[YES]\" if provided documents can maximally support giving a direct, accurate, clear, and engaging\n",
    "answer, similar to the answer of the demonstration, closely related to the core of the corresponding question(s),\n",
    "otherwise \"[NO]\".\n",
    "\n",
    "## Workflow\n",
    "1. Read and understand the questions posed by the user.\n",
    "2. Browse through documents to judge whether they can support giving a direct, accurate, clear, and engaging\n",
    "answer, similar to the answer of the demonstration, closely related to the core of the corresponding question(s).\n",
    "3. Output your final verdict.\n",
    "\n",
    "## Reminder\n",
    "You will always remind yourself of the role settings.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_call(context, query, model=\"gpt-3.5-turbo-16k\", temperature=0.2):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": judge_gpt_instruction},\n",
    "        {\"role\": \"user\", \"content\": f'Context:{context}\\n{query}'},\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [01:51<00:00,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judged to be answered: 36\n",
      "Judged not to be answered: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds1000_pytorch = DS1000Dataset(\"ds1000_data\")['Pytorch']\n",
    "yes_problems = []\n",
    "no_problems = []\n",
    "for problem in tqdm(ds1000_pytorch):\n",
    "    prefix = \"\"\n",
    "    suffix = \"\"\n",
    "    insert_flag = False\n",
    "    first_line_flag = True\n",
    "    \n",
    "    for line in problem[\"prompt\"].split(\"\\n\"):\n",
    "        if \"[insert]\" in line:\n",
    "            insert_flag = True\n",
    "            continue\n",
    "        if first_line_flag:\n",
    "            first_line_flag = False\n",
    "        else:\n",
    "            line = \"\\n\" + line\n",
    "        if not insert_flag:\n",
    "            prefix += line\n",
    "        else:\n",
    "            suffix += line\n",
    "\n",
    "    query = prefix + '\\n' + suffix\n",
    "    context = bm25.get_relevant_documents(query)\n",
    "    response = api_call(str(context[0]), query)\n",
    "    if '[YES]' in response:\n",
    "        yes_problems.append(problem)\n",
    "    else:\n",
    "        no_problems.append(problem)\n",
    "\n",
    "print(f\"Judged to be answered: {len(yes_problems)}\")\n",
    "print(f\"Judged not to be answered: {len(no_problems)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./yes_problems.pkl', 'wb') as f:\n",
    "    pickle.dump(yes_problems, f)\n",
    "\n",
    "with open('./no_problems.pkl', 'wb') as f:\n",
    "    pickle.dump(no_problems, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./yes_problems.pkl', 'rb') as f:\n",
    "    yes_problems = pickle.load(f)\n",
    "with open('./no_problems.pkl', 'rb') as f:\n",
    "    no_problems = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:27<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "for idx, problem in enumerate(tqdm(yes_problems)):\n",
    "    cur_data = {}\n",
    "    prefix = \"\"\n",
    "    suffix = \"\"\n",
    "    insert_flag = False\n",
    "    first_line_flag = True\n",
    "\n",
    "    for line in problem[\"prompt\"].split(\"\\n\"):\n",
    "        if \"[insert]\" in line:\n",
    "            insert_flag = True\n",
    "            continue\n",
    "        if first_line_flag:\n",
    "            first_line_flag = False\n",
    "        else:\n",
    "            line = \"\\n\" + line\n",
    "        if not insert_flag:\n",
    "            prefix += line\n",
    "        else:\n",
    "            suffix += line\n",
    "\n",
    "    query = prefix + '\\n' + suffix\n",
    "\n",
    "    cur_data['query'] = query\n",
    "    cur_data['reference_code'] = problem['reference_code']\n",
    "    cur_data['contexts'] = bm25.get_contexts(query)\n",
    "    \n",
    "    dataset[f'q{idx+1}'] = cur_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('dataset.json', 'w') as f:\n",
    "    json.dump(dataset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
