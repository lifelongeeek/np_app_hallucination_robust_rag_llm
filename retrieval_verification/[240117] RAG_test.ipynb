{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from ds1000 import DS1000Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve-augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Retriever:\n",
    "    def __init__(self, contexts, corpus, tokenizer_name=\"bert-base-uncased\"):\n",
    "        self.contexts = contexts\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "        tokenized_contexts = [\n",
    "            self.tokenizer(context)[\"input_ids\"] for context in tqdm(corpus)\n",
    "        ]\n",
    "        self.bm25 = BM25Okapi(tokenized_contexts)\n",
    "\n",
    "    def get_relevant_documents(self, query: str, topk=5) -> str:\n",
    "        tokenized_query = self.tokenizer(query)[\"input_ids\"]\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        retrieved_document_indices = sorted(\n",
    "            enumerate(scores), key=lambda x: x[1], reverse=True\n",
    "        )[:topk]\n",
    "        doc_indices = [document[0] for document in retrieved_document_indices]\n",
    "        doc_scores = [document[1] for document in retrieved_document_indices]\n",
    "        retrieved_documents = [self.contexts[idx] for idx in doc_indices]\n",
    "\n",
    "        return retrieved_documents, doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/jykim/.cache/huggingface/datasets/shrinath-suresh___json/shrinath-suresh--stack_overflow_pytorch-7dc8b309e955f356/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d33d099838745b0aec9704c1a1054d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset('shrinath-suresh/stack_overflow_pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [data['input'] for data in dataset['train']]\n",
    "output_texts = [data['output'] for data in dataset['train']]\n",
    "corpus = [f'Q:{input_text}\\nA:{output_text}' for input_text, output_text in zip(input_texts, output_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10763 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (988 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 10763/10763 [00:12<00:00, 837.73it/s]\n"
     ]
    }
   ],
   "source": [
    "bm25 = BM25Retriever(\n",
    "    contexts=output_texts,\n",
    "    corpus=corpus\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_gpt_instruction = \"\"\"\n",
    "# Role: JudgeGPT\n",
    "## Profile\n",
    "- Language: English\n",
    "- Description: You are JudgeGPT, capable of judging whether a specified number (k) of documents can maximally\n",
    "support giving a direct, accurate, clear and engaging answer, similar to the answer of the demonstration, closely\n",
    "related to the core of the user’s specific question(s).\n",
    "\n",
    "### Input\n",
    "- Question: The specific question(s).\n",
    "- Candidate Documents: Documents whose combination may maximally support giving a direct, accurate, clear\n",
    "and engaging answer, similar to the answer of the demonstration, closely related to the core of the corresponding\n",
    "question(s).\n",
    "\n",
    "### Skill\n",
    "1. Analyzing the given question(s) and understanding the required information.\n",
    "2. Searching through documents to judge whether they can maximally support giving a direct, accurate, clear\n",
    "and engaging answer, similar to the answer of the demonstration, closely related to the core of the corresponding\n",
    "question(s).\n",
    "\n",
    "### Output\n",
    "- Judgment: \"[YES]\" if provided documents can maximally support giving a direct, accurate, clear, and engaging\n",
    "answer, similar to the answer of the demonstration, closely related to the core of the corresponding question(s),\n",
    "otherwise \"[NO]\".\n",
    "\n",
    "### Output Format\n",
    "Judgment: [YES] or [NO]\n",
    "\n",
    "### Output Example\n",
    "If provided documents can maximally support giving a direct, accurate, clear, and engaging answer, similar to\n",
    "the answer of the demonstration, closely related to the core of the corresponding question(s), the output should\n",
    "be as follows: [YES]\n",
    "\n",
    "## Rules\n",
    "1. Don’t break character.\n",
    "2. When outputting final verdict, only providing \"[YES]\" or \"[NO]\".\n",
    "3. Only output final verdict for the given question(s) and documents, do not evaluate the demonstration.\n",
    "4. Strictly follow the specified output format. Do not answer the given question. Just conduct the specified\n",
    "judgment task.\n",
    "\n",
    "## Judgment Criteria (Very Important)\n",
    "1. Do not allow the length of the documents to influence your evaluation.\n",
    "2. Be as objective as possible.\n",
    "3. Output \"[YES]\" if provided documents can maximally support giving a direct, accurate, clear, and engaging\n",
    "answer, similar to the answer of the demonstration, closely related to the core of the corresponding question(s),\n",
    "otherwise \"[NO]\".\n",
    "\n",
    "## Workflow\n",
    "1. Read and understand the questions posed by the user.\n",
    "2. Browse through documents to judge whether they can support giving a direct, accurate, clear, and engaging\n",
    "answer, similar to the answer of the demonstration, closely related to the core of the corresponding question(s).\n",
    "3. Output your final verdict.\n",
    "\n",
    "## Reminder\n",
    "You will always remind yourself of the role settings.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_call(context, query, , model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": judge_gpt_instruction},\n",
    "        {\"role\": \"user\", \"content\": f'Context:{context}\\n{query}'},\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/68 [00:02<02:32,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/68 [00:03<01:59,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3/68 [00:04<01:38,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 4/68 [00:07<02:05,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 5/68 [00:10<02:19,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given documents provide information on how to load a pre-trained Word2Vec embedding with gensim into a PyTorch embedding layer. The documents include code examples and explanations on how to obtain the embedding weights from gensim and use them in the PyTorch embedding layer.\n",
      "\n",
      "Based on the analysis of the documents, it can be concluded that the provided documents can maximally support giving a direct, accurate, clear, and engaging answer to the question of how to load a pre-trained Word2Vec embedding with gensim into a PyTorch embedding layer. Therefore, the judgment is [YES].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 6/68 [00:12<02:11,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 7/68 [00:13<01:57,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 8/68 [00:15<01:47,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 9/68 [00:18<02:20,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 10/68 [00:21<02:14,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided documents do not contain a clear and direct answer to the user's question. Therefore, the judgment is [NO].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 11/68 [00:22<02:02,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 12/68 [00:24<01:55,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 13/68 [00:26<01:56,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 14/68 [00:28<01:49,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 15/68 [00:30<01:42,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 16/68 [00:32<01:37,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 17/68 [00:33<01:22,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 18/68 [00:35<01:23,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 19/68 [00:36<01:17,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 20/68 [00:37<01:09,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 21/68 [00:38<01:04,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 22/68 [00:42<01:39,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided documents are not sufficient to give a direct, accurate, clear, and engaging answer to the question.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 23/68 [00:43<01:23,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 24/68 [00:44<01:11,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 25/68 [00:46<01:06,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 26/68 [00:47<01:03,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 27/68 [00:49<01:06,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 28/68 [00:51<01:04,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 29/68 [00:52<01:01,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 30/68 [00:54<00:57,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 31/68 [00:56<01:05,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 32/68 [00:58<01:01,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 33/68 [01:00<01:05,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 34/68 [01:02<01:07,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided documents do not directly answer the question. The documents discuss various topics such as solving a problem using torch.stack, understanding error messages, linear regression, and fixing issues with matplotlib and ipython. However, none of the documents specifically address the question of how to convert a list of tensors to a tensor of tensors. Therefore, the judgment is [NO].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 35/68 [01:04<01:02,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 36/68 [01:05<00:55,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 37/68 [01:07<00:55,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 38/68 [01:08<00:50,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 39/68 [01:10<00:48,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 40/68 [01:12<00:46,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 41/68 [01:14<00:45,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 42/68 [01:16<00:49,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 43/68 [01:18<00:46,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 44/68 [01:19<00:42,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 45/68 [01:21<00:38,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 46/68 [01:23<00:44,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 47/68 [01:26<00:44,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 48/68 [01:30<00:51,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 49/68 [01:30<00:40,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 50/68 [01:31<00:31,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 51/68 [01:32<00:26,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 52/68 [01:34<00:22,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 53/68 [01:35<00:21,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 54/68 [01:36<00:19,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 55/68 [01:39<00:24,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 56/68 [01:44<00:31,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgment: [YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 57/68 [01:46<00:29,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 58/68 [01:48<00:23,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 59/68 [01:49<00:17,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 60/68 [01:50<00:14,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 61/68 [01:53<00:14,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 62/68 [01:55<00:11,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 63/68 [01:56<00:09,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[YES]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 64/68 [01:58<00:07,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 65/68 [02:01<00:06,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 66/68 [02:03<00:03,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 67/68 [02:05<00:01,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [02:06<00:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds1000_pytorch = DS1000Dataset(\"ds1000_data\")['Pytorch']\n",
    "yes_problems = []\n",
    "no_problems = []\n",
    "for problem in tqdm(ds1000_pytorch):\n",
    "    prefix = \"\"\n",
    "    suffix = \"\"\n",
    "    insert_flag = False\n",
    "    first_line_flag = True\n",
    "    \n",
    "    for line in problem[\"prompt\"].split(\"\\n\"):\n",
    "        if \"[insert]\" in line:\n",
    "            insert_flag = True\n",
    "            continue\n",
    "        if first_line_flag:\n",
    "            first_line_flag = False\n",
    "        else:\n",
    "            line = \"\\n\" + line\n",
    "        if not insert_flag:\n",
    "            prefix += line\n",
    "        else:\n",
    "            suffix += line\n",
    "\n",
    "    query = prefix + '\\n' + suffix\n",
    "    context = bm25.get_relevant_documents(query)\n",
    "    response = api_call(context, query)\n",
    "    if '[YES]' in response:\n",
    "        yes_problems.append(problem)\n",
    "        print(response)\n",
    "    else:\n",
    "        no_problems.append(problem)\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 43)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yes_problems), len(no_problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10763 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (988 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 10763/10763 [00:12<00:00, 833.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from BM25Ret import BM25Retriever\n",
    "\n",
    "bm25_new = BM25Retriever(\n",
    "    contexts=output_texts,\n",
    "    corpus=corpus\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YES problem example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      "\n",
      "I'm trying to convert a torch tensor to pandas DataFrame.\n",
      "However, the numbers in the data is still tensors, what I actually want is numerical values.\n",
      "This is my code\n",
      "import torch\n",
      "import pandas as  pd\n",
      "x = torch.rand(4,4)\n",
      "px = pd.DataFrame(x)\n",
      "And px looks like\n",
      "\n",
      "0   1   2   3\n",
      "tensor(0.3880)  tensor(0.4598)  tensor(0.4239)  tensor(0.7376)\n",
      "tensor(0.4174)  tensor(0.9581)  tensor(0.0987)  tensor(0.6359)\n",
      "tensor(0.6199)  tensor(0.8235)  tensor(0.9947)  tensor(0.9679)\n",
      "tensor(0.7164)  tensor(0.9270)  tensor(0.7853)  tensor(0.6921)\n",
      "How can I just get rid of 'tensor'?\n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "<code>\n",
      "import numpy as np\n",
      "import torch\n",
      "import pandas as pd\n",
      "x = load_data()\n",
      "</code>\n",
      "BEGIN SOLUTION\n",
      "<code>\n",
      "\n",
      "</code>\n",
      "END SOLUTION\n",
      "<code>\n",
      "print(px)\n",
      "</code>\n"
     ]
    }
   ],
   "source": [
    "problem = yes_problems[2]\n",
    "\n",
    "prefix = \"\"\n",
    "suffix = \"\"\n",
    "insert_flag = False\n",
    "first_line_flag = True\n",
    "\n",
    "for line in problem[\"prompt\"].split(\"\\n\"):\n",
    "    if \"[insert]\" in line:\n",
    "        insert_flag = True\n",
    "        continue\n",
    "    if first_line_flag:\n",
    "        first_line_flag = False\n",
    "    else:\n",
    "        line = \"\\n\" + line\n",
    "    if not insert_flag:\n",
    "        prefix += line\n",
    "    else:\n",
    "        suffix += line\n",
    "\n",
    "query = prefix + '\\n' + suffix\n",
    "\n",
    "documents, scores = bm25_new.get_relevant_documents(query)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I found one possible way by converting torch first to numpy:\\nimport torch\\nimport pandas as  pd\\n\\nx = torch.rand(4,4)\\npx = pd.DataFrame(x.numpy())\\n\\n',\n",
       " \"Input data:\\nimport pandas as pd\\nimport numpy as np\\nfrom torch import tensor\\n\\nmatch_df = pd.DataFrame({'INCIDENT_NUMBER': ['INC000030884498',\\n  'INC000029956111',\\n  'INC000029555353',\\n  'INC000029555338'],\\n 'enc_rep': [[[tensor(0.2971), tensor(0.4831), tensor(0.8239), tensor(0.2048)]],\\n  [[tensor(0.3481), tensor(0.8104) , tensor(0.2879), tensor(0.9747)]],\\n  [[tensor(0.2210), tensor(0.3478), tensor(0.2619), tensor(0.2429)]],\\n  [[tensor(0.2951), tensor(0.6698), tensor(0.9654), tensor(0.5733)]]]})\\n\\ninput_sentence_embed = [[tensor(0.0590), tensor(0.3919), tensor(0.7821) , tensor(0.1967)]]\\n\\n\\nHow to broadcast 'input_sentence_embed' as a new column to the 'matched_df'\\n\\nmatch_df[&quot;input_sentence_embed&quot;] = [input_sentence_embed] * len(match_df)\\n\\n\\nHow to find cosine similarity between tensors stored in two column\\n\\na = np.vstack(match_df[&quot;enc_rep&quot;])\\nb = np.hstack(input_sentence_embed)\\nmatch_df[&quot;cosine_similarity&quot;] = a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b))\\n\\nOutput result:\\n   INCIDENT_NUMBER                                            enc_rep                               input_sentence_embed  cosine_similarity\\n0  INC000030884498  [[tensor(0.2971), tensor(0.4831), tensor(0.823...  [[tensor(0.0590), tensor(0.3919), tensor(0.782...           0.446067\\n1  INC000029956111  [[tensor(0.3481), tensor(0.8104), tensor(0.287...  [[tensor(0.0590), tensor(0.3919), tensor(0.782...           0.377775\\n2  INC000029555353  [[tensor(0.2210), tensor(0.3478), tensor(0.261...  [[tensor(0.0590), tensor(0.3919), tensor(0.782...           0.201116\\n3  INC000029555338  [[tensor(0.2951), tensor(0.6698), tensor(0.965...  [[tensor(0.0590), tensor(0.3919), tensor(0.782...           0.574257\\n\\n\",\n",
       " 'Use Numpy array instead of dataframe. You can use to_numpy() to convert dataframe to numpy array.\\ntrain_dl = DataLoader(train_df.to_numpy(), bs, shuffle=True)\\ntest_dl = DataLoader(test_df.to_numpy(), len(test_df), shuffle=False)\\nval_dl = DataLoader(val_df.to_numpy(), bs, shuffle=False)\\n\\n',\n",
       " \"I'm referring to the question in the title as you haven't really specified anything else in the text, so just converting the DataFrame into a PyTorch tensor. \\n\\nWithout information about your data, I'm just taking float values as example targets here.\\n\\nConvert Pandas dataframe to PyTorch tensor?\\n\\nimport pandas as pd\\nimport torch\\nimport random\\n\\n# creating dummy targets (float values)\\ntargets_data = [random.random() for i in range(10)]\\n\\n# creating DataFrame from targets_data\\ntargets_df = pd.DataFrame(data=targets_data)\\ntargets_df.columns = ['targets']\\n\\n# creating tensor from targets_df \\ntorch_tensor = torch.tensor(targets_df['targets'].values)\\n\\n# printing out result\\nprint(torch_tensor)\\n\\n\\nOutput:\\n\\ntensor([ 0.5827,  0.5881,  0.1543,  0.6815,  0.9400,  0.8683,  0.4289,\\n         0.5940,  0.6438,  0.7514], dtype=torch.float64)\\n\\n\\nTested with Pytorch 0.4.0.\\n\\nI hope this helps, if you have any further questions - just ask. :)\\n\",\n",
       " 'When you dont normalize the data the model can be easily fooled.\\nYour train set is composed of 1000 examples that by the looks of it, the majority of the values are in the range [-1, 1].\\nWhen you test your model however, you feed it with much much much higher numbers.\\nThe solution is normalization. When you normalize your input your model can be free to learn the true distribution function of the data rather then &quot;memorize&quot; numbers.\\nYou should normalize both the training set and the test set. Then your values will range between 0 and 1 and your network will have much better chance of picking up the desired correlation.\\nimport torch\\nimport torch.nn.functional as f\\n\\ntrain = torch.rand((4, 2))*100\\n\\n\\ntensor([[36.9267,  7.3306],\\n        [63.5794, 42.9968],\\n        [61.3316, 67.6096],\\n        [88.4657, 11.7254]])\\n\\nf.normalize(train, p=2, dim=1)\\n\\ntensor([[0.9809, 0.1947],\\n        [0.8284, 0.5602],\\n        [0.6719, 0.7407],\\n        [0.9913, 0.1314]])\\n\\n']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'px = pd.DataFrame(x.numpy())'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer\n",
    "problem['reference_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      "\n",
      "I'm trying to slice a PyTorch tensor using an index on the columns. The index, contains a list of columns that I want to select in order. You can see the example later.\n",
      "I know that there is a function index_select. Now if I have the index, which is a LongTensor, how can I apply index_select to get the expected result?\n",
      "\n",
      "For example:\n",
      "the expected output:\n",
      "C = torch.LongTensor([[1, 3], [4, 6]])\n",
      "# 1 3\n",
      "# 4 6\n",
      "the index and the original data should be:\n",
      "idx = torch.LongTensor([1, 2])\n",
      "B = torch.LongTensor([[2, 1, 3], [5, 4, 6]])\n",
      "\n",
      "Thanks.\n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "<code>\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import torch\n",
      "idx, B = load_data()\n",
      "</code>\n",
      "BEGIN SOLUTION\n",
      "<code>\n",
      "\n",
      "</code>\n",
      "END SOLUTION\n",
      "<code>\n",
      "print(C)\n",
      "</code>\n"
     ]
    }
   ],
   "source": [
    "problem = yes_problems[5]\n",
    "\n",
    "prefix = \"\"\n",
    "suffix = \"\"\n",
    "insert_flag = False\n",
    "first_line_flag = True\n",
    "\n",
    "for line in problem[\"prompt\"].split(\"\\n\"):\n",
    "    if \"[insert]\" in line:\n",
    "        insert_flag = True\n",
    "        continue\n",
    "    if first_line_flag:\n",
    "        first_line_flag = False\n",
    "    else:\n",
    "        line = \"\\n\" + line\n",
    "    if not insert_flag:\n",
    "        prefix += line\n",
    "    else:\n",
    "        suffix += line\n",
    "\n",
    "query = prefix + '\\n' + suffix\n",
    "\n",
    "documents, scores = bm25_new.get_relevant_documents(query)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I think this is implemented as the index_select function, you can try\\nimport torch\\n\\nA_idx = torch.LongTensor([0, 2]) # the index vector\\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\\nC = B.index_select(1, A_idx)\\n# 1 3\\n# 4 6\\n\\n',\n",
       " 'Possible answer for 2-dimentional sparse indices\\n\\nFind an answer below, playing with several pytorch methods (torch.eq(), torch.unique(), torch.sort(), etc.) in order to output a compact, sliced tensor of shape (len(idx), len(idx)).\\n\\nI tested several edge cases (unordered idx, v with 0s, i with multiple same index pairs, etc.), though I may have forgot some. Performance should also be checked.\\n\\nimport torch\\nimport numpy as np\\n\\ndef in1D(x, labels):\\n    \"\"\"\\n    Sub-optimal equivalent to numpy.in1D().\\n    Hopefully this feature will be properly covered soon\\n    c.f. https://github.com/pytorch/pytorch/issues/3025\\n    Snippet by Aron Barreira Bordin\\n    Args:\\n        x (Tensor):             Tensor to search values in\\n        labels (Tensor/list):   1D array of values to search for\\n\\n    Returns:\\n        Tensor: Boolean tensor y of same shape as x, with y[ind] = True if x[ind] in labels\\n\\n    Example:\\n        &gt;&gt;&gt; in1D(torch.FloatTensor([1, 2, 0, 3]), [2, 3])\\n        FloatTensor([False, True, False, True])\\n    \"\"\"\\n    mapping = torch.zeros(x.size()).byte()\\n    for label in labels:\\n        mapping = mapping | x.eq(label)\\n    return mapping\\n\\n\\ndef compact1D(x):\\n    \"\"\"\\n    \"Compact\" values 1D uint tensor, so that all values are in [0, max(unique(x))].\\n    Args:\\n        x (Tensor): uint Tensor\\n\\n    Returns:\\n        Tensor: uint Tensor of same shape as x\\n\\n    Example:\\n        &gt;&gt;&gt; densify1D(torch.ByteTensor([5, 8, 7, 3, 8, 42]))\\n        ByteTensor([1, 3, 2, 0, 3, 4])\\n    \"\"\"\\n    x_sorted, x_sorted_ind = torch.sort(x, descending=True)\\n    x_sorted_unique, x_sorted_unique_ind = torch.unique(x_sorted, return_inverse=True)\\n    x[x_sorted_ind] = x_sorted_unique_ind\\n    return x\\n\\n# Input sparse tensor:\\ni = torch.from_numpy(np.array([[0,1,4,3,2,1],[0,1,3,1,4,1]]).astype(\"int64\"))\\nv = torch.from_numpy(np.arange(1, 7).astype(\"float32\"))\\ntest1 = torch.sparse.FloatTensor(i, v)\\nprint(test1.to_dense())\\n# tensor([[ 1.,  0.,  0.,  0.,  0.],\\n#         [ 0.,  8.,  0.,  0.,  0.],\\n#         [ 0.,  0.,  0.,  0.,  5.],\\n#         [ 0.,  4.,  0.,  0.,  0.],\\n#         [ 0.,  0.,  0.,  3.,  0.]])\\n\\n# note: test1[1, 1] = v[i[1,:]] + v[i[6,:]] = 2 + 6 = 8\\n#       since both i[1,:] and i[6,:] are [1,1]\\n\\n# Input slicing indices:\\nidx = [4,1,3]\\n\\n# Getting the elements in `i` which correspond to `idx`:\\nv_idx = in1D(i, idx).byte()\\nv_idx = v_idx.sum(dim=0).squeeze() == i.size(0) # or `v_idx.all(dim=1)` for pytorch 0.5+\\nv_idx = v_idx.nonzero().squeeze()\\n\\n# Slicing `v` and `i` accordingly:\\nv_sliced = v[v_idx]\\ni_sliced = i.index_select(dim=1, index=v_idx)\\n\\n# Building sparse result tensor:\\ni_sliced[0] = compact1D(i_sliced[0])\\ni_sliced[1] = compact1D(i_sliced[1])\\n\\n# To make sure to have a square dense representation:\\nsize_sliced = torch.Size([len(idx), len(idx)])\\nres = torch.sparse.FloatTensor(i_sliced, v_sliced, size_sliced)\\n\\nprint(res)\\n# torch.sparse.FloatTensor of size (3,3) with indices:\\n# tensor([[ 0,  2,  1,  0],\\n#         [ 0,  1,  0,  0]])\\n# and values:\\n# tensor([ 2.,  3.,  4.,  6.])\\n\\nprint(res.to_dense())\\n# tensor([[ 8.,  0.,  0.],\\n#         [ 4.,  0.,  0.],\\n#         [ 0.,  3.,  0.]])\\n\\n\\n\\n\\nPrevious answer for 1-dimentional sparse indices\\n\\nHere is a (probably sub-optimal and not covering all edge cases) solution, following the intuitions shared in a related open issue (hopefully this feature will be properly covered soon):\\n\\n# Constructing a sparse tensor a bit more complicated for the sake of demo:\\ni = torch.LongTensor([[0, 1, 5, 2]])\\nv = torch.FloatTensor([[1, 3, 0], [5, 7, 0], [9, 9, 9], [1,2,3]])\\ntest1 = torch.sparse.FloatTensor(i, v)\\n\\n# note: if you directly have sparse `test1`, you can get `i` and `v`:\\n# i, v = test1._indices(), test1._values()\\n\\n# Getting the slicing indices:\\nidx = [1,2]\\n\\n# Preparing to slice `v` according to `idx`.\\n# For that, we gather the list of indices `v_idx` such that i[v_idx[k]] == idx[k]:\\ni_squeeze = i.squeeze()\\nv_idx = [(i_squeeze == j).nonzero() for j in idx] # &lt;- doesn\\'t seem optimal...\\nv_idx = torch.cat(v_idx, dim=1)\\n\\n# Slicing `v` accordingly:\\nv_sliced = v[v_idx.squeeze()][:,idx]\\n\\n# Now defining your resulting sparse tensor.\\n# I\\'m not sure what kind of indexing you want, so here are 2 possibilities:\\n# 1) \"Dense\" indixing:\\ntest1x = torch.sparse.FloatTensor(torch.arange(v_idx.size(1)).long().unsqueeze(0), v_sliced)\\nprint(test1x)\\n# torch.sparse.FloatTensor of size (3,2) with indices:\\n#\\n#  0  1\\n# [torch.LongTensor of size (1,2)]\\n# and values:\\n#\\n#  7  0\\n#  2  3\\n# [torch.FloatTensor of size (2,2)]\\n\\n# 2) \"Sparse\" indixing using the original `idx`:\\ntest1x = torch.sparse.FloatTensor(autograd.Variable(torch.LongTensor(idx)).unsqueeze(0), v_sliced)\\n# note: this indexing would fail if elements of `idx` were not in `i`.\\nprint(test1x)\\n# torch.sparse.FloatTensor of size (3,2) with indices:\\n#\\n#  1  2\\n# [torch.LongTensor of size (1,2)]\\n# and values:\\n#\\n#  7  0\\n#  2  3\\n# [torch.FloatTensor of size (2,2)]\\n\\n',\n",
       " \"NEW ANSWER\\nAs of PyTorch 1.1, there is a one_hot function in torch.nn.functional. Given any tensor of indices indices and a maximal index n, you can create a one_hot version as follows:\\n\\nn = 5\\nindices = torch.randint(0,n, size=(4,7))\\none_hot = torch.nn.functional.one_hot(indices, n) # size=(4,7,n)\\n\\n\\nVery old Answer\\n\\nAt the moment, slicing and indexing can be a bit of a pain in PyTorch from my experience. I assume you don't want to convert your tensors to numpy arrays. The most elegant way I can think of at the moment is to use sparse tensors and then convert to a dense tensor. That would work as follows:\\n\\nfrom torch.sparse import FloatTensor as STensor\\n\\nbatch_size = 4\\nseq_length = 6\\nfeat_dim = 16\\n\\nbatch_idx = torch.LongTensor([i for i in range(batch_size) for s in range(seq_length)])\\nseq_idx = torch.LongTensor(list(range(seq_length))*batch_size)\\nfeat_idx = torch.LongTensor([[5, 3, 2, 11, 15, 15], [1, 4, 6, 7, 3, 3],                            \\n                             [2, 4, 7, 8, 9, 10], [11, 12, 15, 2, 5, 7]]).view(24,)\\n\\nmy_stack = torch.stack([batch_idx, seq_idx, feat_idx]) # indices must be nDim * nEntries\\nmy_final_array = STensor(my_stack, torch.ones(batch_size * seq_length), \\n                         torch.Size([batch_size, seq_length, feat_dim])).to_dense()    \\n\\nprint(my_final_array)\\n\\n\\nNote: PyTorch is undergoing some work currently, that will add numpy style broadcasting and other functionalities within the next two or three weeks and other functionalities. So it's possible, there'll be better solutions available in the near future. \\n\\nHope this helps you a bit. \\n\",\n",
       " \"In your case, here is how your input tensor are interpreted:\\n\\na = torch.LongTensor([[1, 2, 3, 4], [4, 3, 2, 1]]) # 2 sequences of 4 elements\\n\\n\\nMoreover, this is how your embedding layer is interpreted:\\n\\nembedding = nn.Embedding(num_embeddings=10, embedding_dim=3) # 10 distinct elements and each those is going to be embedded in a 3 dimensional space\\n\\n\\nSo, it doesn't matter if your input tensor has more than 10 elements, as long as they are in the range [0, 9]. For example, if we create a tensor of two elements such as:\\n\\nd = torch.LongTensor([[1, 10]]) # 1 sequence of 2 elements\\n\\n\\nWe would get the following error when we pass this tensor through the embedding layer:\\n\\n\\n  RuntimeError: index out of range: Tried to access index 10 out of table with 9 rows\\n\\n\\nTo summarize num_embeddings is total number of unique elements in the vocabulary, and embedding_dim is the size of each embedded vector once passed through the embedding layer. Therefore, you can have a tensor of 10+ elements, as long as each element in the tensor is in the range [0, 9], because you defined a vocabulary size of 10 elements.\\n\",\n",
       " \"&gt; 1. Is my understanding of as_strided correct?\\nThe stride is an interface for your tensor to access the underlying contiguous data buffer. It does not insert values, no copies of the values are done by torch.as_strided, the strides define the artificial layout of what we refer to as multi-dimensional array (in NumPy) or tensor (in PyTorch).\\nAs Andreas K. puts it in another answer:\\n\\nStrides are the number of bytes to jump over in the memory in order to get from one item to the next item along each direction/dimension of the array. In other words, it's the byte-separation between consecutive items for each dimension.\\n\\nPlease feel free to read the answers over there if you have some trouble with strides. Here we will take your example and look at how it is implemented with as_strided.\\nThe example given by Scipy for linalg.toeplitz is the following:\\n&gt;&gt;&gt; toeplitz([1,2,3], [1,4,5,6])\\narray([[1, 4, 5, 6],\\n       [2, 1, 4, 5],\\n       [3, 2, 1, 4]])\\n\\nTo do so they first construct the list of values (what we can refer to as the underlying values, not actually underlying data): vals which is constructed as [3 2 1 4 5 6], i.e. the Toeplitz column and row flattened.\\nNow notice the arguments passed to np.lib.stride_tricks.as_strided:\\n\\nvalues: vals[len(c)-1:] notice the slice: the tensors show up smaller, yet the underlying values remain, and they correspond to those of vals. Go ahead and compare the two with storage_offset: it's just an offset of 2, the values are still there! How this works is that it essentially shifts the indices such that index=0 will refer to value 1, index=1 to 4, etc...\\n\\nshape: given by the column/row inputs, here (3, 4). This is the shape of the resulting object.\\n\\nstrides: this is the most important piece: (-n, n), in this case (-1, 1)\\n\\n\\nThe most intuitive thing to do with strides is to describe a mapping between the multi-dimensional space: (i, j) ∈ [0,3[ x [0,4[ and the flattened 1D space: k ∈ [0, 3*4[. Since the strides are equal to (-n, n) = (-1, 1), the mapping is -n*i + n*j = -1*i + 1*j = j-i. Mathematically you can describe your matrix as M[i, j] = F[j-i] where F is the flattened values vector [3 2 1 4 5 6].\\nFor instance, let's try with i=1 and j=2. If you look at the Topleitz matrix above M[1, 2] = 4. Indeed F[k] = F[j-i] = F[1] = 4\\nIf you look closely you will see the trick behind negative strides: they allow you to 'reference' to negative indices: for instance, if you take j=0 and i=2, then you see k=-2. Remember how vals was given with an offset of 2 by slicing vals[len(c)-1:]. If you look at its own underlying data storage it's still [3 2 1 4 5 6], but has an offset. The mapping for vals (in this case i: 1D -&gt; k: 1D) would be M'[i] = F'[k] = F'[i+2] because of the offset. This means M'[-2] = F'[0] = 3.\\nIn the above I defined M' as vals[len(c)-1:] which basically equivalent to the following tensor:\\n&gt;&gt;&gt; torch.as_strided(vals, size=(len(vals)-2,), stride=(1,), storage_offset=2)\\ntensor([1, 4, 5, 6])\\n\\nSimilarly, I defined F' as the flattened vector of underlying values: [3 2 1 4 5 6].\\nThe usage of strides is indeed a very clever way to define a Toeplitz matrix!\\n\\n&gt; 2. Is there a simple way to rewrite this so negative strides work?\\nThe issue is, negative strides are not implemented in PyTorch... I don't believe there is a way around it with torch.as_strided, otherwise it would be rather easy to extend the current implementation and provide support for that feature.\\nThere are however alternative ways to solve the problem. It is entirely possible to construct a Toeplitz matrix in PyTorch, but that won't be with torch.as_strided.\\nWe will do the mapping ourselves: for each element of M indexed by (i, j), we will find out the corresponding index k which is simply j-i. This can be done with ease, first by gathering all (i, j) pairs from M:\\n&gt;&gt;&gt; i, j = torch.ones(3, 4).nonzero().T\\n(tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]),\\n tensor([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]))\\n\\nNow we essentially have k:\\n&gt;&gt;&gt; j-i\\ntensor([ 0,  1,  2,  3, -1,  0,  1,  2, -2, -1,  0,  1])\\n\\nWe just need to construct a flattened tensor of all possible values from the row r and column c inputs. Negative indexed values (the content of c) are put last and flipped:\\n&gt;&gt;&gt; values = torch.cat((r, c[1:].flip(0)))\\ntensor([1, 4, 5, 6, 3, 2])\\n\\nFinally index values with k and reshape:\\n&gt;&gt;&gt; values[j-i].reshape(3, 4)\\ntensor([[1, 4, 5, 6],\\n        [2, 1, 4, 5],\\n        [3, 2, 1, 4]])\\n\\nTo sum it up, my proposed implementation would be:\\ndef toeplitz(c, r):\\n    vals = torch.cat((r, c[1:].flip(0)))\\n    shape = len(c), len(r)\\n    i, j = torch.ones(*shape).nonzero().T\\n    return vals[j-i].reshape(*shape)\\n\\n\\n&gt; 3. Will I be able to pass a gradient w.r.t c (or r) through toeplitz_torch?\\nThat's an interesting question because torch.as_strided doesn't have a backward function implemented. This means you wouldn't have been able to backpropagate to c and r! With the above method, however, which uses 'backward-compatible' builtins, the backward pass comes free of charge.\\nNotice the grad_fn on the output:\\n&gt;&gt;&gt; toeplitz(torch.tensor([1.,2.,3.], requires_grad=True), \\n             torch.tensor([1.,4.,5.,6.], requires_grad=True))\\ntensor([[1., 4., 5., 6.],\\n        [2., 1., 4., 5.],\\n        [3., 2., 1., 4.]], grad_fn=&lt;ViewBackward&gt;)\\n\\n\\nThis was a quick draft (that did take a little while to write down), I will make some edits.  If you have some questions or remarks, don't hesitate to comment!  I would be interested in seeing other answers as I am not an expert with strides, this is just my take on the problem.\\n\"]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C = B.index_select(1, idx)'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer\n",
    "problem['reference_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      "\n",
      "In pytorch, given the tensors a of shape (1X11) and b of shape (1X11), torch.stack((a,b),0) would give me a tensor of shape (2X11)\n",
      "\n",
      "However, when a is of shape (2X11) and b is of shape (1X11), torch.stack((a,b),0) will raise an error cf. \"the two tensor size must exactly be the same\".\n",
      "\n",
      "Because the two tensor are the output of a model (gradient included), I can't convert them to numpy to use np.stack() or np.vstack().\n",
      "\n",
      "Is there any possible solution to give me a tensor ab of shape (3X11)?\n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "<code>\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import torch\n",
      "a, b = load_data()\n",
      "def solve(a, b):\n",
      "</code>\n",
      "BEGIN SOLUTION\n",
      "<code>\n",
      "\n",
      "</code>\n",
      "END SOLUTION\n",
      "<code>\n",
      "    return ab\n",
      "ab = solve(a, b)\n",
      "print(ab)\n",
      "</code>\n"
     ]
    }
   ],
   "source": [
    "problem = yes_problems[10]\n",
    "\n",
    "prefix = \"\"\n",
    "suffix = \"\"\n",
    "insert_flag = False\n",
    "first_line_flag = True\n",
    "\n",
    "for line in problem[\"prompt\"].split(\"\\n\"):\n",
    "    if \"[insert]\" in line:\n",
    "        insert_flag = True\n",
    "        continue\n",
    "    if first_line_flag:\n",
    "        first_line_flag = False\n",
    "    else:\n",
    "        line = \"\\n\" + line\n",
    "    if not insert_flag:\n",
    "        prefix += line\n",
    "    else:\n",
    "        suffix += line\n",
    "\n",
    "query = prefix + '\\n' + suffix\n",
    "\n",
    "documents, scores = bm25_new.get_relevant_documents(query)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It seems you want to use torch.cat() (concatenate tensors along an existing dimension) and not torch.stack() (concatenate/stack tensors along a new dimension):\\n\\nimport torch\\n\\na = torch.randn(1, 42, 1, 1)\\nb = torch.randn(1, 42, 1, 1)\\n\\nab = torch.stack((a, b), 0)\\nprint(ab.shape)\\n# torch.Size([2, 1, 42, 1, 1])\\n\\nab = torch.cat((a, b), 0)\\nprint(ab.shape)\\n# torch.Size([2, 42, 1, 1])\\naab = torch.cat((a, ab), 0)\\nprint(aab.shape)\\n# torch.Size([3, 42, 1, 1])\\n\\n',\n",
       " 'Edit: The cause is actually known. The recommended solution is to build both packages from source.\\n\\n\\n\\nThere is a known issue with importing both open3d and PyTorch. The cause is unknown. https://github.com/pytorch/pytorch/issues/19739\\n\\nA few possible workarounds exist:\\n\\n(1) Some people have found that changing the order in which you import the two packages can resolve the issue, though in my personal testing both ways crash.\\n\\n(2) Other people have found compiling both packages from source to help.\\n\\n(3) Still others have found that moving open3d and PyTorch to be called from separate scripts resolves the issue.\\n',\n",
       " \"The Approach\\nWe are going to take advantage of the Numpy community and libraries, as well as the fact that Pytorch tensors and Numpy arrays can be converted to/from one another without copying or moving the underlying arrays in memory (so conversions are low cost). From the Pytorch documentation:\\n\\nConverting a torch Tensor to a Numpy array and vice versa is a breeze. The torch Tensor and Numpy array will share their underlying memory locations, and changing one will change the other.\\n\\nSolution One\\nWe are first going to use the Numba library to write a function that will be just-in-time (JIT) compiled upon its first usage, meaning we can get C speeds without having to write C code ourselves. Of course, there are caveats to what can get JIT-ed, and one of those caveats is that we work with Numpy functions. But this isn't too bad because, remember, converting from our torch tensor to Numpy is low cost. The function we create is:\\n@njit(cache=True)\\ndef indexFunc(array, item):\\n    for idx, val in np.ndenumerate(array):\\n        if val == item:\\n            return idx\\n\\nThis function if from another Stackoverflow answer located here (This was the answer which introduced me to Numba). The function takes an N-Dimensional Numpy array and looks for the first occurrence of a given item. It immediately returns the index of the found item on a successful match. The @njit decorator is short for @jit(nopython=True), and tells the compiler that we want it to compile the function using no Python objects, and to throw an error if it is not able to do so (Numba is the fastest when no Python objects are used, and speed is what we are after).\\nWith this speedy function backing us, we can get the indices of the max values in a tensor as follows:\\nimport numpy as np\\n\\nx =  x.numpy()\\nmaxVals = np.amax(x, axis=(2,3))\\nmax_indices = np.zeros((n,p,2),dtype=np.int64)\\nfor index in np.ndindex(x.shape[0],x.shape[1]):\\n    max_indices[index] = np.asarray(indexFunc(x[index], maxVals[index]),dtype=np.int64)\\nmax_indices = torch.from_numpy(max_indices)\\n\\nWe use np.amax because it can accept a tuple for its axis argument, allowing it to return the max values of each 2D feature map in the 4D input. We initialize max_indices with np.zeros ahead of time because appending to numpy arrays is expensive, so we allocate the space we need ahead of time. This approach is much faster than the Typical Solution in the question (by an order of magnitude), but it also uses a for loop outside the JIT-ed function, so we can improve...\\nSolution Two\\nWe will use the following solution:\\n@njit(cache=True)\\ndef indexFunc(array, item):\\n    for idx, val in np.ndenumerate(array):\\n        if val == item:\\n            return idx\\n    raise RuntimeError\\n\\n@njit(cache=True, parallel=True)\\ndef indexFunc2(x,maxVals):\\n    max_indices = np.zeros((x.shape[0],x.shape[1],2),dtype=np.int64)\\n    for i in prange(x.shape[0]):\\n        for j in prange(x.shape[1]):\\n            max_indices[i,j] = np.asarray(indexFunc(x[i,j], maxVals[i,j]),dtype=np.int64)\\n    return max_indices\\n\\nx = x.numpy()\\nmaxVals = np.amax(x, axis=(2,3))\\nmax_indices = torch.from_numpy(indexFunc2(x,maxVals))\\n\\nInstead of iterating through our feature maps one-at-a-time with a for loop, we can take advantage of parallelization using Numba's prange function (which behaves exactly like range but tells the compiler we want the loop to be parallelized) and the parallel=True decorator argument. Numba also parallelizes the np.zeros function. Because our function is compiled Just-In-Time and uses no Python objects, Numba can take advantage of all the threads available in our system! It is worth noting that there is now a raise RuntimeError in the indexFunc. We need to include this, otherwise the Numba compiler will try to infer the return type of the function and infer that it will either be an array or None. This doesn't jive with our usage in indexFunc2, so the compiler would throw an error. Of course, from our setup we know that indexFunc will always return an array, so we can simply raise and error in the other logical branch.\\nThis approach is functionally identical to Solution One, but changes the iteration using nd.index into two for loops using prange. This approach is about 4x faster than Solution One.\\nSolution Three\\nSolution Two is fast, but it is still finding the max values using regular Python. Can we speed this up using a more comprehensive JIT-ed function?\\n@njit(cache=True)\\ndef indexFunc(array, item):\\n    for idx, val in np.ndenumerate(array):\\n        if val == item:\\n            return idx\\n    raise RuntimeError\\n\\n@njit(cache=True, parallel=True)\\ndef indexFunc3(x):\\n    maxVals = np.zeros((x.shape[0],x.shape[1]),dtype=np.float32)\\n    for i in prange(x.shape[0]):\\n        for j in prange(x.shape[1]):\\n            maxVals[i][j] = np.max(x[i][j])\\n    max_indices = np.zeros((x.shape[0],x.shape[1],2),dtype=np.int64)\\n    for i in prange(x.shape[0]):\\n        for j in prange(x.shape[1]):\\n            x[i][j] == np.max(x[i][j])\\n            max_indices[i,j] = np.asarray(indexFunc(x[i,j], maxVals[i,j]),dtype=np.int64)\\n    return max_indices\\n\\nmax_indices = torch.from_numpy(indexFunc3(x))\\n\\nIt might look like there is a lot more going on in this solution, but the only change is that instead of calculating the maximum values of each feature map using np.amax, we have now parallelized the operation. This approach is marginally faster than Solution Two.\\nSolution Four\\nThis solution is the best I've been able to come up with:\\n@njit(cache=True, parallel=True)\\ndef indexFunc4(x):\\n    max_indices = np.zeros((x.shape[0],x.shape[1],2),dtype=np.int64)\\n    for i in prange(x.shape[0]):\\n        for j in prange(x.shape[1]):\\n            maxTemp = np.argmax(x[i][j])\\n            max_indices[i][j] = [maxTemp // x.shape[2], maxTemp % x.shape[2]] \\n    return max_indices\\n\\nmax_indices = torch.from_numpy(indexFunc4(x))\\n\\nThis approach is more condensed and also the fastest at 33% faster than Solution Three and 50x faster than the Typical Solution. We use np.argmax to get the index of the max value of each feature map, but np.argmax only returns the index as if each feature map were flattened. That is, we get a single integer telling us which number the element is in our feature map, not the indices we need to be able to access that element. The math [maxTemp // x.shape[2], maxTemp % x.shape[2]] is to turn that singular int into the [row,column] that we need.\\nBenchmarking\\nAll approaches were benchmarked together against a random input of shape [32,d,64,64], where d was incremented from 5 to 245. For each d, 15 samples were gathered and the times were averaged. An equality test ensured that all solutions provided identical values. An example of the benchmark output is:\\n\\nA plot of the benchmarking times as d increased is (leaving out the Typical Solution so the graph isn't squashed):\\n\\nWoah! What is going on at the start with those spikes?\\nSolution Five\\nNumba allows us to produce Just-In-Time compiled functions, but it doesn't compile them until the first time we use them; It then caches the result for when we call the function again. This means the very first time we call our JIT-ed functions we get a spike in compute time as the function is compiled. Luckily, there is a way around this- if we specify ahead of time what our function's return type and argument types will be, the function will be eagerly compiled instead of compiled just-in-time. Applying this knowledge to Solution Four we get:\\n@njit('i8[:,:,:](f4[:,:,:,:])',cache=True, parallel=True)\\ndef indexFunc4(x):\\n    max_indices = np.zeros((x.shape[0],x.shape[1],2),dtype=np.int64)\\n    for i in prange(x.shape[0]):\\n        for j in prange(x.shape[1]):\\n            maxTemp = np.argmax(x[i][j])\\n            max_indices[i][j] = [maxTemp // x.shape[2], maxTemp % x.shape[2]] \\n    return max_indices    \\n\\nmax_indices6 = torch.from_numpy(indexFunc4(x))\\n\\nAnd if we restart our kernel and rerun our benchmark, we can look at the first result where d==5 and the second result where d==10 and note that all of the JIT-ed solutions were slower when d==5 because they had to be compiled, except for Solution Four, because we explicitly provided the function signature ahead of time:\\n\\nThere we go! That's the best solution I have so far for this problem.\\n\\nEDIT #1\\nSolution Six\\nAn improved solution has been developed which is 33% faster than the previously posted best solution. This solution only works if the input array is C-contiguous, but this isn't a big restriction since numpy arrays or torch tensors will be contiguous unless they are reshaped, and both have functions to make the array/tensor contiguous if needed.\\nThis solution is the same as the previous best, but the function decorator which specifies the input and return types are changed from\\n@njit('i8[:,:,:](f4[:,:,:,:])',cache=True, parallel=True)\\nto\\n@njit('i8[:,:,::1](f4[:,:,:,::1])',cache=True, parallel=True)\\nThe only difference is that the last : in each array typing becomes ::1, which signals to the numba njit compiler that the input arrays are C-contiguous, allowing it to better optimize.\\nThe full solution six is then:\\n@njit('i8[:,:,::1](f4[:,:,:,::1])',cache=True, parallel=True)\\ndef indexFunc5(x):\\n    max_indices = np.zeros((x.shape[0],x.shape[1],2),dtype=np.int64)\\n    for i in prange(x.shape[0]):\\n        for j in prange(x.shape[1]):\\n            maxTemp = np.argmax(x[i][j])\\n            max_indices[i][j] = [maxTemp // x.shape[2], maxTemp % x.shape[2]] \\n    return max_indices \\n\\nmax_indices7 = torch.from_numpy(indexFunc5(x))\\n\\nThe benchmark including this new solution confirms the speedup:\\n\\n\",\n",
       " 'I had a similar issue and spent some time to find the easiest and fastest solution. Now you can compute batched distance by using PyTorch cdist which will give you BxMxN tensor:\\n\\ntorch.cdist(Y, X)\\n\\n\\nAlso, it works well if you just want to compute distances between each pair of rows of two matrixes. \\n',\n",
       " 'For anyone who will meet the problem:\\ndef polynomial(t: torch.Tensor, degree: int = 2, interaction_only: bool = False) -&gt; torch.Tensor:\\n    cols = t.hsplit(t.shape[1])\\n    if interaction_only:\\n        degree = 2\\n        combs = combinations(cols, degree)\\n    else:\\n        combs = combinations_with_replacement(cols, degree)\\n    prods = [torch.prod(torch.hstack(comb), -1).unsqueeze(-1) for comb in combs]\\n    r = torch.hstack(prods)\\n    return torch.hstack((t, r)) if degree == 2 else torch.hstack((polynomial(t, degree - 1), r))\\n\\n']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# def solve(a, b):\\n    ### BEGIN SOLUTION\\n    ab = torch.cat((a, b), 0)\\n    ### END SOLUTION\\n    # return ab\\n# ab = solve(a, b)\\n'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer\n",
    "problem['reference_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No problem examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      "\n",
      "I want to load a pre-trained word2vec embedding with gensim into a PyTorch embedding layer.\n",
      "How do I get the embedding weights loaded by gensim into the PyTorch embedding layer?\n",
      "here is my current code\n",
      "And I need to embed my input data use this weights. Thanks\n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "runnable code\n",
      "<code>\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import torch\n",
      "from gensim.models import Word2Vec\n",
      "from gensim.test.utils import common_texts\n",
      "input_Tensor = load_data()\n",
      "word2vec = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
      "def get_embedded_input(input_Tensor):\n",
      "</code>\n",
      "BEGIN SOLUTION\n",
      "<code>\n",
      "\n",
      "</code>\n",
      "END SOLUTION\n",
      "<code>\n",
      "    return embedded_input\n",
      "embedded_input = get_embedded_input(input_Tensor)\n",
      "print(embedded_input)\n",
      "</code>\n"
     ]
    }
   ],
   "source": [
    "problem = no_problems[3]\n",
    "\n",
    "prefix = \"\"\n",
    "suffix = \"\"\n",
    "insert_flag = False\n",
    "first_line_flag = True\n",
    "\n",
    "for line in problem[\"prompt\"].split(\"\\n\"):\n",
    "    if \"[insert]\" in line:\n",
    "        insert_flag = True\n",
    "        continue\n",
    "    if first_line_flag:\n",
    "        first_line_flag = False\n",
    "    else:\n",
    "        line = \"\\n\" + line\n",
    "    if not insert_flag:\n",
    "        prefix += line\n",
    "    else:\n",
    "        suffix += line\n",
    "\n",
    "query = prefix + '\\n' + suffix\n",
    "\n",
    "documents, scores = bm25_new.get_relevant_documents(query)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I just wanted to report my findings about loading a gensim embedding with PyTorch.\\n\\n\\n\\n\\nSolution for PyTorch 0.4.0 and newer:\\n\\n\\nFrom v0.4.0 there is a new function from_pretrained() which makes loading an embedding very comfortable.\\nHere is an example from the documentation.\\n\\nimport torch\\nimport torch.nn as nn\\n\\n# FloatTensor containing pretrained weights\\nweight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\\nembedding = nn.Embedding.from_pretrained(weight)\\n# Get embeddings for index 1\\ninput = torch.LongTensor([1])\\nembedding(input)\\n\\n\\nThe weights from gensim can easily be obtained by:\\n\\nimport gensim\\nmodel = gensim.models.KeyedVectors.load_word2vec_format('path/to/file')\\nweights = torch.FloatTensor(model.vectors) # formerly syn0, which is soon deprecated\\n\\n\\nAs noted by @Guglie: in newer gensim versions the weights can be obtained by model.wv:\\n\\nweights = model.wv\\n\\n\\n\\n\\n\\nSolution for PyTorch version 0.3.1 and older:\\n\\n\\nI'm using version 0.3.1 and from_pretrained() isn't available in this version.\\n\\nTherefore I created my own from_pretrained so I can also use it with 0.3.1.\\n\\nCode for from_pretrained for PyTorch versions 0.3.1 or lower:\\n\\ndef from_pretrained(embeddings, freeze=True):\\n    assert embeddings.dim() == 2, \\\\\\n         'Embeddings parameter is expected to be 2-dimensional'\\n    rows, cols = embeddings.shape\\n    embedding = torch.nn.Embedding(num_embeddings=rows, embedding_dim=cols)\\n    embedding.weight = torch.nn.Parameter(embeddings)\\n    embedding.weight.requires_grad = not freeze\\n    return embedding\\n\\n\\nThe embedding can be loaded then just like this:\\n\\nembedding = from_pretrained(weights)\\n\\n\\nI hope this is helpful for someone.\\n\",\n",
       " 'The documentation says the following\\n\\n\\n  This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.\\n\\n\\nSo if you want to feed in a sentence, you give a LongTensor of indices, each corresponding to a word in the vocabulary, which the nn.Embedding layer will map into word vectors going forward. \\n\\nHere\\'s an illustration\\n\\ntest_voc = [\"ok\", \"great\", \"test\"]\\n# The word vectors for \"ok\", \"great\" and \"test\"\\n# are at indices, 0, 1 and 2, respectively.\\n\\nmy_embedding = torch.rand(3, 50)\\ne = nn.Embedding.from_pretrained(my_embedding)\\n\\n# LongTensor of indicies corresponds to a sentence,\\n# reshaped to (1, 3) because batch size is 1\\nmy_sentence = torch.tensor([0, 2, 1]).view(1, -1)\\n\\nres = e(my_sentence)\\nprint(res.shape)\\n# =&gt; torch.Size([1, 3, 50])\\n# 1 is the batch dimension, and there\\'s three vectors of length 50 each\\n\\n\\nIn terms of RNNs, next you can feed that tensor into your RNN module, e.g\\n\\nlstm = nn.LSTM(input_size=50, hidden_size=5, batch_first=True)\\noutput, h = lstm(res)\\nprint(output.shape)\\n# =&gt; torch.Size([1, 3, 5])\\n\\n\\nI also recommend you look into torchtext. It can automatate some of the stuff you will have to do manually otherwise.\\n',\n",
       " \"The top issue with your code is that you haven't used the Word2Vec initialization parameter necessary to toggle loss-tracking on: compute_loss=True\\n(See 'parameters' section of https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec )\\nEven with that fix, the loss-reporting is still quite buggy (as of gensim-3.8.3 &amp; this writing in August 2020):\\n\\nit's not the per-epoch total, or per-example average, one might expect. (So if you need that, as a workaround, your callback should be remembering the last value and computing the delta, or resetting the internal counter to 0.0, each epoch's end.)\\nit definitely loses precision in larger training runs, eventually becoming useless. (This may not be an issue for you.)\\nit might lose some tallies due to multithreaded value-overwriting. (This may not be a practical issue for you, depending on why you're consulting the loss value.)\\n\\n\",\n",
       " \"I'm by no means a PyTorch expert, but that snippet looks fishy to me:\\n\\n    # Put the embedded inputs into the GRU.\\n    output, hidden = self.gru(embedded, hidden)\\n    # Matrix manipulation magic.\\n    batch_size, sequence_len, hidden_size = output.shape\\n    # Technically, linear layer takes a 2-D matrix as input, so more manipulation...\\n    output = output.contiguous().view(batch_size * sequence_len, hidden_size)\\n\\n\\n\\nWhen GRU is not instantiated with batch_first=True, then the output shape is (seq_len, batch, num_directions * hidden_size) -- not that seq_len and batch_size are flipped. For the view command it actually doesn't technically matter, but that's my main issue here.\\nview(batch_size * sequence_len, hidden_size) doesn't look right at all. Say you start with a batch of size 32, but after that you have size of 32*seq_len. Usually, only the output of the last step is used (or the average or the max over all steps)\\n\\n\\nSomething like this should work:\\n\\n    # Put the embedded inputs into the GRU.\\n    output, hidden = self.gru(embedded, hidden)\\n    # Not needed, just to show the true output shape order\\n    seq_len, batch_size, hidden_size = output.shape\\n    # Given the shape of output, this is the last step\\n    output = output[-1]\\n    # output.shape = (batch_size, hidden_size) &lt;-- What you want\\n\\n\\nTwo personal words of warning:\\n\\n\\nview() is a dangerous command! PyTorch or any other framework only throws errors when the dimensions of the tensors do not match up. But just because the dimensions fit after view() does not mean the reshaping was done correctly, i.e., that the values are in the right spot of the output tensor. For example, if you have to flatten a shape (seq_len, batch_size, hidden_size) to (batch_size, seq_len*hidden_size), you cannot simply do view(batch_size, -1), but first have to do transpose(1,0) to get a shape of (batch_size, seq_len, hidden_size). With out without transpose(), view() will work and the dimensions will be correct. But only with transpose(), the values are at the right position after view()\\nSince this is such an easy mistake to make, I saw many examples on GitHub and such where in my opinion it's no done correctly. The problem is that the network often still learns something. In short, I'm not much more careful when looking and adopting code snippets and the view() command is in my opinion the biggest trap.\\n\\n\\nIf it helps, here's the forward method of a GRU classifier network:\\n\\ndef forward(self, batch, method='last_step'):\\n    embeds = self.word_embeddings(batch)\\n    x = torch.transpose(embeds, 0, 1)\\n    x, self.hidden = self.gru(x, self.hidden)\\n\\n    if method == 'last_step':\\n        x = x[-1]\\n    elif method == 'average_pooling':\\n        x = torch.sum(x, dim=0) / len(batch[0])\\n    elif method == 'max_pooling':\\n        x, _ = torch.max(x, dim=0)\\n    else:\\n        raise Exception('Unknown method.')\\n    # A series of Linear layers with ReLU and Dropout\\n    for l in self.linears:\\n        x = l(x)\\n    log_probs = F.log_softmax(x, dim=1)\\n    return log_probs\\n\\n\",\n",
       " 'Bert-as-service is a great example of doing exactly what you are asking about. \\n\\nThey use padding. But read the FAQ, in terms of which layer to get the representation from how to pool it: long story short, depends on the task. \\n\\nEDIT: I am not saying \"use Bert-as-service\"; I am saying \"rip off what Bert-as-service does.\" \\n\\nIn your example, you are getting word embeddings (because of the layer you are extracting from). Here is how Bert-as-service does that. So, it actually shouldn\\'t surprise you that this depends on sentence length.\\n\\nYou then talk about getting sentence embeddings by mean pooling over word embeddings. That is... a way to do it. But, using Bert-as-service as a guide for how to get a fixed-length representation from Bert...\\n\\n\\n  Q: How do you get the fixed representation? Did you do pooling or something?\\n  \\n  A: Yes, pooling is required to get a fixed representation of a sentence. In the default strategy REDUCE_MEAN, I take the second-to-last hidden layer of all of the tokens in the sentence and do average pooling.\\n\\n\\nSo, to do Bert-as-service\\'s default behavior, you\\'d do\\n\\ndef embed(sentence):\\n   tokens = camembert.encode(sentence)\\n   # Extract all layer\\'s features (layer 0 is the embedding layer)\\n   all_layers = camembert.extract_features(tokens, return_all_hiddens=True)\\n   pooling_layer = all_layers[-2]\\n   embedded = pooling_layer.mean(1)  # 1 is the dimension you want to average ovber\\n   # note, using numpy to take the mean is bad if you want to stay on GPU\\n   return embedded\\n\\n']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# def get_embedded_input(input_Tensor):\\n    weights = torch.FloatTensor(word2vec.wv.vectors)\\n    embedding = torch.nn.Embedding.from_pretrained(weights)\\n    embedded_input = embedding(input_Tensor)\\n    # return embedded_input'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer\n",
    "problem['reference_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      "\n",
      "How to convert a numpy array of dtype=object to torch Tensor?\n",
      "\n",
      "array([\n",
      "   array([0.5, 1.0, 2.0], dtype=float16),\n",
      "   array([4.0, 6.0, 8.0], dtype=float16)\n",
      "], dtype=object)\n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "<code>\n",
      "import pandas as pd\n",
      "import torch\n",
      "import numpy as np\n",
      "x_array = load_data()\n",
      "def Convert(a):\n",
      "</code>\n",
      "BEGIN SOLUTION\n",
      "<code>\n",
      "\n",
      "</code>\n",
      "END SOLUTION\n",
      "<code>\n",
      "    return t\n",
      "x_tensor = Convert(x_array)\n",
      "print(x_tensor)\n",
      "</code>\n"
     ]
    }
   ],
   "source": [
    "problem = no_problems[12]\n",
    "\n",
    "prefix = \"\"\n",
    "suffix = \"\"\n",
    "insert_flag = False\n",
    "first_line_flag = True\n",
    "\n",
    "for line in problem[\"prompt\"].split(\"\\n\"):\n",
    "    if \"[insert]\" in line:\n",
    "        insert_flag = True\n",
    "        continue\n",
    "    if first_line_flag:\n",
    "        first_line_flag = False\n",
    "    else:\n",
    "        line = \"\\n\" + line\n",
    "    if not insert_flag:\n",
    "        prefix += line\n",
    "    else:\n",
    "        suffix += line\n",
    "\n",
    "query = prefix + '\\n' + suffix\n",
    "\n",
    "documents, scores = bm25_new.get_relevant_documents(query)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You need to make sure that the data has the same type. In this case x_train is a 32 bit float while y_train is a Double. You have to use:\\n\\ny_train = np.array([[152.],[185.],[180.],[196.],[142.]],dtype=np.float32)\\n\\n',\n",
       " \"It is difficult to answer properly since you do not show us how you try to do it. From your error message I can see that you try to convert a numpy array containing objects to a torch tensor. This does not work, you will need a numeric data type:\\nimport torch\\nimport numpy as np\\n\\n# Your test array without 'dtype=object'\\na = np.array([\\n    np.array([0.5, 1.0, 2.0], dtype=np.float16),\\n    np.array([4.0, 6.0, 8.0], dtype=np.float16),\\n])\\n\\nb = torch.from_numpy(a)\\n\\nprint(a.dtype) # This should not be 'object'\\nprint(b)\\n\\nOutput\\nfloat16\\ntensor([[0.5000, 1.0000, 2.0000],\\n        [4.0000, 6.0000, 8.0000]], dtype=torch.float16)\\n\\n\",\n",
       " 'The reason is indeed a floating point precision issue. torch defaults to single precision, so once the truncation error becomes small enough, the total error is basically determined by the roundoff error, and reducing the truncation error further by increasing the number of steps &lt;=> decreasing the time step doesn\\'t lead to any decrease in the total error. \\n\\nTo fix this, we need to enforce double precision 64bit floats for all floating point torch tensors and numpy arrays. Note that the right way to do this is to use respectively torch.float64 and np.float64 rather than, e.g., torch.double and np.double, because the former are fixed-sized float values,  (always 64bit) while the latter depend on the machine and/or compiler. Here\\'s the fixed code:\\n\\nimport torch\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\ndef Euler(f, IC, time_grid):\\n\\n    y0 = torch.tensor([IC], dtype=torch.float64)\\n    time_grid = time_grid.to(y0[0])\\n    values = y0\\n\\n    for i in range(0, time_grid.shape[0] - 1):\\n        t_i = time_grid[i]\\n        t_next = time_grid[i+1]\\n        y_i = values[i]\\n        dt = t_next - t_i\\n        dy = f(t_i, y_i) * dt\\n        y_next = y_i + dy\\n        y_next = y_next.unsqueeze(0)\\n        values = torch.cat((values, y_next), dim=0)\\n\\n    return values\\n\\ndef RungeKutta4(f, IC, time_grid):\\n\\n    y0 = torch.tensor([IC], dtype=torch.float64)\\n    time_grid = time_grid.to(y0[0])\\n    values = y0\\n\\n    for i in range(0, time_grid.shape[0] - 1):\\n        t_i = time_grid[i]\\n        t_next = time_grid[i+1]\\n        y_i = values[i]\\n        dt = t_next - t_i\\n        dtd2 = 0.5 * dt\\n        f1 = f(t_i, y_i)\\n        f2 = f(t_i + dtd2, y_i + dtd2 * f1)\\n        f3 = f(t_i + dtd2, y_i + dtd2 * f2)\\n        f4 = f(t_next, y_i + dt * f3)\\n        dy = 1/6 * dt * (f1 + 2 * (f2 + f3) +f4)\\n        y_next = y_i + dy\\n        y_next = y_next.unsqueeze(0)\\n        values = torch.cat((values, y_next), dim=0)\\n\\n    return values\\n\\n    # differential equation\\ndef f(T, X):\\n    return X \\n\\n# initial condition\\nIC = 1.\\n\\n# integration interval\\ndef integration_interval(steps, ND=1):\\n    return torch.linspace(0, ND, steps, dtype=torch.float64)\\n\\n# analytical solution\\ndef analytical_solution(t_range):\\n    return np.exp(t_range, dtype=np.float64)\\n\\n# test a numerical method\\ndef test_method(method, t_range, analytical_solution):\\n    numerical_solution = method(f, IC, t_range)\\n    L_inf_err = torch.dist(numerical_solution, analytical_solution, float(\\'inf\\'))\\n    return L_inf_err\\n\\n\\nif __name__ == \\'__main__\\':\\n\\n    Euler_error = np.array([0.,0.,0.], dtype=np.float64)\\n    RungeKutta4_error = np.array([0.,0.,0.], dtype=np.float64)\\n    indices = np.arange(1, Euler_error.shape[0]+1)\\n    n_steps = np.power(10, indices)\\n    for i, n in np.ndenumerate(n_steps):\\n        t_range = integration_interval(steps=n)\\n        solution = analytical_solution(t_range)\\n        Euler_error[i] = test_method(Euler, t_range, solution).numpy()\\n        RungeKutta4_error[i] = test_method(RungeKutta4, t_range, solution).numpy()\\n\\n    plots_path = \"./plots\"\\n    a = plt.figure()\\n    plt.xscale(\\'log\\')\\n    plt.yscale(\\'log\\')\\n    plt.plot(n_steps, Euler_error, label=\"Euler error\", linestyle=\\'-\\')\\n    plt.plot(n_steps, RungeKutta4_error, label=\"RungeKutta 4 error\", linestyle=\\'-.\\')\\n    plt.legend()\\n    plt.savefig(plots_path + \"/errors.png\")\\n\\n\\nResult:\\n\\n\\n\\nNow, as we decrease the time step, the error of the RungeKutta4 approximation decreases with the correct rate.\\n',\n",
       " 'Depending on your use-case, having everything running through PyTorch could be advantageous (e.g. to keep all computations on the GPU).\\n\\nThe PyTorch-only solution would follow the numpy syntax (i.e. zeros[rows, raw_target] = 1.):\\n\\nimport numpy as np\\nimport torch\\n\\nbatch_size = 5\\nclasses = 4\\nraw_target = torch.from_numpy(np.array([1, 0, 3, 2, 0]))\\nrows = torch.range(0, batch_size-1, dtype=torch.int64)\\n\\nx = torch.zeros((batch_size, classes), dtype=torch.float64)\\nx[rows, raw_target] = 1.\\n\\nprint(x.detach())\\n# tensor([[ 0.,  1.,  0.,  0.],\\n#         [ 1.,  0.,  0.,  0.],\\n#         [ 0.,  0.,  0.,  1.],\\n#         [ 0.,  0.,  1.,  0.],\\n#         [ 1.,  0.,  0.,  0.]], dtype=torch.float64)\\n\\n',\n",
       " 'Yes, t and arr are different Python objects at different regions of memory (hence different id) but both point to the same memory address which contains the data (contiguous (usually) C array).\\n\\nnumpy operates on this region using C code binded to Python functions, same goes for torch (but using C++). id() doesn\\'t know anything about the memory address of data itself, only of it\\'s \"wrappers\". \\n\\nEDIT: When you assign b = a (assuming a is np.array), both are references to the same Python wrapper (np.ndarray). In other words they are the same object with different name. \\n\\nIt\\'s just how Python\\'s assignment works, see documentation. All of the cases below would return True as well:\\n\\nimport torch\\nimport numpy as np\\n\\ntensor = torch.tensor([1,2,3])\\ntensor2 = tensor\\nid(tensor) == id(tensor2)\\n\\narr = np.array([1, 2, 3, 4, 5])\\narr2 = arr\\nid(arr) == id(arr2)\\n\\nsome_str = \"abba\"\\nother_str = some_str\\nid(some_str) == id(other_str)\\n\\nvalue = 0\\nvalue2 = value\\nid(value) == id(value2)\\n\\n\\nNow, when you use torch.from_numpy on np.ndarray you have two objects of different classes (torch.Tensor and original np.ndarray). As those are of different types they can\\'t have the same id. One could see this case as analogous to the one below:\\n\\nvalue = 3\\nstring_value = str(3)\\n\\nid(value) == id(string_value)\\n\\n\\nHere it\\'s intuitive both string_value and value are two different objects at different memory locations.\\n\\nEDIT 2:\\n\\nAll in all concepts of Python object and underlying C array have to be separated. id() doesn\\'t know about C bindings (how could it?), but it knows about memory addresses of Python structures (torch.Tensor, np.ndarray). \\n\\nIn case of numpy and torch.tensor you can have following situations:\\n\\n\\nseparate on Python level but using same memory region for array (torch.from_numpy)\\nseparate on Python level and underlying memory region (one torch.tensor and another np.array). Could be created by from_numpy followed by clone() or a-like deep copy operation.\\nsame on Python level and underlying memory region (e.g. two torch.tensor objects, one referencing another as provided above)\\n\\n']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# def Convert(a):\\n    ### BEGIN SOLUTION\\n    t = torch.from_numpy(a.astype(float))\\n    ### END SOLUTION\\n    # return t\\n# x_tensor = Convert(x_array)\\n'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer\n",
    "problem['reference_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      "\n",
      "Given a 3d tenzor, say: batch x sentence length x embedding dim\n",
      "\n",
      "a = torch.rand((10, 1000, 23))\n",
      "and an array(or tensor) of actual lengths for each sentence\n",
      "\n",
      "lengths =  torch .randint(1000,(10,))\n",
      "outputs tensor([ 137., 152., 165., 159., 145., 264., 265., 276.,1000., 203.])\n",
      "\n",
      "How to fill tensor ‘a’ with 2333 before certain index along dimension 1 (sentence length) according to tensor ‘lengths’ ?\n",
      "\n",
      "I want smth like that :\n",
      "\n",
      "a[ : , : lengths , : ]  = 2333\n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "<code>\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import torch\n",
      "a = torch.rand((10, 1000, 23))\n",
      "lengths = torch.randint(1000, (10,))\n",
      "</code>\n",
      "BEGIN SOLUTION\n",
      "<code>\n",
      "\n",
      "</code>\n",
      "END SOLUTION\n",
      "<code>\n",
      "print(a)\n",
      "</code>\n"
     ]
    }
   ],
   "source": [
    "problem = no_problems[20]\n",
    "\n",
    "prefix = \"\"\n",
    "suffix = \"\"\n",
    "insert_flag = False\n",
    "first_line_flag = True\n",
    "\n",
    "for line in problem[\"prompt\"].split(\"\\n\"):\n",
    "    if \"[insert]\" in line:\n",
    "        insert_flag = True\n",
    "        continue\n",
    "    if first_line_flag:\n",
    "        first_line_flag = False\n",
    "    else:\n",
    "        line = \"\\n\" + line\n",
    "    if not insert_flag:\n",
    "        prefix += line\n",
    "    else:\n",
    "        suffix += line\n",
    "\n",
    "query = prefix + '\\n' + suffix\n",
    "\n",
    "documents, scores = bm25_new.get_relevant_documents(query)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You can do it using a binary mask.\\nUsing lengths as column-indices to mask we indicate where each sequence ends (note that we make mask longer than a.size(1) to allow for sequences with full length).\\nUsing cumsum() we set all entries in mask after the seq len to 1.\\nmask = torch.zeros(a.shape[0], a.shape[1] + 1, dtype=a.dtype, device=a.device)\\nmask[(torch.arange(a.shape[0]), lengths)] = 1\\nmask = mask.cumsum(dim=1)[:, :-1]  # remove the superfluous column\\na = a * (1. - mask[..., None])     # use mask to zero after each column\\n\\nFor a.shape = (10, 5, 96), and lengths = [1, 2, 1, 1, 3, 0, 4, 4, 1, 3].\\nAssigning 1 to respective lengths at each row, mask looks like:\\nmask = \\ntensor([[0., 1., 0., 0., 0., 0.],\\n        [0., 0., 1., 0., 0., 0.],\\n        [0., 1., 0., 0., 0., 0.],\\n        [0., 1., 0., 0., 0., 0.],\\n        [0., 0., 0., 1., 0., 0.],\\n        [1., 0., 0., 0., 0., 0.],\\n        [0., 0., 0., 0., 1., 0.],\\n        [0., 0., 0., 0., 1., 0.],\\n        [0., 1., 0., 0., 0., 0.],\\n        [0., 0., 0., 1., 0., 0.]])\\n\\nAfter cumsum you get\\nmask = \\ntensor([[0., 1., 1., 1., 1.],\\n        [0., 0., 1., 1., 1.],\\n        [0., 1., 1., 1., 1.],\\n        [0., 1., 1., 1., 1.],\\n        [0., 0., 0., 1., 1.],\\n        [1., 1., 1., 1., 1.],\\n        [0., 0., 0., 0., 1.],\\n        [0., 0., 0., 0., 1.],\\n        [0., 1., 1., 1., 1.],\\n        [0., 0., 0., 1., 1.]])\\n\\nNote that it exactly has zeros where the valid sequence entries are and ones beyond the lengths of the sequences. Taking 1 - mask gives you exactly what you want.\\nEnjoy ;)\\n',\n",
       " \"You can create a 2D mask tensor with the number of True's in i-th row given by lengths[i]. Here's one example:\\n\\nbatch_size = 6\\nn = 5\\n\\npreds = torch.arange(batch_size * n).reshape(batch_size, n)\\n# tensor([[ 0,  1,  2,  3,  4],\\n#         [ 5,  6,  7,  8,  9],\\n#         [10, 11, 12, 13, 14],\\n#         [15, 16, 17, 18, 19],\\n#         [20, 21, 22, 23, 24],\\n#         [25, 26, 27, 28, 29]])\\n\\n#lengths = np.random.randint(0, n+1, batch_size)\\nlengths = torch.randint(0, n+1, (batch_size, ))\\n# tensor([2, 0, 5, 3, 3, 2])\\n\\n\\nLet's create the mask and get our result (probably there is a better way to create such a mask, but that's what I came up with):\\n\\n#mask = np.tile(range(n), (batch_size,1)) &lt; lengths[:,None]\\nmask = torch.arange(n).repeat((batch_size,1)) &lt; lengths[:, None]\\n# tensor([[ True,  True, False, False, False],\\n#        [False, False, False, False, False],\\n#        [ True,  True,  True,  True,  True],\\n#        [ True,  True,  True, False, False],\\n#        [ True,  True,  True, False, False],\\n#        [ True,  True, False, False, False]])\\n\\n#result = preds[mask]\\nresult = torch.masked_select(preds, mask)\\n# tensor([0, 1, 10, 11, 12, 13, 14, 15, 16, 17, 20, 21, 22, 25, 26])\\n\\n\\nThis produces the same result as your code:\\n\\nres_pred = []\\nfor i in range(len(preds)):\\n    length = lengths[i].item()\\n    res_pred += [preds[i][:length]]\\n\\nresult = torch.cat(res_pred).flatten()\\n\\n\",\n",
       " 'After being quite stumped by the behavior, I did some more digging into this, and found that it is consistent behavior with the indexing of multi-dimensional NumPy arrays. What makes this counter-intuitive is the less obvious fact that both arrays have to have the same length, i.e. in this case len(lengths).\\n\\nIn fact, it works as the following:\\n * lengths is determining the order in which you access the first dimension. I.e., if you have a 1D array a = [0, 1, 2, ...., 500], and access it with the list b = [300, 200, 100], then the result a[b] = [301, 201, 101] (This also explains the lengths - 1 operator, which simply causes the accessed values to be the same as the index used in b, or lengths, respectively).\\n * range(len(lengths)) then *simply chooses the i-th element in the i-th row. If you have a square matrix, you can interpret this as the diagonal of the matrix. Since you only access a single element for each position along the first two dimensions, this can be stored in a single dimension (thus reducing your 3D tensor to 2D). The latter dimension is simply kept \"as is\".\\n\\nIf you want to play around with this, I strongly recommend to change the range() value to something longer/shorter, which will result in the following error:\\n\\n\\n  IndexError: shape mismatch: indexing arrays could not be broadcast\\n  together with shapes (x,) (y,)\\n\\n\\nwhere x and y are your specific length values.\\n\\nTo write this accessing method out in the long form to understand what happens \"under the hood\", also consider the below example:\\n\\nimport torch\\nx = torch.randint(500, 50, 1)\\nlengths = torch.tensor([2, 30, 1, 4])  # random examples to explore\\ndiag = list(range(len(lengths)))  # [0, 1, 2, 3]\\nresult = []\\nfor i, row in enumerate(lengths):\\n    temp_tensor = x[row, :, :]  # temp_tensor.shape = [1, 50, 1]\\n    temp_tensor = temp_tensor.squeeze(0)[diag[i]]  # temp_tensor.shape = [1, 1]\\n    result.append(temp.tensor)\\n\\n# back to pytorch\\nresult = torch.tensor(result)\\nresult.shape  # [4, 1]\\n\\n',\n",
       " \"For this problem, it might be such easier if you consider the Net() with 1 Linear layer as Linear Regression with inputs features including [x^2, x].\\n\\nGenerate your data\\n\\nimport torch\\nfrom torch import Tensor\\nfrom torch.nn import Linear, MSELoss, functional as F\\nfrom torch.optim import SGD, Adam, RMSprop\\nfrom torch.autograd import Variable\\nimport numpy as np\\n\\n# define our data generation function\\ndef data_generator(data_size=1000):\\n    # f(x) = y = x^2 + 4x - 3\\n    inputs = []\\n    labels = []\\n\\n    # loop data_size times to generate the data\\n    for ix in range(data_size):\\n        # generate a random number between 0 and 1000\\n        x = np.random.randint(2000) / 1000 # I edited here for you\\n\\n        # calculate the y value using the function x^2 + 4x - 3\\n        y = (x * x) + (4 * x) - 3\\n\\n        # append the values to our input and labels lists\\n        inputs.append([x*x, x])\\n        labels.append([y])\\n\\n    return inputs, labels\\n\\n\\nDefine your model\\n\\n# define the model\\nclass Net(torch.nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        self.fc1 = Linear(2, 1)\\n\\n    def forward(self, x):\\n        return self.fc1(x)\\n\\nmodel = Net()\\n\\n\\nThen train it we get:\\n\\nEpoch: 0 Loss: 33.75775909423828\\nEpoch: 1000 Loss: 0.00046704441774636507\\nEpoch: 2000 Loss: 9.437128483114066e-07\\nEpoch: 3000 Loss: 2.0870876138445738e-09\\nEpoch: 4000 Loss: 1.126847400112485e-11\\nPrediction: 5.355223655700684\\nExpected: [5.355224999999999]\\n\\n\\nThe coeffients\\n\\nThe coefficients a, b, c you are looking for are actually the weight and bias of the self.fc1:\\n\\nprint('a &amp; b:', model.fc1.weight)\\nprint('c:', model.fc1.bias)\\n\\n# Output\\na &amp; b: Parameter containing:\\ntensor([[1.0000, 4.0000]], requires_grad=True)\\nc: Parameter containing:\\ntensor([-3.0000], requires_grad=True)\\n\\n\\nIn only 5000 epochs, all converges: a -> 1, b -> 4, and c -> -3.\\n\\nThe model is such a light-weight with only 3 parameters instead of:\\n\\n(100 + 1) + (100 + 1) = 202 parameters in the old model\\n\\n\\nHope this helps you!\\n\",\n",
       " 'I am assuming you have a 3d tensor of shape BxSxW where:\\n\\nB = Batch size\\nS = Sentence length\\nW = Word length\\n\\n\\nAnd you have declared embedding layer as follows.\\n\\nself.embedding = nn.Embedding(dict_size, emsize)\\n\\n\\nWhere:\\n\\ndict_size = No. of unique characters in the training corpus\\nemsize = Expected size of embeddings\\n\\n\\nSo, now you need to convert the 3d tensor of shape BxSxW to a 2d tensor of shape BSxW and give it to the embedding layer.\\n\\nemb = self.embedding(input_rep.view(-1, input_rep.size(2)))\\n\\n\\nThe shape of emb will be BSxWxE where E is the embedding size. You can convert the resulting 3d tensor to a 4d tensor as follows.\\n\\nemb = emb.view(*input_rep.size(), -1)\\n\\n\\nThe final shape of emb will be BxSxWxE which is what you are expecting.\\n']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for i_batch in range(10):\n",
      "    a[i_batch, :lengths[i_batch], :] = 2333\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "print(problem['reference_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document retrieval score comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_avgs = []\n",
    "\n",
    "for problem in yes_problems:\n",
    "    prefix = \"\"\n",
    "    suffix = \"\"\n",
    "    insert_flag = False\n",
    "    first_line_flag = True\n",
    "\n",
    "    for line in problem[\"prompt\"].split(\"\\n\"):\n",
    "        if \"[insert]\" in line:\n",
    "            insert_flag = True\n",
    "            continue\n",
    "        if first_line_flag:\n",
    "            first_line_flag = False\n",
    "        else:\n",
    "            line = \"\\n\" + line\n",
    "        if not insert_flag:\n",
    "            prefix += line\n",
    "        else:\n",
    "            suffix += line\n",
    "\n",
    "    query = prefix + '\\n' + suffix\n",
    "\n",
    "    documents, scores = bm25_new.get_relevant_documents(query)\n",
    "    yes_avgs.append(sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_avgs = []\n",
    "\n",
    "for problem in no_problems:\n",
    "    prefix = \"\"\n",
    "    suffix = \"\"\n",
    "    insert_flag = False\n",
    "    first_line_flag = True\n",
    "\n",
    "    for line in problem[\"prompt\"].split(\"\\n\"):\n",
    "        if \"[insert]\" in line:\n",
    "            insert_flag = True\n",
    "            continue\n",
    "        if first_line_flag:\n",
    "            first_line_flag = False\n",
    "        else:\n",
    "            line = \"\\n\" + line\n",
    "        if not insert_flag:\n",
    "            prefix += line\n",
    "        else:\n",
    "            suffix += line\n",
    "\n",
    "    query = prefix + '\\n' + suffix\n",
    "\n",
    "    documents, scores = bm25_new.get_relevant_documents(query)\n",
    "    no_avgs.append(sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b3cbcc40>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(16, 9))\n",
    "plt.hist(yes_avgs, label=\"YES\", alpha=0.5)\n",
    "plt.hist(no_avgs, label=\"NO\", alpha=0.5)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
